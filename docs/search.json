[
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "This page contains code snippets for algorithms in the book, sometimes in multiple languages. Click on the language icons to view and download the code.\n\nChapter 1 - The Bayesics\n\n\nChapter 2 - One-parameter models\n\n\nChapter 3 - Multi-parameter models\n\n\nChapter 4 - Priors\n\n\nChapter 5 - Regression\n\n\nChapter 6 - Prediction and Decision making\n\n\nChapter 7 - Normal posterior approximation\n\n\nChapter 8 - Classification\n\n\nChapter 9 - Gibbs sampling\n\n\n\n\nGibbs sampling - multivariate normal\n\n\n\n\n\nGibbs sampling - mixture of normals\n\n\n\n\n\nGibbs sampling - mixture of Poissons\n\n\n\n\n\nGibbs sampling - probit regression\n\n\n\n\n\nGibbs sampling - logistic regression\n\n\n\n\n\nGibbs sampling - autoregressive processes\n\n\n\n\n\n\n\n\nChapter 10 - Markov Chain Monte Carlo simulation\n\n\nChapter 11 - Variational inference\n\n\nChapter 12 - Regularization\n\n\n\n\nGibbs sampling - linear regression with L2-regularization\n\n\n\n\n\n\n\n\nChapter 13 - Mixture models and Bayesian nonparametrics\n\n\nChapter 14 - Model comparison and variable selection\n\n\nChapter 15 - Gaussian processes\n\n\nChapter 16 - Interaction models\n\n\nChapter 17 - Dynamic models and sequential inference\n\n\n\n\nKalman filter and parameter estimation"
  },
  {
    "objectID": "halloferrors.html",
    "href": "halloferrors.html",
    "title": "Hall of Typos",
    "section": "",
    "text": "The following people üôè have kindly reported typos and errors in the book (ordered by the number of typos found):\n\nAlice Jonason, Stockholm University\nMichael Sederlin, KTH Royal Institute of Technology\nTea Unneb√§ck, Stockholm University\nFederico M. Stefanini, University of Milan\nDagmar Kallenberg, Stockholm University\n\nPlease email me any typos and errors at my obvious gmail address. Thanks!"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "This page contains a set of notebooks in Julia, R and Python for some of the data analyses presented in the book.\n\nChapter 1 - The Bayesics\n\n\nChapter 2 - One-parameter models\n\n\n\n\nBernoulli model for spam data\n\n\n\n\n\nNormal model for internet download speed data\n\n\n\n\n\nPoisson for number of eBay bidders\n\n\n\n\n\n\n\n\nChapter 3 - Multi-parameter models\n\n\n\n\nMultinomial model for survey data\n\n\n\n\n\n\n\n\n\n\n\nChapter 4 - Priors\n\n\nChapter 5 - Regression\n\n\nChapter 6 - Prediction and Decision making\n\n\nChapter 7 - Normal posterior approximation\n\n\nChapter 8 - Classification\n\n\n\n\nLogistic regression for spam data\n\n\n\n\n\n\n\n\nChapter 9 - Gibbs sampling\n\n\nChapter 10 - Markov Chain Monte Carlo simulation\n\n\nChapter 11 - Variational inference\n\n\nChapter 12 - Regularization\n\n\nChapter 13 - Mixture models and Bayesian nonparametrics\n\n\nChapter 14 - Model comparison and variable selection\n\n\nChapter 15 - Gaussian processes\n\n\nChapter 16 - Interaction models\n\n\nChapter 17 - Dynamic models and sequential inference\n\n\n\n\nFiltering and smoothing of the Nile river data"
  },
  {
    "objectID": "observable/distributions.html#beta-distribution",
    "href": "observable/distributions.html#beta-distribution",
    "title": "Distributions",
    "section": "Beta distribution",
    "text": "Beta distribution"
  },
  {
    "objectID": "observable/distributions.html#binomial-distribution",
    "href": "observable/distributions.html#binomial-distribution",
    "title": "Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution"
  },
  {
    "objectID": "observable/distributions.html#multivariate-normal-distribution",
    "href": "observable/distributions.html#multivariate-normal-distribution",
    "title": "Distributions",
    "section": "Multivariate normal distribution",
    "text": "Multivariate normal distribution"
  },
  {
    "objectID": "observable/distributions.html#poisson-distribution",
    "href": "observable/distributions.html#poisson-distribution",
    "title": "Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution"
  },
  {
    "objectID": "observable/distributions.html#scaled-inverse-chi2-distribution",
    "href": "observable/distributions.html#scaled-inverse-chi2-distribution",
    "title": "Distributions",
    "section": "Scaled inverse chi2 distribution",
    "text": "Scaled inverse chi2 distribution"
  },
  {
    "objectID": "observable/distributions.html#student-t-distribution",
    "href": "observable/distributions.html#student-t-distribution",
    "title": "Distributions",
    "section": "Student-t distribution",
    "text": "Student-t distribution"
  },
  {
    "objectID": "notebooks/Untitled.html",
    "href": "notebooks/Untitled.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "a = 4\n\n4\n\n\n\nb = 4\n\n4\n\n\n\nprintln(\"This is the value of a= $a\")\n\nThis is the value of a= 4"
  },
  {
    "objectID": "notebooks/ebayPoissonOneParam/eBayPoissonR.html",
    "href": "notebooks/ebayPoissonOneParam/eBayPoissonR.html",
    "title": "Modeling the number of Bids in eBay coin auctions",
    "section": "",
    "text": "a notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nWe want learn about the number of bidders in a eBay internet auction. The dataset below contains information on the number of bidders and covariates/features that can be used to predict the number of bidders. We will later use a Poisson regression to build a prediction model, but we will here only analyze the number of bids using an simple iid Poission model without covariates.\n\n\nImport modules and load the data\n\nset.seed(123) # Set the seed for reproducibility\noptions(repr.plot.width=15, repr.plot.height=6, lwd = 4)\n#install.packages(\"RColorBrewer\")\nlibrary(\"RColorBrewer\")\ncolors = brewer.pal(12, \"Paired\")\n\n# Load the data\neBayData = read.csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData$nBids\n\n\n\nData\nThe dataset contains data from 1000 auctions of collector coins. The dataset was collected and first analyzed in the article Bayesian Inference in Structural Second-Price Common Value Auctions. Let‚Äôs read in the full dataset and extract the variable of interest, the number of bids (nBids):\n\neBayData = read.csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData$nBids\n\n\n\nPrior-to-Posterior updating\nWe will model these data using a Poisson distribution: \\[y_1,...,y_n \\vert \\theta \\overset{iid}{\\sim} \\mathrm{Poisson}(\\theta)\\]\nwith a conjugate Gamma prior\n\\[\\theta  \\sim \\mathrm{Gamma}(\\alpha, \\beta)\\]\nso that the posterior is also Gamma: \\[\\theta \\vert y_1,\\ldots,y_n \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n y_i, \\beta + n)\\]\n\nPostPoisson &lt;- function(y, alphaPrior, betaPrior, thetaPriorGrid = NA, thetaPostGrid = NA){\n\n    # Compute Prior density and posterior\n    priorDens = dgamma(thetaPriorGrid, shape = alphaPrior, rate = betaPrior)\n    n = length(y)\n    alphaPost = alphaPrior + sum(y)\n    betaPost = betaPrior + n\n    postDens = dgamma(thetaPostGrid, shape = alphaPost, rate = betaPost)\n    \n    message(paste('Mean number of counts = ', mean(y)))\n    message(paste('Prior mean = ', alphaPrior/betaPrior))\n    message(paste('Posterior mean = ', round(alphaPost/betaPost,3)))\n    message(paste('Prior standard deviation = ', sqrt(alphaPrior/(betaPrior**2))))\n    message(paste('Posterior standard deviation = ', sqrt( (alphaPrior+sum(y))/((betaPrior+n)**2)) ))\n    message(paste('Equal tail 95% prior interval: (' ,qgamma(0.025, shape = alphaPrior, rate = betaPrior),',',qgamma(0.975, shape = alphaPrior, rate = betaPrior),')'))\n    message(paste('Equal tail 95% posterior interval: (' ,qgamma(0.025, shape = alphaPost, rate = betaPost),',',qgamma(0.975, shape = alphaPost, rate = betaPost),')'))\n\n    if ( any(is.na(thetaPriorGrid)) != TRUE){\n        par(mfrow = c(1,2))\n        plot(thetaPriorGrid, priorDens, type = \"l\", lwd = 3, col = colors[2], xlab = expression(theta), ylab = \"PDF\", main = 'Prior distribution')\n        plot(thetaPostGrid, postDens, type = \"l\", lwd = 3, col = colors[8], xlab = expression(theta), ylab = \"PDF\", main = 'Posterior distribution')\n    }\n}\n\nalphaPrior = 2\nbetaPrior = 1/2\nPostPoisson(y = nBids, alphaPrior = 2, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 12, length = 10000), thetaPostGrid = seq(3.25, 4, length = 10000))\n\n\n\nFit of the Poisson model\nLet‚Äôs plot the data along with the fitted Poisson model. We‚Äôll keep things simple and plot the fit for the posterior mean of \\(\\theta\\).\n\nplotPoissonFit &lt;- function(y, alphaPrior, betaPrior){\n    \n    # Compute empirical distribution of the data\n    n = length(y)\n    yGrid = seq(0, max(y))\n    probs = rep(NA,max(y)+1)\n    for (i in yGrid){\n        probs[i+1] = sum(y == i)/n\n    }\n    \n    # Compute posterior mean and Poisson model fit\n    alphaPost = alphaPrior + sum(y)\n    betaPost = betaPrior + n\n    postMean = alphaPost/betaPost\n    \n    # Plot the data and model fit\n    poisFit = dpois(yGrid, lambda = postMean) \n    plot(yGrid, probs, type = \"o\", lwd = 6, xlab = \"y\", ylab = \"PMF\", col = colors[1], main = 'Fitted Poisson model', \n           ylim = c(0,max(probs, poisFit)))\n    lines(yGrid, poisFit, col = colors[2], lwd = 6, type = \"o\")\n    legend(x = \"topright\", inset=.05, legend = c(\"Data distribution\", \"Poisson fit\"), pch = c(19,19), cex = c(1,1),\n       lty = c(1, 1), pt.lwd = c(3,3), col = c(colors[1], colors[2]))\n}\n\n\nalphaPrior = 2\nbetaPrior = 1/2\nplotPoissonFit(y = nBids, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\nWow, that‚Äôs are terrible fit! This data does not look at all like a Poisson distribution. What can we do?\n\n\nAnalyzing only the auctions with low reservation prices\nWe will later model the number of bids using a Poisson regression where we take into account several explanatory variables. But, for now, let‚Äôs split the auctions in two subsets:\ni) auctions with low reservation price in relation to the item‚Äôs book value (MinBidShare&lt;=0)\nii) auctions with high reservation price in relation to the item‚Äôs book value (MinBidShare&gt;0)\nLet‚Äôs start with the 550 auction with low reservation prices. The prior for the auction with low reservation prices is set to \\(\\theta \\sim \\mathrm{Gamma}(4,1/2)\\) to reflect a belief that belief that such auctions are likely to attract more bids.\n\n# Auctions with low reservation prices:\nnBidsLow = nBids[eBayData$MinBidShare&lt;=0]\nPostPoisson(y = nBidsLow, alphaPrior = 4, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 25,length = 10000), thetaPostGrid = seq(4.8, 5.8, length = 10000))\n\nAs expected, the posterior for the mean number of bids is concentrated on a larger number of bids. People like to bid on items where the seller‚Äôs reservation price is low.\nIs the first for these auctions improved? Yes it is, although there is still room for improvement:\n\n# Plot the fit for low bids\nplotPoissonFit(y = nBidsLow, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\nAnalyzing the auctions with high reservation prices\nBelow are the results for the auction with high reservation bids. The prior is here set to \\(\\theta \\sim \\mathrm{Gamma}(1,1/2)\\) implying less on average.\n\n# Auctions with high reservation prices:\nnBidsHigh = nBids[eBayData$MinBidShare&gt;0]\nPostPoisson(y = nBidsHigh, alphaPrior = 1, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 12, length = 10000), thetaPostGrid = seq(1.3, 1.8,length = 10000))\n\nAnd the fit is not perfect for these bids, but better than before.\n\n# Plot the fit for high bids\nplotPoissonFit(y = nBidsHigh, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\nSo, separating the bids into dataset with low and high reservation prices makes the Poisson model a lot better for the data. Later in the book, we will use a Poisson regression with reservation price as one of the features, which an even more fine grained analysis."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "",
    "text": "This tutorial gives a very brief introduction to state-space models, along with inference methods like Kalman filtering, smoothing and forecasting. The methods are illustrated using the R package dlm , exemplified with the local level model fitted to the well-known Nile river data. The tutorial is also sprinkled with some cool interactivity in Javascript."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#piecewise-constant-model",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#piecewise-constant-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Piecewise constant model",
    "text": "Piecewise constant model\nAn extremely simple model for a time series is to treat the observations as independent normally distributed with the same mean \\(\\mu\\) and variance \\(\\sigma_\\varepsilon\\)\n\\[\ny_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\n\nShow the code\n#install.packages(\"latex2exp\")\nlibrary(latex2exp)\nn = 200\nmu = 2\nsigma_eps = 1\ny = rnorm(n, mean = mu, sd = sigma_eps)\nplot(seq(1,n), y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y_t\", lwd = 1.5,\n    main = \"Simulated data from the naive iid model\")\nlines(seq(1,n), rep(mu,n), type = \"l\", col = \"orange\")\nlegend(\"topright\", legend = c(TeX(\"$y_t$\"), TeX(\"$\\\\mu$\")), lty = 1, lwd = 1.5, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\nThis model is of course not something to write home about, it basically ignores the time series nature of the data. Let us start to make it a little more interesting by allowing the mean to vary of time. This means that we will have a time-varying parameter model where the mean \\(\\mu_t\\) changes (abruptly) at certain time points \\(t_1, t_2, \\dots, t_K\\):\n\\[\ny_t = \\mu_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\\[\n\\begin{align}   \n\\mu_t &=\n\\begin{cases}            \n  \\mu_1 & \\text{if $1 \\leq t \\leq t_1$} \\\\\n  \\mu_2 & \\text{if $t_1 &lt; t \\leq t_2$} \\\\            \n  \\vdots & \\vdots \\\\\n  \\mu_K & \\text{if $t_{K-1} &lt; t \\leq T$}. \\\\          \n\\end{cases}\n\\end{align}\n\\]\nHere is a widget that lets you simulate data from the piecewise constant model1."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#local-level-model",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#local-level-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Local level model",
    "text": "Local level model\nThe piecewise constant model has a few abrupt changes in the mean, but what if the mean changes more gradually? The local level model has a constantly changing mean following a random walk model:\n\\[y_t = \\mu_t + \\varepsilon_t,\\qquad \\varepsilon_t \\sim N(0,\\sigma_\\varepsilon^2)\\]\n\\[\\mu_t = \\mu_{t-1} + \\eta_t,\\qquad \\eta_t \\sim N(0,\\sigma_\\eta^2)\\]\nwhich models the observed time series \\(y_t\\) as a mean \\(\\mu_t\\) plus a random measurement error or disturbance \\(\\varepsilon_t\\). The mean \\(\\mu_t\\) evolves over time as a random walk driven by innovations \\(\\eta_t\\).\nHere is a widget that simulates data from the model. Go ahead, experiment with the measurement/noise \\(\\sigma_\\varepsilon\\) and the standard deviation of the innovations to the mean process, \\(\\sigma_\\eta\\). For example, drive \\(\\sigma_\\eta\\) toward zero and note how the mean becomes close to constant over time."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#regression-with-time-varying-parameters",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#regression-with-time-varying-parameters",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Regression with time-varying parameters",
    "text": "Regression with time-varying parameters\nThe usual simple linear time series regression model is\n\\[\ny_t = \\alpha + \\beta x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2) \\qquad t=1,\\ldots,T\n\\]\nwhere \\(y_t\\) is a time series response variable (for example electricity price) that is being explained by the explanatory variable \\(x_t\\) (for example temperature). This model assumes that the parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma_\\varepsilon\\) are constant in time, that the relationship between electricity price and temperature has remained the same throughout the whole observed time period.\nIt sometimes makes sense to let the parameters vary with time. Here is one such model, the time-varying regression model:\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nwhere the intercept \\(\\alpha\\) now has a time \\(t\\) subscript and evolves in time following a random walk process\n\\[\\alpha_{t} = \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)\\]\nso that in every time period, the intercept changes by adding on an innovation \\(\\eta_t\\) drawn from a normal distribution with standard deviation \\(\\sigma_\\alpha\\). This standard deviation therefore controls how much the intercept changes over time. The slope \\(\\beta\\) changes over time in a similar fashion, with the speed of change determined by \\(\\sigma_\\beta\\).\nHere is a widget that simulates data from the time-varying regression above. By moving the slider (show regline at time) you can plot the regression line \\(\\alpha_t + \\beta_t x_t\\) at any time period \\(t\\). The plot highlights (darker blue) data points that are closer in time to the time chosen by the slider. To the left you can see the whole time path of the simulated \\(\\alpha\\) and \\(\\beta\\) with the current parameters highlighted by dots."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#state-space-model---filtering-smoothing-and-forecasting",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#state-space-model---filtering-smoothing-and-forecasting",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "State-space model - filtering, smoothing and forecasting",
    "text": "State-space model - filtering, smoothing and forecasting\n\nThe state space model\nAll of the models above, and many, many, many more can be written as a so called state-space model. A state-space model for a univariate time series \\(y_t\\) with a state vector \\(\\boldsymbol{\\theta}_t\\) can be written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\] where we have written the multivariate distribution \\(N(\\boldsymbol{0},\\boldsymbol{V})\\) for \\(v_t\\), even though it is actually a scalar here, to be consistent with the notation used later.\nFor example, the local level model is a state-space model with a single scalar state variable \\(\\boldsymbol{\\theta}_t = \\mu_t\\) and parameters\n\\[\n\\begin{align}\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\nWe learn about the state \\(\\mu_t\\) from the observed time series \\(y_t\\) . The first equation is often called the observation or measurement model since it gives the connection between the unobserved state and the observed measurements. The measurements can also be a vector, but we will use a single measurement in this tutorial. The second equation is called the state transition model since it determines how the state evolves over time.\nWe can even let the state-space parameters \\(\\boldsymbol{F}, \\boldsymbol{G}, \\boldsymbol{V}, \\boldsymbol{W}\\) be different in every time period. This is in fact needed if we want to write the time-varying regression model in state-space form. Recall the time varying regression model\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nWe can tuck the two time-varying parameters in a vector \\(\\boldsymbol{\\beta}=(\\alpha_t,\\beta_t)^\\top\\) and also write the models as\n\\[\n\\begin{align}  \ny_t &= \\boldsymbol{x}_t^\\top\\boldsymbol{\\beta}_{t}   + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\    \n\\boldsymbol{\\beta}_{t} &= \\boldsymbol{\\beta}_{t-1} + \\boldsymbol{w}_t, \\quad \\quad \\nu_t \\sim N(0, \\boldsymbol{W})\n\\end{align}\n\\]\nwhere\n\\[\n\\begin{align}  \n\\boldsymbol{x}_t &= (1,x_t)^\\top  \\\\    \n\\boldsymbol{w}_t &= (\\eta_t,\\nu_t)^\\top  \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\eta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nNote that this is a state-space model with\n\\[\n\\begin{align}\n\\boldsymbol{F}_t &= \\boldsymbol{x}_t\\\\\n\\boldsymbol{G} &=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix} \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\beta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nand note now that \\(\\boldsymbol{F}\\) changes in every time period, hence the subscript \\(t\\).\nFinally, we can also have multivariate response vector \\(\\boldsymbol{y}_t\\)\nas\n\\[\n\\begin{align}\n\\boldsymbol{y}_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + \\boldsymbol{v}_t,\\hspace{1.5cm} \\boldsymbol{v}_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\n\n\nFiltering and smoothing\nThere are two different types of relevant inferences in state-space models: filtering and smoothing:\n\nThe filtered estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(t\\).\nThe smoothed estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|T}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(T\\), the end of the time series.\n\nThe filtered estimate is therefore the instantaneous estimate, giving the best estimate of the current state. The smoothed estimate is the retrospective estimate that looks back in time and gives us the best estimate using all the data.\nFiltering means to compute the sequence of instantaneous estimates of the unobserved state at every time point \\(t=1,2,\\ldots,T\\)\n\\[\n\\hat{\\boldsymbol{\\theta}}_{1|1},\\hat{\\boldsymbol{\\theta}}_{2|2},\\ldots,\\hat{\\boldsymbol{\\theta}}_{T|T}\n\\]\nWe will take a time series and compute the filtered estimates for the whole time series, but it is important to understand that filtering is often done in real-time, which means it is a continuously ongoing process that returns filtered estimates of the state \\(\\boldsymbol{\\theta}_t\\) as time progresses and new measurements \\(y_t\\) come in. Think about a self-driving car that is continuously trying to understand the environment (people, other cars, the road conditions etc). The environment is the state and the car uses its sensors to collect measurements. The filtering estimates tells the car about the best guess for the environment at every point in time.\nFor state-space models of the type discussed here (linear measurement equation and linear evolution of the state, with independent Normal measurement errors and state innovations), the filtered estimates can be computed with one of the most famous algorithms in statistics: the Kalman filter.\nThe Kalman filter is a little messy to write up if you are shaky on vectors and matrices, but we will do it for completeness. We will however use a package for it so don‚Äôt worry if the linear algebra is intidimating. We will use the notation $\\(\\boldsymbol{\\mu}_{t|t}\\) instead of \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\), but they really mean the same.\n\ntime \\(t = 0\\). The Kalman filter starts with mean \\(\\boldsymbol{\\mu}_{0|0}\\) and covariance matrix \\(\\boldsymbol{\\Omega}_{0|0}\\) for the state at time \\(t=0\\). Think about \\(\\boldsymbol{\\mu}_{0|0}\\) as the best guess \\(\\boldsymbol{\\theta}_0\\) of the state vector at time \\(t=0\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\) representing how sure we can be about this guess2.\ntime \\(t = 1\\). The Kalman filter then uses the first measurement \\(y_1\\) to update \\(\\boldsymbol{\\mu}_{0|0} \\rightarrow \\boldsymbol{\\mu}_{1|1}\\) and \\(\\boldsymbol{\\Omega}_{0|0} \\rightarrow \\boldsymbol{\\Omega}_{1|1}\\) to represent the estimate and the uncertainty for \\(\\boldsymbol{\\theta}_1\\), the state at time \\(t=1\\).\ntime \\(t = 2,...,T\\). It then continues in this fashion using the next measurement \\(y_2\\) to compute \\(\\boldsymbol{\\mu}_{2|2}\\) and \\(\\boldsymbol{\\Omega}_{2|2}\\) and so on all the way to the end of the time series to finally get \\(\\boldsymbol{\\mu}_{T|T}\\) and \\(\\boldsymbol{\\Omega}_{T|T}\\).\n\nHere is the Kalman filter algorithm:\n\n\nInitialization: set \\(\\boldsymbol{\\mu}_{0|0}\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\)\nfor \\(t=1,\\ldots,T\\) do\n\nPrediction update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t-1} &= \\boldsymbol{G} \\boldsymbol{\\mu}_{t-1|t-1} \\\\  \n\\boldsymbol{\\Omega}_{t|t-1} &= \\boldsymbol{G}\\boldsymbol{\\Omega}_{t-1|t-1}  \\boldsymbol{G}^\\top + \\boldsymbol{W}\n\\end{align}\n\\]\nMeasurement update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t} &= \\boldsymbol{\\mu}_{t|t-1} + \\boldsymbol{K}_t ( y_t - \\boldsymbol{F} \\boldsymbol{\\mu}_{t|t-1}  )  \\\\  \n\\boldsymbol{\\Omega}_{t|t} &= (\\boldsymbol{I} - \\boldsymbol{K}_t \\boldsymbol{F} )\\boldsymbol{\\Omega}_{t|t-1}\n\\end{align}\n\\]\n\n\nwhere \\[\\boldsymbol{K}_t = \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top ( \\boldsymbol{F} \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top + \\boldsymbol{V})^{-1}\\] is the Kalman Gain.\n\nThe widget below lets you experiment with the Kalman filter for the local level model fitted to the Nile river data. In the widget we infer (filter) the local levels \\(\\mu_1,\\mu_2,\\ldots,\\mu_T\\) and can experiment with the measurement standard deviation \\(\\sigma_\\varepsilon\\), the standard deviation of the innovations to the local mean \\(\\sigma_\\eta\\), and also the initial guess for \\(\\mu_0\\) and the standard deviation \\(\\sigma_0\\) of that guess.\nHere are few things to try out in the widget below:\n\nIncrease the measurement standard deviation \\(\\sigma_\\varepsilon\\) and note how the filtered mean pays less and less attention to changes in the data (because the model believes that the data is very poor quality (noisy) and tells us basically nothing about the level). Then move \\(\\sigma_\\varepsilon\\) to smaller values and note how the filtered mean starts chasing the data (because the model believes that the data are super informative about the level).\nMake the standard deviation for the initial level \\(\\sigma_0\\) very small and then change the initial mean \\(\\mu_0\\) to see how this affects the filtered mean at the first part of the time series.\nMove the standard deviation of the innovations to the level \\(\\sigma_\\eta\\) small and note how the filtered mean becomes smoother and smoother over time."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#the-dlm-package-in-r",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#the-dlm-package-in-r",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "The dlm package in R",
    "text": "The dlm package in R\nThe dlm package is a user-friendly R package for analyzing some state-space models. The package has a nice vignette that is worth reading if you plan to use the package more seriously.\n\nFiltering\nLet‚Äôs first do some filtering in the dlm package. Start by loading the dlm package:\n\n#install.packages(\"dlm\") # uncomment the first time to install.\nlibrary(dlm)\n\nWe now need to tell the dlm package what kind of state-space model we want to estimate. The means setting up the matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\). We will keep it simple and use the local level model as example, where all parameter matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\) are scalars (single numbers). As we have seen above, the local level model corresponds to a state-space model with parameters\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\eta^2\n\\end{align}\n\\]\nSo we only need to set \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) to start the fun. We will for now set \\(\\sigma_\\varepsilon^2 = 100^2\\) and \\(\\sigma_\\eta^2 = 100^2\\), and return to this when we learn how the dlm package can find maximum likelihood estimates for these parameters. Here is how you setup the local level model in the dlm package:\n\nmodel = dlm(FF = 1, V = 100^2, GG = 1, W = 100^2, m0 = 1000, C0 = 1000^2)\n\nCompute the filtering estimate using the Kalman filter and plot the result\n\nnileFilter &lt;- dlmFilter(Nile, model)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nParameter estimation by maximum likelihood\nThe parameters \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) were just set to some values above. Let‚Äôs instead estimate them by maximum likelihood. The function dlmMLE does this for us, but we need to set up a model build object so the dlm package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.\n\n modelBuild &lt;- function(param) {\n   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 1000, C0 = 1000^2)\n }\n fit &lt;- dlmMLE(Nile, parm = c(10,10), build = modelBuild)\n\nWe need to take the exponential of the estimates to get the estimated variance parameters.\n\n exp(fit$par)\n\n[1] 15101.488  1467.014\n\n\nor the square roots, to get the maximum likelihood estimates of the standard deviations\n\nsqrt(exp(fit$par))\n\n[1] 122.88811  38.30162\n\n\nWe can redo the filter, this time using the maximum likelihood estimates of the parameters:\n\nmodel_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 1000, C0 = 1000^2)\nnileFilter &lt;- dlmFilter(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lwd = 1.5, lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\nSmoothing\nWe can also use the dlm package to compute the smoothed retrospective estimates of the local level \\(\\mu_t\\) at time \\(t\\) using all the data from \\(t=1\\) until the end of the time series \\(T\\). We haven‚Äôt showed the mathematical algorithm for smoothing, but you can look it up in many books. Anyway, here is the smoothing results for the Nile data, using the function dlmSmooth from the dlm package. The filtered estimates are also shown.\n\nnileSmooth &lt;- dlmSmooth(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlines(dropFirst(nileSmooth$s), type = 'l', col = \"red\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\",\"Smoothed\"), lty = 1, lwd = 1.5, col = c(\"steelblue\", \"orange\", \"red\"))\n\n\n\n\n\n\nForecasting\nWe can also use state-space models for forecasting. Here is how it is done in the dlm package.\n\nnileFore &lt;- dlmForecast(nileFilter, nAhead = 5)\nsqrtR &lt;- sapply(nileFore$R, function(x) sqrt(x))\npl &lt;- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)\npu &lt;- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)\nx &lt;- ts.union(window(Nile, start = c(1900, 1)),\n              window(nileSmooth$s, start = c(1900, 1)), \n              nileFore$a, pl, pu)\n\nplot(x, plot.type = \"single\", type = 'o', pch = c(NA, NA, NA, NA, NA), lwd = 1.5,\n     col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"),\n     ylab = \"River flow\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\", \"Forecast\", \n    \"90% probability limit\"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, lwd = 1.5,\n    col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"))"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#non-gaussian-state-space-models",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#non-gaussian-state-space-models",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Non-Gaussian state-space models",
    "text": "Non-Gaussian state-space models\n\nPoisson time series model\nA useful model for time series of counts \\(Y \\in \\{0,1,2,\\ldots \\}\\) is a Poisson distribution with time-varying intensity \\(\\lambda_t = \\exp(z_t)\\), where \\(z_t\\) is some continuous stochastic process with autocorrelation, most commonly a random walk:\n\\[\n\\begin{align} y_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= z_{t-1} + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nNote that because of the exponential function \\(\\lambda_t = \\exp(z_t)\\) is guaranteed to be positive for all \\(t\\), as required for the Poisson distribution. It is easily to simulate data from the Poisson time series model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimPoisTimeSeries &lt;- function(T, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the Poisson variables with different intensities, lambda_t = exp(z_t) for each time\n  lambda = exp(z)\n  return (rpois(T, lambda = lambda[2:(T+1)]))\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1) \nT = 100\nsigma_eta = 0.1\ny = simPoisTimeSeries(T, sigma_eta)\nplot(y, type = \"o\", pch = 19, col = \"steelblue\", yaxt = \"n\", xlab = \"time, t\", ylab = \"counts, y\", \n      main = paste(\"A simulated Poisson time series with sigma_eta =\", sigma_eta))\naxis(side = 2, at = seq(0,max(y)))\n\n\n\n\n\nWhat‚Äôs life without widgets? Here is one for a slightly more general Poisson time series model where the random walk is replaced by an autoregressive process of order 1:\n\\[\n\\begin{align}\ny_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= \\mu + \\phi(z_{t-1} -\\mu) + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\n\nThe Poisson time series model is an example of a non-linear (the observations are not linear functions of the state) and non-Gaussian (well, Poisson is not Gaussian) and can therefore not be analyzed with the Kalman filter. There are (approximate) extensions of the Kalman filter and also purely simulation based filtering methods called particle methods. But that is stuff for another day.\n\n\nStochastic volatility models\nMany time series, particularly in the finance, has a variance that is changing over time. Furthermore, it is common to find volatility clustering in the data, meaning that once the the variance is high (turbulent stock market) it tends to remain high for a while and vice versa. The basic stochastic volatility (SV) model tries to capture this:\n\\[\n\\begin{align}\ny_t &= \\mu + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= z_{t-1} + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere we have for simplicity assumed just a constant mean \\(\\mu\\), but we can extend this with and autoregressive process, or basically any model of your preference. The thing that set the SV model apart from the other model presented so far is that the variance of the measurement errors \\(Var(y_t)=Var(\\varepsilon_t) = \\exp(z_t)\\) is heteroscedastic, that is, it varies over time. The variance is driven by the \\(z_t\\) process, which here is modeled as a random walk, which will induce volatility clustering. Note again that we use the exponential function to ensure that the variance is positive for all \\(t\\). The model is Gaussian (only normal distributions), but non-Gaussian (the variance is an exponential of the state) so the Kalman filter can not be used, and we would to resort to other methods for the filtering and smoothing. Here is code to simulate from this basic stochastic volatility model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVol &lt;- function(T, mu, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigma¬≤_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\nLet‚Äôs use that function to simulate a time series and plot it:\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(2) \nT = 100\nmu = 3\nsigma_eta = 1\nsimuldata = simStochVol(T, mu, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\nWe can replace the random walk for the \\(z_t\\) with a more well-behaved AR(1) process:\n\\[\n\\begin{align}\ny_t &= \\mu_y + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= \\mu_z + \\phi(z_{t-1} - \\mu_z) + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere \\(\\mu_y\\) is the mean of the time series \\(y\\) and \\(\\mu_z\\) is the mean of the (log) variance process \\(z_t\\). The parameter \\(\\mu_z\\) therefore determines how much variance \\(y_t\\) has on average and \\(\\phi\\) determines how much volatility clustering there is. A \\(\\phi\\) close to 1 gives long periods of persistently large or small variance. Here is the code:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVolAR &lt;- function(T, mu_y, mu_z, phi, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = mu_z + phi*(z[t-1] - mu_z) + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigma¬≤_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu_y, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1)\nT = 1000\nmu_y = 3\nmu_z = -1\nphi = 0.95\nsigma_eta = 1\nsimuldata = simStochVolAR(T, mu_y, mu_z, phi, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\nWidget time!"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#bonus-implementing-the-kalman-filter-from-scratch",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#bonus-implementing-the-kalman-filter-from-scratch",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Bonus: Implementing the Kalman filter from scratch",
    "text": "Bonus: Implementing the Kalman filter from scratch\nFor the curious, the code below implements the Kalman filter from scratch in R. Let us first implement a function kalmanfilter_update that does the update for a single time step:\n\nkalmanfilter_update &lt;- function(mu, Omega, y, G, C, V, W) {\n  \n  # Prediction step - moving state forward without new measurement\n  muPred &lt;- G %*% mu\n  omegaPred &lt;- G %*% Omega %*% t(G) + W\n  \n  # Measurement update - updating the N(muPred, omegaPred) prior with the new data point\n  K &lt;- omegaPred %*% t(F) / (F %*% omegaPred %*% t(F) + V) # Kalman Gain\n  mu &lt;- muPred + K %*% (y - F %*% muPred)\n  Omega &lt;- (diag(length(mu)) - K %*% F) %*% omegaPred\n  \n  return(list(mu, Omega))\n}\n\nThen we implement a function that does all the Kalman iterations, using the kalmanfilter_update function above:\n\nkalmanfilter &lt;- function(Y, G, F, V, W, mu0, Sigma0) {\n  T &lt;- dim(Y)[1]  # Number of time steps\n  n &lt;- length(mu0)  # Dimension of the state vector\n  \n  # Storage for the mean and covariance state vector trajectory over time\n  mu_filter &lt;- matrix(0, nrow = T, ncol = n)\n  Sigma_filter &lt;- array(0, dim = c(n, n, T))\n  \n  # The Kalman iterations\n  mu &lt;- mu0\n  Sigma &lt;- Sigma0\n  for (t in 1:T) {\n    result &lt;- kalmanfilter_update(mu, Sigma, t(Y[t, ]), G, F, V, W)\n    mu &lt;- result[[1]]\n    Sigma &lt;- result[[2]]\n    mu_filter[t, ] &lt;- mu\n    Sigma_filter[,,t] &lt;- Sigma\n  }\n  \n  return(list(mu_filter, Sigma_filter))\n}\n\nLet‚Äôs try it out on the Nile river data:\n\n# Analyzing the Nile river data\nprettycolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\")\ny = as.vector(Nile)\nV = 100^2\nW = 100^2\nmu0 = 1000\nSigma0 = 1000^2\n\n# Set up state-space model for local level model\nT = length(y)\nG = 1\nF = 1\nY = matrix(0,T,1)\nY[,1] = y\nfilterRes = kalmanfilter(Y, G, F, V, W, mu0, Sigma0)\nmeanFilter = filterRes[[1]]\nstd_filter = sqrt(filterRes[[2]][,,, drop =TRUE])\n\nplot(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\npolygon(c(seq(1:T), rev(seq(1:T))), \n        c(meanFilter - 1.96*std_filter, rev(meanFilter + 1.96*std_filter)), \n        col = \"#F0F0F0\", border = NA)\nlines(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\nlines(seq(1:T), meanFilter, type = \"l\", col = prettycolors[3], lwd = 1.5)\nlegend(\"topright\", legend = c(\"time series\", \"filter mean\", \"95% intervals\"), lty = 1, lwd = 1.5,\n    col = c(prettycolors[1], prettycolors[3], \"#F0F0F0\"))"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#footnotes",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#footnotes",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClicking on the  below the widget will take you to the Observable notebook of the widget where you can also change the locations of the thresholds, \\(t_1,t_2,\\ldots,t_{K-1}\\), if you are really into that sort of thing.‚Ü©Ô∏é\nIts all about that Bayes\nThe Kalman filter is often presented from a frequentist point of view in statistics, where the Kalman filtered estimates are the optimal estimates in the mean square error sense.\n\nThe Kalman filter can also be derived as simple Bayesian updating, using Bayes‚Äô theorem to update the information about the state as a new measurement comes in. The \\(\\boldsymbol{\\mu_{0|0}}\\) and \\(\\boldsymbol{\\Omega_{0|0}}\\) can be seen as the prior mean and prior covariance matrix summarizing your prior information about the state before collecting any measurements.\n\nThe Kalman filter is great. When something is great, Bayes usually lurks in the background! üòú‚Ü©Ô∏é"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/LocalLevelNileDataPython-Copy1.html",
    "href": "notebooks/KalmanFilteringSmoothing/LocalLevelNileDataPython-Copy1.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma, norm\n\n# Get the macro dataset\nnile = pd.read_csv('nile.csv')\n\n\n\"\"\"\nUnivariate Local Linear Trend Model\n\"\"\"\nclass LocalLinearTrend(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 2\n\n        # Initialize the statespace\n        super(LocalLinearTrend, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1, 0])\n        self.ssm['transition'] = np.array([[1, 1],\n                                       [0, 1]])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level', 'sigma2.trend']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*3\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearTrend, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]\n\n\n\"\"\"\nUnivariate Local Linear Level Model\n\"\"\"\nclass LocalLinearLevel(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 1\n\n        # Initialize the statespace\n        super(LocalLinearLevel, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1])\n        self.ssm['transition'] = np.array([[1]])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*2\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearLevel, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]\n\n\nmod = LocalLinearLevel(nile['flow'])\ninitial_state = np.array([1000])  # Example: Modify based on your model's state dimension\ninitial_state_cov = np.eye(1) * 1000**2  # Small non-zero covariance for each state\n\n# Initialize the model with known values\nmod.initialize_known(initial_state, initial_state_cov)\n\nconstraints = {'sigma2.measurement': 100**2, 'sigma2.level': 100**2}\n# Fit the model with constraints\nres = mod.fit_constrained(constraints)\n\n#res = mod.fit(disp=False)\nprint(res.summary())\n\n                           Statespace Model Results                           \n==============================================================================\nDep. Variable:                   flow   No. Observations:                  100\nModel:               LocalLinearLevel   Log Likelihood                -636.763\nDate:                Thu, 29 Feb 2024   AIC                           1273.526\nTime:                        18:47:39   BIC                           1273.526\nSample:                             0   HQIC                          1273.526\n                                - 100                                         \nCovariance Type:                  opg                                         \n==============================================================================================\n                                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nsigma2.measurement (fixed)      1e+04        nan        nan        nan         nan         nan\nsigma2.level (fixed)            1e+04        nan        nan        nan         nan         nan\n===================================================================================\nLjung-Box (L1) (Q):                   2.16   Jarque-Bera (JB):                 0.40\nProb(Q):                              0.14   Prob(JB):                         0.82\nHeteroskedasticity (H):               0.65   Skew:                             0.13\nProb(H) (two-sided):                  0.22   Kurtosis:                         2.83\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nmod = LocalLinearLevel(nile['flow'])\n\nres = mod.fit(disp=False)\nprint(res.summary())\n\n                           Statespace Model Results                           \n==============================================================================\nDep. Variable:                   flow   No. Observations:                  100\nModel:               LocalLinearLevel   Log Likelihood                -632.538\nDate:                Thu, 29 Feb 2024   AIC                           1269.075\nTime:                        18:33:54   BIC                           1274.266\nSample:                             0   HQIC                          1271.175\n                                - 100                                         \nCovariance Type:                  opg                                         \n======================================================================================\n                         coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nsigma2.measurement  1.513e+04   2591.445      5.837      0.000       1e+04    2.02e+04\nsigma2.level        1461.9955    843.753      1.733      0.083    -191.730    3115.721\n===================================================================================\nLjung-Box (L1) (Q):                   1.36   Jarque-Bera (JB):                 0.05\nProb(Q):                              0.24   Prob(JB):                         0.98\nHeteroskedasticity (H):               0.61   Skew:                            -0.03\nProb(H) (two-sided):                  0.16   Kurtosis:                         3.08\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Construct a local level model for inflation\nmod = sm.tsa.UnobservedComponents(nile.flow, 'llevel')\n\n# Fit the model's parameters (sigma2_varepsilon and sigma2_eta)\n# via maximum likelihood\nres = mod.fit()\nprint(res.params)\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method\n\nsigma2.irregular    15078.011658\nsigma2.level         1478.811445\ndtype: float64\n\n\n\nnsimulations = 20\nsimulated_state_kfs = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=dta.index)\nsimulated_state_cfa = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=dta.index)\n\nfor i in range(nsimulations):\n    # Apply KFS simulation smoothing\n    sim_kfs.simulate()\n    # Save the KFS simulated state\n    simulated_state_kfs.iloc[:, i] = sim_kfs.simulated_state[0]\n\n    # Apply CFA simulation smoothing\n    sim_cfa.simulate()\n    # Save the CFA simulated state\n    simulated_state_cfa.iloc[:, i] = sim_cfa.simulated_state[0]\n\n\nmod\n\n&lt;statsmodels.tsa.statespace.structural.UnobservedComponents at 0x70a3e514c130&gt;"
  },
  {
    "objectID": "notebooks/SpamLogisticReg/SpamLogisticReg.html",
    "href": "notebooks/SpamLogisticReg/SpamLogisticReg.html",
    "title": "Posterior approximation - logistic regression",
    "section": "",
    "text": "Load packages\n\n# install.packages(\"mvtnorm\") \n# install.packages(\"RColorBrewer\") \nlibrary(mvtnorm) # package with multivariate normal density\nlibrary(RColorBrewer) # just some fancy colors for plotting\nprettyCol = brewer.pal(10,\"Paired\")\n\n\n\nSettings\n\nchooseCov &lt;- c(1:16) # covariates to include in the model\ntau &lt;- 10;           # Prior std beta~N(0,tau^2*I)\n\n\n\nReading data and setting up the prior\n\nData&lt;-read.table(\"https://raw.githubusercontent.com/mattiasvillani/BayesLearnCourse/master/Notebooks/R/SpamReduced.dat\",header=TRUE) # Reduced spambase data (http://archive.ics.uci.edu/ml/datasets/Spambase/)\ncovNames &lt;- names(Data)[2:length(names(Data))]; # Read off the covariate names\ny &lt;- as.vector(Data[,1]); \nX &lt;- as.matrix(Data[,2:17]);\nX &lt;- X[,chooseCov];                             # Pick out the chosen covariates \ncovNames &lt;- covNames[chooseCov];                # ... and their names\nnPara &lt;- dim(X)[2];\n\n# Setting up the prior\nmu &lt;- as.vector(rep(0,nPara)) # Prior mean vector\nSigma &lt;- tau^2*diag(nPara);\n\n\nCoding up the log posterior function\n\nLogPostLogistic &lt;- function(betaVect,y,X,mu,Sigma){\n  nPara &lt;- length(betaVect);\n  linPred &lt;- X%*%betaVect;\n  logLik &lt;- sum( linPred*y -log(1 + exp(linPred)));\n  logPrior &lt;- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);\n  return(logLik + logPrior)\n}\n\n\n\nFinding the mode and observed information using optim\n\ninitVal &lt;- as.vector(rep(0,nPara)); \nOptimResults&lt;-optim(initVal,LogPostLogistic,gr=NULL,y,X,mu,Sigma,\n  method=c(\"BFGS\"), control=list(fnscale=-1),hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covariance matrix\npostStd &lt;- sqrt(diag(postCov)) # Computing approximate stdev\nnames(postMode) &lt;- covNames      # Naming the coefficient by covariates\nnames(postStd) &lt;- covNames # Naming the coefficient by covariates\n\n\n\n\nThe posterior mode is\n\nprint(postMode)\n\n          our          over        remove      internet          free \n 0.4182337582  1.1753728476  2.9209159589  0.9696191548  1.2944179828 \n          hpl            X.          X..1     CapRunMax   CapRunTotal \n-1.3114765304  0.5673271835  8.2721841199  0.0118045995  0.0005570864 \n        const            hp        george         X1999            re \n-1.4278739763 -2.0411544503 -6.0021765135 -0.4565997686 -0.8577822552 \n          edu \n-1.6854611460 \n\n\n\n\nThe posterior standard deviations are computed from the covariance\n\nprint(postStd)\n\n         our         over       remove     internet         free          hpl \n0.0730320059 0.2321086478 0.3302456199 0.1671111765 0.1412670451 0.4017479109 \n          X.         X..1    CapRunMax  CapRunTotal        const           hp \n0.0947016268 0.6851475429 0.0017545736 0.0001418867 0.0847302222 0.2998192165 \n      george        X1999           re          edu \n1.1494146395 0.1902088194 0.1476136565 0.2554459768 \n\n\n\n\nPlot the marginal posterior of \\(\\beta\\) for the free and hpl covariates\n\npar(mfrow=c(1,2))\ngridVals = seq(postMode['free']-3*postStd['free'], postMode['free']+3*postStd['free'], \n               length = 100)\nplot(gridVals, dnorm(gridVals, mean = postMode['free'], sd = postStd['free']), \n     xlab = expression(beta), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n     lwd = 2, col = prettyCol[2], main = expression(beta[free]))\ngridVals = seq(postMode['hpl']-3*postStd['hpl'], postMode['hpl']+3*postStd['hpl'], \n               length = 100)\nplot(gridVals, dnorm(gridVals, mean = postMode['hpl'], sd = postStd['hpl']), \n     xlab = expression(beta), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n     lwd = 2, col = prettyCol[2], main = expression(beta[hpl]))\n\n\n\n\n\n\nSimulate from normal approximation and make prediction at mean covariate\n\nxStar = colMeans(X)\nnSim = 1000\nprobSpam = rep(0,nSim)\nspamPred = rep(0,nSim)\nfor (i in 1:nSim){\n  betaDraw = as.vector(rmvnorm(1, postMode, postCov)) # Simulate a beta draw from approx post\n  linPred = t(xStar)%*%betaDraw\n  probSpam[i] = exp(linPred)/(1+exp(linPred)) # draw from posterior of Pr(spam|x)\n  spamPred[i] = rbinom(n=1,size=1,probSpam[i]) # draw from model given probSpam[i]\n}\npar(mfrow=c(1,2))\nhist(probSpam, freq = FALSE, xlab = expression(theta[i]), ylab= \"\", col = prettyCol[3],\n     main = \"Posterior distribution for Pr(spam|x)\", cex.main = 0.7)\nbarplot(c(sum(spamPred==0),sum(spamPred==1))/nSim, names.arg  = c(\"ham\",\"spam\"), col = prettyCol[7],\n     main = \"Predictive distribution spam\", cex.main = 0.7)"
  },
  {
    "objectID": "notebooks/SpamBern/SpamBernR.html",
    "href": "notebooks/SpamBern/SpamBernR.html",
    "title": "Analyzing email spam data with a Bernoulli model",
    "section": "",
    "text": "a notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nThe SpamBase dataset from the UCI repository consists of \\(n=4601\\) emails that have been manually classified as spam (junk email) or ham (non-junk email).\nThe dataset also contains a vector of covariates/features for each email, such as the number of capital letters or $-signs; this information can be used to build a spam filter that automatically separates spam from ham.\nThis notebook analyzes only the proportion of spam emails without using the covariates.\n\n\nGetting started\nFirst, load libraries and setting up colors.\n\noptions(repr.plot.width=16, repr.plot.height=5, lwd = 4)\nlibrary(\"RColorBrewer\") # for pretty colors\nlibrary(\"tidyverse\")    # for string interpolation to print variables in plots.\nlibrary(\"latex2exp\")    # the TeX() function makes it possible to print latex math\ncolors = brewer.pal(12, \"Paired\")[c(1,2,7,8,3,4,5,6,9,10)];\n\n\n\nData\n\ndata = read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\", sep=\",\", header = TRUE)\nspam = data$X1 # This is the binary data where spam = 1, ham = 0.\nn = length(spam)\nspam = sample(spam, size = n) # Randomly shuffle the data.\n\n\n\nModel, Prior and Posterior\nModel \\[ X_1,\\ldots,X_n | \\theta \\sim \\mathrm{Bern}(\\theta)\\]\nPrior \\[\\theta\\sim\\mathrm{Beta}(\\alpha,\\beta)\\]\nPosterior \\[\\theta | x_1,\\ldots,x_n \\sim\\mathrm{Beta}(\\alpha+s,\\beta+f),\\]\nwhere \\(s=\\sum_{i=1}^n\\) is the number of ‚Äòsuccesses‚Äô (spam) and \\(f=n-s\\) is the number of ‚Äòfailures‚Äô (ham).\nLet us define a function that computes the posterior and plots it.\n\nBernPost &lt;- function(x, alphaPrior, betaPrior, legend = TRUE){\n    thetaGrid = seq(0,1, length = 1000)\n    n = length(x)\n    s = sum(x)\n    f = n - s\n    alphaPost = alphaPrior + s\n    betaPost = betaPrior + f\n    priorPDF = dbeta(thetaGrid, alphaPrior, betaPrior)\n    normLikePDF = dbeta(thetaGrid, s + 1, f + 1) # Trick to get the normalized likelihood\n    postPDF = dbeta(thetaGrid, alphaPost, betaPost)\n    \n    plot(1, type=\"n\", axes=FALSE, xlab = expression(theta), ylab = \"\", \n         xlim=c(min(thetaGrid),max(thetaGrid)), \n         ylim = c(0,max(priorPDF,postPDF,normLikePDF)), \n         main = TeX(sprintf(\"Prior: $\\\\mathrm{Beta}(\\\\alpha = %0.0f, \\\\beta = %0.0f)\", alphaPrior, betaPrior)))\n    axis(side = 1)\n    lines(thetaGrid, priorPDF, type = \"l\", lwd = 4, col = colors[6])\n    lines(thetaGrid, normLikePDF, lwd = 4, col = colors[2])\n    lines(thetaGrid, postPDF, lwd = 4, col = colors[4])\n    if (legend){\n        legend(x = \"topleft\", inset=.05,\n           legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),  \n           lty = c(1, 1, 1), pt.lwd = c(3, 3, 3), \n           col = c(colors[6], colors[2], colors[4]))\n    }\n    cat(\"Posterior mean is \", round(alphaPost/(alphaPost + betaPost),3), \"\\n\")\n    cat(\"Posterior standard deviation is \", round(sqrt(  alphaPost*betaPost/( (alphaPost+betaPost)^2*(alphaPost+betaPost+1))),3), \"\\n\")\n    return(list(\"alphaPost\" = alphaPrior + s, \"betaPost\" = betaPrior + f))\n}\n\nLet start by analyzing only the first 10 data points.\n\nn = 10\nx = spam[1:n]\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nSince we only have \\(n=10\\) data points, the posteriors for the three different priors differ a lot. Priors matter when the data are weak. Let‚Äôs try with the \\(n=100\\) first observations.\n\nn = 100\nx = spam[1:n]\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nThe effect of the prior is now almost gone. Finally let‚Äôs use all \\(n=4601\\) observations in the dataset:\n\nx = spam\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nWe see two things: * The effect of the prior is completely gone. All three prior give identical posteriors. We have reached a subjective consensus among the three persons. * We are quite sure now that the spam probability \\(\\theta\\) is around \\(0.4\\).\nA later notebook will re-analyze this data using for example logistic regression."
  },
  {
    "objectID": "notebooks/SurveyMultinomial/multinomial.html",
    "href": "notebooks/SurveyMultinomial/multinomial.html",
    "title": "Bayesian analysis of multinomial data",
    "section": "",
    "text": "Prepared for the course: Bayesian Learning Author: Mattias Villani, Stockholm and Link√∂ping University, http://mattiasvillani.com\n\nData\nA company has conduct a survey of mobile phone usage. 513 participants were asked the question: ‚ÄòWhat kind of mobile phone do you main use?‚Äô with the four options:\n\niPhone\nAndroid Phone\nWindows Phone\nOther\n\nThe responses in the four categories were: 180, 230, 62, 41.\n\n\nModel\n\\[\n(y_1,\\ldots,y_4) \\vert \\theta_1,\\ldots,\\theta_4 \\sim \\mathrm{multinomial}(\\theta_1,\\ldots,\\theta_4)\n\\]\n\n\nPrior\nThe conjugate prior for multinomial data is the Dirichlet prior.\n\\[\n(\\theta_1,\\ldots,\\theta_K) \\sim \\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_K),\n\\] where the \\(\\alpha_k\\) are positive hyperparameters such that \\(\\mathbb{E}(\\theta_k) = \\alpha_k /\\sum_{j=1}^K \\alpha_j\\). Also, the sum of the \\(\\alpha\\)‚Äôs, \\(\\sum_{k=1}^K \\alpha_j\\), determines the precision (inverse variance) of the Dirichlet distribution. We will determine the prior hyperparameters from data from a similar survey that was conducted four year ago. The proportions in the four categories back then were: 30%, 30%, 20% and 20%. This was a large survey, but since time has passed and user patterns most likely has changed, we value the information in this older survey as being equivalent to a survey with only 50 participants. This gives us the prior: \\[\n(\\theta_1,\\ldots,\\theta_4) \\sim \\mathrm{Dirichlet}(\\alpha_1 = 15,\\alpha_2 = 15,\\alpha_3 = 10,\\alpha_4=10)\n\\]\nnote that \\(\\mathbb{E}(\\theta_1) = 15/50 = 0.3\\) and so on, so the prior mean is set equal to the proportions from the older survey. Also, \\(\\sum_{k=1}^4 \\alpha_k = 50\\), so the prior information is equivalent to a survey based on 50 respondents, as required.\n\n\nPosterior\n\\[(\\theta_1,\\ldots,\\theta_K) \\vert \\mathbf{y} \\sim \\mathrm{Dirichlet}(\\alpha_1 + y_1,\\ldots,\\alpha_K + y_K)\\] We can easily simulate from a Dirichlet distribution since if \\(x_k \\sim \\mathrm{Gamma}(\\alpha_k,1)\\) for \\(k=1,\\ldots,K\\), then the vector \\((z_1,\\ldots,z_K)\\) where \\(z_k = y_k /\\sum_{j=1}^K y_k\\), can be shown to follow the \\(\\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_K)\\) distribution. The code below in a (inefficient) implementation of this simulator.\n\nSimDirichlet &lt;- function(nIter, param){     \n  nCat &lt;- length(param)     \n  thetaDraws &lt;- as.data.frame(matrix(NA, nIter, nCat)) # Storage.   \n  for (j in 1:nCat){        \n    thetaDraws[,j] &lt;- rgamma(nIter,param[j],1)  \n  }     \n  for (i in 1:nIter){       \n    thetaDraws[i,] = thetaDraws[i,]/sum(thetaDraws[i,]) \n  }     \n  return(thetaDraws) \n}\n\nOk, let‚Äôs use this piece of code on the survey data to obtain a sample from the posterior.\n\n# Data and prior\nset.seed(123) # Set the seed for reproducibility\ny &lt;- c(180,230,62,41) # The cell phone survey data (K=4)\nalpha &lt;- c(15,15,10,10) # Dirichlet prior hyperparameters \nnIter &lt;- 1000 # Number of posterior draws\nthetaDraws &lt;- SimDirichlet(nIter,y + alpha)\nnames(thetaDraws) &lt;- c('theta1','theta2','theta3','theta4')\nhead(thetaDraws)\n\n     theta1    theta2    theta3     theta4\n1 0.3530702 0.4539865 0.1079301 0.08501313\n2 0.3676441 0.4020532 0.1406601 0.08964270\n3 0.3347276 0.4782798 0.1030357 0.08395696\n4 0.3395357 0.4487847 0.1217003 0.08997931\n5 0.3769253 0.4175829 0.1180137 0.08747814\n6 0.3607870 0.4186397 0.1275708 0.09300241\n\n\nSo thetaDraws is a nIter-by-4 matrix, where the \\(k\\)th column holds the posterior draws for \\(\\theta_k\\). We can now approximate the marginal posterior of \\(\\theta_k\\) by a histogram or a a kernel density estimate.\n\npar(mfrow = c(1,2)) # Splits the graphical window in four parts\nhist(thetaDraws[,1], breaks = 25, xlab = 'Fraction IPhone users', main ='iPhone', freq = FALSE)  \nlines(density(thetaDraws[,1]), col = \"blue\", lwd = 2)\nhist(thetaDraws[,2], breaks = 25, xlab = 'Fraction Android users', main ='Android', freq = FALSE)\nlines(density(thetaDraws[,2]), col = \"blue\", lwd = 2)\n\n\n\n\nWe can also compute the probability that Android has the largest market share by simply the proportion of posterior draws where Android is largest. You can for example see that this was the case in the first six draws shown above. This code does this calculation.\n\n# Computing the posterior probability that Android is the largest\nPrAndroidLargest &lt;- sum(thetaDraws[,2]&gt;apply(thetaDraws[,c(1,3,4)],1,max))/nIter\nmessage(paste('Pr(Android has the largest market share) = ', PrAndroidLargest))\n\nPr(Android has the largest market share) =  0.991"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/LocalTrendNileDataPython.html",
    "href": "notebooks/KalmanFilteringSmoothing/LocalTrendNileDataPython.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma, norm\n\nnile = pd.read_csv('nile.csv')\n\n\n\"\"\"\nUnivariate Local Linear Level Model\n\"\"\"\nclass LocalLinearLevel(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 1\n\n        # Initialize the statespace\n        super(LocalLinearLevel, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1])\n        self.ssm['transition'] = np.array([1])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*2\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearLevel, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]\n\n\nmod = LocalLinearLevel(nile['flow'])\ninitial_state = np.array([1000])  # Example: Modify based on your model's state dimension\ninitial_state_cov = np.eye(1) * 1000**2  # Small non-zero covariance for each state\n\n# Initialize the model with known values\nmod.initialize_known(initial_state, initial_state_cov)\n\nconstraints = {'sigma2.measurement': 100**2, 'sigma2.level': 100**2}\n# Fit the model with constraints\nres = mod.fit_constrained(constraints)\n\n#res = mod.fit(disp=False)\nprint(res.summary())\n\n                           Statespace Model Results                           \n==============================================================================\nDep. Variable:                   flow   No. Observations:                  100\nModel:               LocalLinearLevel   Log Likelihood                -636.763\nDate:                Thu, 29 Feb 2024   AIC                           1273.526\nTime:                        18:53:38   BIC                           1273.526\nSample:                             0   HQIC                          1273.526\n                                - 100                                         \nCovariance Type:                  opg                                         \n==============================================================================================\n                                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nsigma2.measurement (fixed)      1e+04        nan        nan        nan         nan         nan\nsigma2.level (fixed)            1e+04        nan        nan        nan         nan         nan\n===================================================================================\nLjung-Box (L1) (Q):                   2.16   Jarque-Bera (JB):                 0.40\nProb(Q):                              0.14   Prob(JB):                         0.82\nHeteroskedasticity (H):               0.65   Skew:                             0.13\nProb(H) (two-sided):                  0.22   Kurtosis:                         2.83\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method\n\n\nnsimulations = 50000\nsimulated_state_kfs = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=nile.year)\nsimulated_state_cfa = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=nile.year)\n\nfor i in range(nsimulations):\n    # Apply KFS simulation smoothing\n    sim_kfs.simulate()\n    # Save the KFS simulated state\n    simulated_state_kfs.iloc[:, i] = sim_kfs.simulated_state[0]\n\n    # Apply CFA simulation smoothing\n    sim_cfa.simulate()\n    # Save the CFA simulated state\n    simulated_state_cfa.iloc[:, i] = sim_cfa.simulated_state[0]\n\n\nsimulated_state_kfs.iloc[93,:].mean()\n\n1013.6584161831919\n\n\n\nsimulated_state_kfs.iloc[1,:].std()\n\n68.68731718279996"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/Julia_KalmanFilteringSmoothing.html",
    "href": "notebooks/KalmanFilteringSmoothing/Julia_KalmanFilteringSmoothing.html",
    "title": "Kalman filtering and Smoothing for the river nile data",
    "section": "",
    "text": "This notebook analyzes the River Nile data using the local level model with full implementation of the Kalman filter and smoother.\n\nusing Pkg\nPkg.add(\"Plots\");\n\n   Resolving package versions...\n  No Changes to `~/Dropbox/BayesBookWeb/BayesianLearningBook/notebooks/KalmanFilteringSmoothing/Project.toml`\n  No Changes to `~/Dropbox/BayesBookWeb/BayesianLearningBook/notebooks/KalmanFilteringSmoothing/Manifest.toml`\n\n\nDefine the Kalman filter update at a given time point:\n\nfunction kalmanfilter_update(Œº, Œ©, u, y, A, B, C, Œ£‚Çë, Œ£‚Çô)\n\n    # Prediction step - moving state forward without new measurement\n    ŒºÃÑ = A*Œº .+ B*u\n    Œ©ÃÑ = A*Œ©*A' + Œ£‚Çô\n\n    # Measurement update - updating the N(ŒºÃÑ, Œ©ÃÑ) prior with the new data point\n    K = Œ©ÃÑ*C' / (C*Œ©ÃÑ*C' .+ Œ£‚Çë) # Kalman Gain\n    Œº = ŒºÃÑ + K*(y .- C*ŒºÃÑ)\n    Œ© = (I - K*C)*Œ©ÃÑ\n    return Œº, Œ©\n\nend;\n\nDefining a function for the Kalman filter algorithm that calls on the kalmanfilter_update at each time-step:\n\nfunction kalmanfilter(U, Y, A, B, C, Œ£‚Çë, Œ£‚Çô, Œº‚ÇÄ, Œ£‚ÇÄ)\n    \n    T = size(Y,1)   # Number of time steps\n    n = length(Œº‚ÇÄ)  # Dimension of the state vector      \n\n    # Storage for the mean and covariance state vector trajectory over time\n    Œº_filter = zeros(T, n)   \n    Œ£_filter = zeros(n, n, T)\n    \n    # The Kalman iterations\n    Œº = Œº‚ÇÄ\n    Œ£ = Œ£‚ÇÄ\n    for t = 1:T\n        Œº, Œ£ = kalmanfilter_update(Œº, Œ£, U[t,:]', Y[t,:]', A, B, C, Œ£‚Çë, Œ£‚Çô)\n        Œº_filter[t,:] .= Œº\n        Œ£_filter[:,:,t] .= Œ£\n    end\n\n    return Œº_filter, Œ£_filter\n\nend;"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/Python_KalmanFilteringSmoothing.html",
    "href": "notebooks/KalmanFilteringSmoothing/Python_KalmanFilteringSmoothing.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nThe answer is 8"
  },
  {
    "objectID": "notebooks/ebayPoissonOneParam/eBayPoissonPython.html",
    "href": "notebooks/ebayPoissonOneParam/eBayPoissonPython.html",
    "title": "Modeling the number of bids in eBay coin auctions",
    "section": "",
    "text": "an Jupyter notebook for the book Bayesian Learning by Mattias Villani\nThe dataset contains data from 1000 auctions of collector coins. The dataset was collected and first analyzed in the article Bayesian Inference in Structural Second-Price Common Value Auctions.\n\nImport modules and load the data\n\nimport numpy as np\nimport scipy.stats as sps\nimport pandas as pd\nimport matplotlib.pyplot as plt\nnp.random.seed(seed=123) # Set the seed for reproducibility\n\n# Load the data\neBayData = pd.read_csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData['nBids']\n\nWe will model these data using a Poisson distribution: \\[y_1,...,y_n \\vert \\theta \\overset{iid}{\\sim} \\mathrm{Poisson}(\\theta).\\] with a conjugate Gamma prior\n\\[\\theta  \\sim \\mathrm{Gamma}(\\alpha, \\beta)\\]\nso that the posterior is also Gamma:\n\\[\\theta \\vert y_1,\\ldots,y_n \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n y_i, \\beta + n)\\]\n\n# Define the Gamma distribution in the rate parametrization\ndef gammaPDF(x, alpha, beta):\n    return(sps.gamma.pdf(x, a = alpha, scale = 1/beta))\n\n\ndef PostPoisson(y, alphaPrior, betaPrior, thetaPriorGrid = None, thetaPostGrid = None):\n\n    # Compute Prior density and posterior\n    priorDens = gammaPDF(x = thetaPriorGrid, alpha = alphaPrior, beta = betaPrior)\n    n = len(y)\n    alphaPost = alphaPrior + np.sum(y)\n    betaPost = betaPrior + n\n    postDens = gammaPDF(x = thetaPostGrid, alpha = alphaPost, beta = betaPost)\n    \n    print('Number of data points = ' + str(len(y)))\n    print('Sum of number of counts = ' + str(np.sum(y)))\n    print('Mean number of counts = ' + str(np.mean(y)))\n    print('Prior mean = ' + str(alphaPrior/betaPrior))\n    print('Prior standard deviation = '+ str(np.sqrt(alphaPrior/(betaPrior**2))))\n    print('Equal tail 95% prior interval: ' + str(sps.gamma.interval(0.95, a = alphaPrior, scale = 1/betaPrior)))  \n    print('Posterior mean = ' + str(round(alphaPost/betaPost,3)))\n    print('Posterior standard deviation = '+ str(np.sqrt(    (alphaPrior+np.sum(y))/  ((betaPrior+n)**2)  )    ))\n    print('Equal tail 95% posterior interval: ' + str(sps.gamma.interval(0.95, a = alphaPost, scale = 1/betaPost)))  \n\n    if (thetaPriorGrid.any() != None):\n        fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n        h1, = ax[0].plot(thetaPriorGrid, priorDens, lw = 3);\n        ax[0].set_xlabel(r'$\\theta$');ax[0].set_ylabel('PDF');\n        ax[0].set_title('Prior distribution');\n\n        h2, = ax[1].plot(thetaPostGrid, postDens, lw = 3, color =\"orange\");\n        ax[1].set_xlabel(r'$\\theta$');ax[1].set_ylabel('PDF');\n        ax[1].set_title('Posterior distribution');\n\nalphaPrior = 2\nbetaPrior = 1/2\nPostPoisson(y = nBids, alphaPrior = 2, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,12,10000), thetaPostGrid = np.linspace(3.25,4,10000))\n\nNumber of data points = 1000\nSum of number of counts = 3635\nMean number of counts = 3.635\nPrior mean = 4.0\nPrior standard deviation = 2.8284271247461903\nEqual tail 95% prior interval: (0.48441855708793014, 11.143286781877796)\nPosterior mean = 3.635\nPosterior standard deviation = 0.06027740643004339\nEqual tail 95% posterior interval: (3.5179903738284697, 3.7542677655304297)\n\n\n\n\n\n\n\nFit of the Poisson model\nLet‚Äôs plot the data along with the fitted Poisson model. We‚Äôll keep things simple and plot the fit for the posterior mean of \\(\\theta\\).\n\ndef plotPoissonFit(y, alphaPrior, betaPrior):\n    \n    # Plot data\n    maxY = np.max(y)\n    yGrid = np.arange(maxY)\n    probs = [np.sum(y==k)/len(y) for k in range(maxY)]\n    h1 = plt.bar(yGrid, probs, alpha = 0.3);\n    plt.xlabel('y');plt.ylabel('PMF');\n    plt.xticks(yGrid);\n    plt.title('Fitted Poisson model based on posterior mean estimate');\n    \n    # Compute posterior mean\n    n = len(y)\n    alphaPost = alphaPrior + np.sum(y)\n    betaPost = betaPrior + n\n    postMean = alphaPost/betaPost\n    \n    # Plot the fit based on the posterior mean of theta\n    poisFit = sps.poisson.pmf(yGrid, mu = postMean) \n    plt.plot(yGrid, poisFit, color = 'orange', lw = 3)\n\n\n# Plot the fit for all bids\nalphaPrior = 2\nbetaPrior = 1/2\nplotPoissonFit(y = nBids, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\nWow, that‚Äôs are terrible fit! This data does not look at all like a Poisson distribution. What can we do?\n\n\nAnalyzing the auction with low and high reservation prices separately.\nWe will later model the number of bids using a Poisson regression where we take into account several explanatory variables. But, for now, let‚Äôs split the auctions in two subsets:\ni) auctions with low reservation price in relation to the item‚Äôs book value (MinBidShare&lt;=0)\nii) auctions with high reservation price in relation to the item‚Äôs book value (MinBidShare&gt;0)\nLet‚Äôs start with the 550 auction with low reservation prices. The prior for the auction with low reservation prices is set to \\(\\theta \\sim \\mathrm{Gamma}(4,1/2)\\) to reflect a belief that belief that such auctions are likely to attract more bids.\n\n# Auctions with low reservation prices:\nnBidsLow = nBids[eBayData['MinBidShare']&lt;=0]\n\nPostPoisson(y = nBidsLow, alphaPrior = 4, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,25,10000), thetaPostGrid = np.linspace(4.8,5.8,10000))\n\nNumber of data points = 550\nSum of number of counts = 2927\nMean number of counts = 5.321818181818182\nPrior mean = 8.0\nPrior standard deviation = 4.0\nEqual tail 95% prior interval: (2.17973074725265, 17.534546139484647)\nPosterior mean = 5.324\nPosterior standard deviation = 0.0983446153216288\nEqual tail 95% posterior interval: (5.13322503650632, 5.518717305739481)\n\n\n\n\n\nAs expected, the posterior for the mean number of bids is concentrated on a larger number of bids. People like to bid on items where the seller‚Äôs reservation price is low.\nIs the first for these auctions improved? Yes it is, although there is still room for improvement:\n\n# Plot the fit for low bids\nplotPoissonFit(y = nBidsLow, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\nBelow are the results for the auction with high reservation bids. The prior is here set to \\(\\theta \\sim \\mathrm{Gamma}(1,1/2)\\) implying less on average.\n\n# Auctions with high reservation prices:\nnBidsHigh = nBids[eBayData['MinBidShare']&gt;0]\n\nPostPoisson(y = nBidsHigh, alphaPrior = 1, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,12,10000), thetaPostGrid = np.linspace(1.3,1.8,10000))\n\nNumber of data points = 450\nSum of number of counts = 708\nMean number of counts = 1.5733333333333333\nPrior mean = 2.0\nPrior standard deviation = 2.0\nEqual tail 95% prior interval: (0.050635615968579795, 7.377758908227871)\nPosterior mean = 1.574\nPosterior standard deviation = 0.05910555807189499\nEqual tail 95% posterior interval: (1.4600786825716714, 1.6917395497993104)\n\n\n\n\n\nAnd the fit is not perfect for these bids, but better than before.\n\n# Plot the fit for high bids\nplotPoissonFit(y = nBidsHigh, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\nSo, separating the bids into dataset with low and high reservation prices makes the Poisson model a lot better for the data. Later in the book, we will use a Poisson regression with reservation price as one of the features, which an even more fine grained analysis."
  },
  {
    "objectID": "notebooks/DownloadSpeedNormal/DownloadSpeedNormalR.html",
    "href": "notebooks/DownloadSpeedNormal/DownloadSpeedNormalR.html",
    "title": "Analyzing internet download speeds with a Gaussian model",
    "section": "",
    "text": "a notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nThe maximum internet connection speed downstream in my home is 50 Mbit/sec.¬†This maximum will typically never be reached, but my internet service provider (ISP) claims that the average speed is at least 20Mbit/sec.¬†I want to collect some data to investigate this.\n\n\nGetting started\nFirst, some housekeeping: loading libraries and setting up colors.\n\noptions(repr.plot.width=8, repr.plot.height=6, lwd = 4)\nlibrary(\"RColorBrewer\") # for pretty colors\nlibrary(\"tidyverse\")    # for string interpolation to print variables in plots.\nlibrary(\"latex2exp\")    # the TeX() function makes it possible to print latex math\ncolors = brewer.pal(12, \"Paired\")[c(1,2,7,8,3,4,5,6,9,10)];\n\n\n\nData\nI collect a total of five measurements over the course of five consecutive using an speed testing internet service:\n\nx = c(15.77, 20.5, 8.26, 14.37, 21.09)\n\n\n\nModel\nThe measurements are assumed to be \\[x_1,\\ldots,x_n \\overset{\\mathrm{iid}}{\\sim} \\mathrm{N}(\\theta,\\sigma^2),\\] where \\(\\theta\\) is the average speed; we ignore for simplicity that the measurements cannot be negative.\nThe measurements are reported to have a standard deviation of \\(\\sigma=5\\) by speed testing service and we take this as the given \\(\\sigma\\).\n\nsigma2 = 5^2\n\n\n\nPrior\nI will use a prior centered on the average claimed by the ISP, \\(\\mu_0=20\\), with a prior standard deviation of \\(\\tau_0=5\\). My prior beliefs are therefore that \\(\\theta \\in [10,30]\\) with approximately \\(95\\%\\) probability.\n\nmu_0 = 20\ntau2_0 = 5^2\n\n\n\nPosterior\nA normal prior for a normal model gives us a posterior which is also normal:\n\\[ \\theta | \\mathbf{x} \\sim \\mathrm{N}(\\mu_n,\\tau_n^2), \\] where the posterior precision (1/variance) is the sum of the data precision and the prior precision \\[ \\frac{1}{\\tau_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}  \\] and the posterior mean is a weighted average of the sample mean and the prior mean \\[ \\mu_n = w \\bar x + (1-w)\\mu_0\\] where the weight is the relative precision of the data and prior information \\[ w = \\frac{n/\\sigma^2}{n/\\sigma^2 + 1/\\tau_0^2}\\]\nLet‚Äôs write a small function that computes the posterior mean and variance, and plots the prior, likelihood and posterior.\n\npostGaussianIID &lt;- function(x, mu_0, tau2_0, sigma2, thetaGrid, areaPoint){\n    \n    # compute posterior mean and variance\n    n = length(x)\n    tau2_n = 1/(n/sigma2 + 1/tau2_0) \n    w = (n/sigma2)/(n/sigma2 + 1/tau2_0)\n    mu_n = w*mean(x) + (1-w)*mu_0\n    \n    # plot PDFs. Likelihood is normalized.\n    priorPDF = dnorm(thetaGrid, mean = mu_0, sd = sqrt(tau2_0))\n    postPDF = dnorm(thetaGrid, mean = mu_n, sd = sqrt(tau2_n))\n    postProbAbovePoint = 1-pnorm(areaPoint, mean = mu_n, sd = sqrt(tau2_n))\n    normLikePDF = dnorm(thetaGrid, mean = mean(x[1:n]), sd = sqrt(sigma2/n))\n    plot(1, type=\"n\", axes=FALSE, xlab = expression(theta), ylab = \"\", \n         xlim=c(min(thetaGrid),max(thetaGrid)), \n         ylim = c(0,max(priorPDF,postPDF,normLikePDF)))\n    axis(side = 1)\n    polygon(c(thetaGrid[thetaGrid&gt;=areaPoint], max(thetaGrid), areaPoint), \n            c(postPDF[thetaGrid&gt;=areaPoint], 0, 0), \n            col=adjustcolor(colors[4],alpha.f=0.3), border=NA)\n    lines(thetaGrid, priorPDF, type = \"l\", lwd = 4, col = colors[6])\n    lines(thetaGrid, normLikePDF, lwd = 4, col = colors[2])\n    lines(thetaGrid, postPDF, lwd = 4, col = colors[4])\n    legend(x = \"topright\", inset=.05, cex = c(1,1,1,1), \n           legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\", \n           TeX(sprintf(\"$Pr(\\\\theta \\\\geq %2.0f | \\\\mathbf{x}) =  %0.3f$\", areaPoint, postProbAbovePoint))),  \n           lty = c(1, 1, 1, 1), pt.lwd = c(3, 3, 3, 3), \n           col = c(colors[6], colors[2], colors[4], adjustcolor(colors[4],alpha.f=0.3)))\n    cat(\"Posterior mean is \", round(mu_n,3), \"\\n\")\n    cat(\"Posterior standard deviation is \", round(sqrt(tau2_n),3), \"\\n\")\n    cat(\"The weight on the sample mean is \", round(w,3))\n    return(list(\"mu_n\" = mu_n, \"tau2_n\" = tau2_n, \"w\" = w))\n}\n\nLet us start by analyzing just the first observation \\(x_1=15.77\\) using this function.\n\nthetaGrid = seq(0, 40, length = 1000) # Some suitable grid of values to plot over\nareaPoint = 20 # shade the region where theta&gt;= areaPoint (20 in my example)\nn = 1\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  17.885 \nPosterior standard deviation is  3.536 \nThe weight on the sample mean is  0.5\n\n\n\n\n\nWe see that the prior and data information happen to get the same weight (w) in the posterior. That is a coincidence from the fact that the prior variance \\(\\tau_0^2\\) is the same as the data variance \\(\\sigma^2\\).\nMoving on, let‚Äôs add the next measurement to the analysis:\n\nn = 2\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  18.757 \nPosterior standard deviation is  2.887 \nThe weight on the sample mean is  0.667\n\n\n\n\n\nWe now see that the posterior is more affected by the data information than the prior information (w = 0.666).\nFinally, adding all \\(n=5\\) data points gives:\n\nn = 5\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  16.665 \nPosterior standard deviation is  2.041 \nThe weight on the sample mean is  0.833\n\n\n\n\n\nI am now rather sure that my average download speed is less than 20 MBit/sec since the posterior probability of \\(\\theta\\geq20\\) is only \\(0.051\\)."
  },
  {
    "objectID": "interactive.html",
    "href": "interactive.html",
    "title": "Interactive",
    "section": "",
    "text": "This page contains interactive widgets for the book.\n\nChapter 1 - The Bayesics\n\n\n\n\nThe Bernoulli distribution\n\n\n\n\n\nMaximum likelihood iid Bernoulli data\n\n\n\n\n\nBayes‚Äô theorem for events\n\n\n\n\n\n\n\n\nChapter 2 - One-parameter models\n\n\n\n\nBeta distribution\n\n\n\n\n\nBayesian inference for iid Bernoulli data\n\n\n\n\n\nNormal distribution\n\n\n\n\n\nBayesian inference for Gaussian iid data with known variance\n\n\n\n\n\nPoisson distribution\n\n\n\n\n\nGamma distribution\n\n\n\n\n\nBayesian inference for iid Poisson counts\n\n\n\n\n\nExponential distribution\n\n\n\n\n\nBayesian inference for Exponential iid data\n\n\n\n\n\n\n\n\nChapter 3 - Multi-parameter models\n\n\n\n\nMultinomial distribution\n\n\n\n\n\nDirichlet distribution\n\n\n\n\n\nBayesian inference for multinomial data\n\n\n\n\n\n\n\n\nChapter 4 - Priors\n\n\nChapter 5 - Regression\n\n\nChapter 6 - Prediction and Decision making\n\n\nChapter 7 - Normal posterior approximation\n\n\nChapter 8 - Classification\n\n\nChapter 9 - Gibbs sampling\n\n\nChapter 10 - Markov Chain Monte Carlo simulation\n\n\nChapter 11 - Variational inference\n\n\nChapter 12 - Regularization\n\n\nChapter 13 - Mixture models and Bayesian nonparametrics\n\n\nChapter 14 - Model comparison and variable selection\n\n\nChapter 15 - Gaussian processes\n\n\nChapter 16 - Interaction models\n\n\nChapter 17 - Dynamic models and sequential inference\n\n\n\n\nKalman filter and parameter estimation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Learning",
    "section": "",
    "text": "This is the home for the book Bayesian Learning, which is still work in progress. The book is currently used for the course Bayesian Learning at Stockholm University.\nA pdf of the book will always be available, even after the book gets published.\nThe Notebooks tab contains Quarto/Jupyter/Pluto notebooks for the chapters in the book.\nThe Code tab contains code for some algorithms used in the book.\nThe Interactive tab contains interactive Observable widgets.\n\nContents\n\nThe Bayesics\nSingle-parameter models\nMulti-parameter models\nPriors\nRegression\nPrediction and Decision making\nNormal posterior approximation\nClassification\nPosterior simulation\nVariational inference\nRegularization\nModel comparison\nVariable selection\nGaussian processes\nInteraction models\nMixture models\nDynamic models and sequential inference\nAppendix: Some Mathematical results\n\nThanks to all who found typos and error in the book."
  }
]