[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Learning - the books",
    "section": "",
    "text": "Bayesian Learning\nThis book is suitable for advanced undergraduate and master’s level courses, with some of the more advanced chapters more suited for a PhD course, or a specialized master’s level course.\nThe book is currently used for the master’s course Bayesian Learning and the PhD course Advanced Bayesian Learning.\nBayesian Learning - the prequel\nThis book contains all basic mathematics and statistics needed to read the Bayesian Learning book.\nThe prequel book is used on the intermediate statistics course Statistical Theory and Modelling as part of the Master’s Program in Data Science, Statistics and Decision Analysis.\n\n\nContents\n\nThe Bayesics\nSingle-parameter Models\nMulti-parameter Models\nPriors\nLinear Regression\nPrediction and Decision Making\nNormal Posterior Approximation\nClassification and Generalized Regression\nGibbs Sampling\nMarkov Chain Monte Carlo Simulation\nVariational Inference\nRegularization\nMixture Models and Bayesian Nonparametrics\nModel Comparison and Variable Selection\nGaussian Processes\nInteraction Models\nDynamic Models and Sequential Inference\nAppendix: Some Mathematical Results\n\nContents\n\nMathematics\nProbability\nDiscrete random variables\nContinuous random variables\nConvergence and central theorems\nTransformation of random variables\nJoint distributions\nLikelihood inference\nRegression\nTime series\n\n\n\n\nPDF versions of the books will always be available, even after they get published.\nThe exercises tab contains solutions to many of the problems in the book, including the computer-based exercises.\nThe Notebooks tab contains Quarto/Jupyter/Pluto notebooks for the chapters in the book.\nThe Code tab contains code for some algorithms used in the book.\nThe Interactive tab contains interactive Observable widgets.\nThanks to all who found typos and errors in the book."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "This page contains code snippets for algorithms in the book, sometimes in multiple languages. Click on the language icons to view and download the code.\n\nChapter 1 - The Bayesics\n\n\nChapter 2 - One-parameter models\n\n\nChapter 3 - Multi-parameter models\n\n\nChapter 4 - Priors\n\n\nChapter 5 - Regression\n\n\nChapter 6 - Prediction and Decision making\n\n\nChapter 7 - Normal posterior approximation\n\n\nChapter 8 - Classification\n\n\nChapter 9 - Gibbs sampling\n\n\n\nGibbs sampling - multivariate normal\n\n\n\n\n\nGibbs sampling - mixture of normals\n\n\n\n\n\nGibbs sampling - mixture of Poissons\n\n\n\n\n\nGibbs sampling - probit regression\n\n\n\n\n\nGibbs sampling - logistic regression\n\n\n\n\n\nGibbs sampling - autoregressive processes\n\n\n\n\n\n\n\n\nChapter 10 - Markov Chain Monte Carlo simulation\n\n\nChapter 11 - Variational inference\n\n\nChapter 12 - Regularization\n\n\n\nGibbs sampling - linear regression with L2-regularization\n\n\n\n\n\n\n\n\nChapter 13 - Mixture models and Bayesian nonparametrics\n\n\nChapter 14 - Model comparison and variable selection\n\n\nChapter 15 - Gaussian processes\n\n\nChapter 16 - Interaction models\n\n\nChapter 17 - Dynamic models and sequential inference\n\n\n\nKalman filter and parameter estimation"
  },
  {
    "objectID": "interactive.html",
    "href": "interactive.html",
    "title": "Interactive",
    "section": "",
    "text": "This page contains interactive widgets for the book.\n\nChapter 1 - The Bayesics\n\n\n\nThe Bernoulli distribution\n\n\n\n\n\nMaximum likelihood iid Bernoulli data\n\n\n\n\n\nBayes’ theorem for events\n\n\n\n\n\n\n\n\nChapter 2 - One-parameter models\n\n\n\nBeta distribution\n\n\n\n\n\nBayesian inference for iid Bernoulli data\n\n\n\n\n\nNormal distribution\n\n\n\n\n\nBayesian inference for Gaussian iid data with known variance\n\n\n\n\n\nPoisson distribution\n\n\n\n\n\nGamma distribution\n\n\n\n\n\nBayesian inference for iid Poisson counts\n\n\n\n\n\nExponential distribution\n\n\n\n\n\nBayesian inference for Exponential iid data\n\n\n\n\n\n\n\n\nChapter 3 - Multi-parameter models\n\n\n\nMultinomial distribution\n\n\n\n\n\nDirichlet distribution\n\n\n\n\n\nBayesian inference for multinomial data\n\n\n\n\n\n\n\n\nChapter 4 - Priors\n\n\nChapter 5 - Regression\n\n\nChapter 6 - Prediction and Decision making\n\n\nChapter 7 - Normal posterior approximation\n\n\nChapter 8 - Classification\n\n\nChapter 9 - Gibbs sampling\n\n\nChapter 10 - Markov Chain Monte Carlo simulation\n\n\nChapter 11 - Variational inference\n\n\nChapter 12 - Regularization\n\n\nChapter 13 - Mixture models and Bayesian nonparametrics\n\n\nChapter 14 - Model comparison and variable selection\n\n\nChapter 15 - Gaussian processes\n\n\nChapter 16 - Interaction models\n\n\nChapter 17 - Dynamic models and sequential inference\n\n\n\nKalman filter and parameter estimation"
  },
  {
    "objectID": "exercise_solutions.html",
    "href": "exercise_solutions.html",
    "title": "Exercise Solutions - Bayesian Learning book",
    "section": "",
    "text": "Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapter 5 Chapter 6 Chapter 7 Chapter 8 Chapter 9 Chapter 10 Chapter 11 Chapter 12 Chapter 13 Chapter 14 Chapter 15 Chapter 16 Chapter 17"
  },
  {
    "objectID": "observable/distributions.html#beta-distribution",
    "href": "observable/distributions.html#beta-distribution",
    "title": "Distributions",
    "section": "Beta distribution",
    "text": "Beta distribution"
  },
  {
    "objectID": "observable/distributions.html#binomial-distribution",
    "href": "observable/distributions.html#binomial-distribution",
    "title": "Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution"
  },
  {
    "objectID": "observable/distributions.html#multivariate-normal-distribution",
    "href": "observable/distributions.html#multivariate-normal-distribution",
    "title": "Distributions",
    "section": "Multivariate normal distribution",
    "text": "Multivariate normal distribution"
  },
  {
    "objectID": "observable/distributions.html#poisson-distribution",
    "href": "observable/distributions.html#poisson-distribution",
    "title": "Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution"
  },
  {
    "objectID": "observable/distributions.html#scaled-inverse-chi2-distribution",
    "href": "observable/distributions.html#scaled-inverse-chi2-distribution",
    "title": "Distributions",
    "section": "Scaled inverse chi2 distribution",
    "text": "Scaled inverse chi2 distribution"
  },
  {
    "objectID": "observable/distributions.html#student-t-distribution",
    "href": "observable/distributions.html#student-t-distribution",
    "title": "Distributions",
    "section": "Student-t distribution",
    "text": "Student-t distribution"
  },
  {
    "objectID": "notebooks/TitanicLogistic/TitanicLogistic.html",
    "href": "notebooks/TitanicLogistic/TitanicLogistic.html",
    "title": "Bayesian logistic regression for the Titanic data",
    "section": "",
    "text": "Load packages\n\nlibrary(mvtnorm)      # package with multivariate normal density\nlibrary(latex2exp)    # latex maths in plots\n\n\n\nSettings\n\ntau &lt;- 10             # Prior std beta~N(0,tau^2*I)\n\n\n\nRead data and set up prior\n\ndf &lt;- read.csv(\n  \"https://github.com/mattiasvillani/introbayes/raw/main/data/titanic.csv\", \n  header=TRUE) \ny &lt;- as.vector(df[,1])\nX &lt;- df[,c(5, 4, 2)]\nX[, 2] &lt;- 1*(X[,2] == \"female\")\nX[, 3] &lt;- 1*(X[,3] == 1)\nX &lt;- as.matrix(cbind(1,X))\np &lt;- dim(X)[2]\nvarNames = c(\"intercept\", \"age\", \"sex\", \"firstclass\")\n\n# Setting up the prior - choose between the two priors in the book\nnoninformative = TRUE\nif (noninformative){\n  mu &lt;- as.vector(rep(0,p)) # Prior mean vector\n  Sigma &lt;- tau^2*diag(p)\n}else{\n  mu &lt;- c(-1, -1/80, 1, 1)\n  Sigma &lt;- diag(c(0.25, 1/(80^2), 0.5, 1))\n}\n\n\n\nCoding up the log posterior function\n\nLogPostLogistic &lt;- function(betaVect, y, X, mu, Sigma){\n  p &lt;- length(betaVect)\n  linPred &lt;- X%*%betaVect\n  logLik &lt;- sum( linPred*y - log(1 + exp(linPred)))\n  logPrior &lt;- dmvnorm(betaVect, mu, Sigma, log=TRUE)\n  return(logLik + logPrior)\n}\n\n\n\nFinding the mode and observed information using optim\n\ninitVal &lt;- as.vector(rep(0,p)); \nOptimResults&lt;-optim(initVal, LogPostLogistic, gr=NULL, y, X, mu, Sigma,\n  method = c(\"BFGS\"), control=list(fnscale=-1), hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covar matrix\npostStd &lt;- sqrt(diag(postCov))         # Approximate stdev\n\n\n\nPlot the marginal posterior of \\(\\beta\\)\nSince we have approximated the joint posterior \\(\\boldsymbol{\\beta}\\) as multivariate normal, the marginal posterior for each \\(\\beta_j\\) is univariate normal.\n\npar(mfrow=c(2,2))\nfor (j in 1:4){\n  gridVals = seq(postMode[j] - 3*postStd[j], postMode[j] + 3*postStd[j], \n                 length = 100)\n  plot(gridVals, dnorm(gridVals, mean = postMode[j], sd = postStd[j]), \n    xlab = TeX(sprintf(r'($\\beta_%d$)', j-1)), ylab= \"posterior density\", \n    type =\"l\", bty = \"n\", lwd = 2, col = \"cornflowerblue\", main =varNames[j])\n}\n\n\n\n\n\n\n\n\n\n\nPlot the marginal posterior of the odds \\(\\exp(\\beta)\\)\nSince the marginal posterior for each \\(\\beta_j\\) is approximated as normal, the approximate posterior distribution for the odds \\(\\exp(\\beta_j)\\) is log-normal.\n\npar(mfrow=c(2,2)) \nfor (j in 1:4){\n  gridVals = exp(seq(postMode[j] - 3*postStd[j], postMode[j] + 3*postStd[j], \n                     length = 100))\n  plot(gridVals, dlnorm(gridVals, meanlog = postMode[j], sdlog = postStd[j]), \n    xlab = TeX(sprintf(r'($\\exp(\\beta_%d$))', j-1)), ylab= \"posterior density\", \n    type =\"l\", bty = \"n\",  lwd = 2, col = \"cornflowerblue\", main = varNames[j])\n}"
  },
  {
    "objectID": "notebooks/DownloadSpeedNormal/DownloadSpeedNormalR.html",
    "href": "notebooks/DownloadSpeedNormal/DownloadSpeedNormalR.html",
    "title": "Analyzing internet download speeds with a Gaussian model",
    "section": "",
    "text": "a notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nThe maximum internet connection speed downstream in my home is 50 Mbit/sec. This maximum will typically never be reached, but my internet service provider (ISP) claims that the average speed is at least 20Mbit/sec. I want to collect some data to investigate this.\n\n\nGetting started\nFirst, some housekeeping: loading libraries and setting up colors.\n\noptions(repr.plot.width=8, repr.plot.height=6, lwd = 4)\nlibrary(\"RColorBrewer\") # for pretty colors\nlibrary(\"tidyverse\")    # for string interpolation to print variables in plots.\nlibrary(\"latex2exp\")    # the TeX() function makes it possible to print latex math\ncolors = brewer.pal(12, \"Paired\")[c(1,2,7,8,3,4,5,6,9,10)];\n\n\n\nData\nI collect a total of five measurements over the course of five consecutive using an speed testing internet service:\n\nx = c(15.77, 20.5, 8.26, 14.37, 21.09)\n\n\n\nModel\nThe measurements are assumed to be \\[x_1,\\ldots,x_n \\overset{\\mathrm{iid}}{\\sim} \\mathrm{N}(\\theta,\\sigma^2),\\] where \\(\\theta\\) is the average speed; we ignore for simplicity that the measurements cannot be negative.\nThe measurements are reported to have a standard deviation of \\(\\sigma=5\\) by speed testing service and we take this as the given \\(\\sigma\\).\n\nsigma2 = 5^2\n\n\n\nPrior\nI will use a prior centered on the average claimed by the ISP, \\(\\mu_0=20\\), with a prior standard deviation of \\(\\tau_0=5\\). My prior beliefs are therefore that \\(\\theta \\in [10,30]\\) with approximately \\(95\\%\\) probability.\n\nmu_0 = 20\ntau2_0 = 5^2\n\n\n\nPosterior\nA normal prior for a normal model gives us a posterior which is also normal:\n\\[ \\theta | \\mathbf{x} \\sim \\mathrm{N}(\\mu_n,\\tau_n^2), \\] where the posterior precision (1/variance) is the sum of the data precision and the prior precision \\[ \\frac{1}{\\tau_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}  \\] and the posterior mean is a weighted average of the sample mean and the prior mean \\[ \\mu_n = w \\bar x + (1-w)\\mu_0\\] where the weight is the relative precision of the data and prior information \\[ w = \\frac{n/\\sigma^2}{n/\\sigma^2 + 1/\\tau_0^2}\\]\nLet’s write a small function that computes the posterior mean and variance, and plots the prior, likelihood and posterior.\n\npostGaussianIID &lt;- function(x, mu_0, tau2_0, sigma2, thetaGrid, areaPoint){\n    \n    # compute posterior mean and variance\n    n = length(x)\n    tau2_n = 1/(n/sigma2 + 1/tau2_0) \n    w = (n/sigma2)/(n/sigma2 + 1/tau2_0)\n    mu_n = w*mean(x) + (1-w)*mu_0\n    \n    # plot PDFs. Likelihood is normalized.\n    priorPDF = dnorm(thetaGrid, mean = mu_0, sd = sqrt(tau2_0))\n    postPDF = dnorm(thetaGrid, mean = mu_n, sd = sqrt(tau2_n))\n    postProbAbovePoint = 1-pnorm(areaPoint, mean = mu_n, sd = sqrt(tau2_n))\n    normLikePDF = dnorm(thetaGrid, mean = mean(x[1:n]), sd = sqrt(sigma2/n))\n    plot(1, type=\"n\", axes=FALSE, xlab = expression(theta), ylab = \"\", \n         xlim=c(min(thetaGrid),max(thetaGrid)), \n         ylim = c(0,max(priorPDF,postPDF,normLikePDF)))\n    axis(side = 1)\n    polygon(c(thetaGrid[thetaGrid&gt;=areaPoint], max(thetaGrid), areaPoint), \n            c(postPDF[thetaGrid&gt;=areaPoint], 0, 0), \n            col=adjustcolor(colors[4],alpha.f=0.3), border=NA)\n    lines(thetaGrid, priorPDF, type = \"l\", lwd = 4, col = colors[6])\n    lines(thetaGrid, normLikePDF, lwd = 4, col = colors[2])\n    lines(thetaGrid, postPDF, lwd = 4, col = colors[4])\n    legend(x = \"topright\", inset=.05, cex = c(1,1,1,1), \n           legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\", \n           TeX(sprintf(\"$Pr(\\\\theta \\\\geq %2.0f | \\\\mathbf{x}) =  %0.3f$\", areaPoint, postProbAbovePoint))),  \n           lty = c(1, 1, 1, 1), pt.lwd = c(3, 3, 3, 3), \n           col = c(colors[6], colors[2], colors[4], adjustcolor(colors[4],alpha.f=0.3)))\n    cat(\"Posterior mean is \", round(mu_n,3), \"\\n\")\n    cat(\"Posterior standard deviation is \", round(sqrt(tau2_n),3), \"\\n\")\n    cat(\"The weight on the sample mean is \", round(w,3))\n    return(list(\"mu_n\" = mu_n, \"tau2_n\" = tau2_n, \"w\" = w))\n}\n\nLet us start by analyzing just the first observation \\(x_1=15.77\\) using this function.\n\nthetaGrid = seq(0, 40, length = 1000) # Some suitable grid of values to plot over\nareaPoint = 20 # shade the region where theta&gt;= areaPoint (20 in my example)\nn = 1\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  17.885 \nPosterior standard deviation is  3.536 \nThe weight on the sample mean is  0.5\n\n\n\n\n\n\n\n\n\nWe see that the prior and data information happen to get the same weight (w) in the posterior. That is a coincidence from the fact that the prior variance \\(\\tau_0^2\\) is the same as the data variance \\(\\sigma^2\\).\nMoving on, let’s add the next measurement to the analysis:\n\nn = 2\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  18.757 \nPosterior standard deviation is  2.887 \nThe weight on the sample mean is  0.667\n\n\n\n\n\n\n\n\n\nWe now see that the posterior is more affected by the data information than the prior information (w = 0.666).\nFinally, adding all \\(n=5\\) data points gives:\n\nn = 5\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  16.665 \nPosterior standard deviation is  2.041 \nThe weight on the sample mean is  0.833\n\n\n\n\n\n\n\n\n\nI am now rather sure that my average download speed is less than 20 MBit/sec since the posterior probability of \\(\\theta\\geq20\\) is only \\(0.051\\)."
  },
  {
    "objectID": "notebooks/ebayPoissonOneParam/eBayPoissonPython.html",
    "href": "notebooks/ebayPoissonOneParam/eBayPoissonPython.html",
    "title": "Modeling the number of bids in eBay coin auctions",
    "section": "",
    "text": "an Jupyter notebook for the book Bayesian Learning by Mattias Villani\nThe dataset contains data from 1000 auctions of collector coins. The dataset was collected and first analyzed in the article Bayesian Inference in Structural Second-Price Common Value Auctions.\n\nImport modules and load the data\n\nimport numpy as np\nimport scipy.stats as sps\nimport pandas as pd\nimport matplotlib.pyplot as plt\nnp.random.seed(seed=123) # Set the seed for reproducibility\n\n# Load the data\neBayData = pd.read_csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData['nBids']\n\nWe will model these data using a Poisson distribution: \\[y_1,...,y_n \\vert \\theta \\overset{iid}{\\sim} \\mathrm{Poisson}(\\theta).\\] with a conjugate Gamma prior\n\\[\\theta  \\sim \\mathrm{Gamma}(\\alpha, \\beta)\\]\nso that the posterior is also Gamma:\n\\[\\theta \\vert y_1,\\ldots,y_n \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n y_i, \\beta + n)\\]\n\n# Define the Gamma distribution in the rate parametrization\ndef gammaPDF(x, alpha, beta):\n    return(sps.gamma.pdf(x, a = alpha, scale = 1/beta))\n\n\ndef PostPoisson(y, alphaPrior, betaPrior, thetaPriorGrid = None, thetaPostGrid = None):\n\n    # Compute Prior density and posterior\n    priorDens = gammaPDF(x = thetaPriorGrid, alpha = alphaPrior, beta = betaPrior)\n    n = len(y)\n    alphaPost = alphaPrior + np.sum(y)\n    betaPost = betaPrior + n\n    postDens = gammaPDF(x = thetaPostGrid, alpha = alphaPost, beta = betaPost)\n    \n    print('Number of data points = ' + str(len(y)))\n    print('Sum of number of counts = ' + str(np.sum(y)))\n    print('Mean number of counts = ' + str(np.mean(y)))\n    print('Prior mean = ' + str(alphaPrior/betaPrior))\n    print('Prior standard deviation = '+ str(np.sqrt(alphaPrior/(betaPrior**2))))\n    print('Equal tail 95% prior interval: ' + str(sps.gamma.interval(0.95, a = alphaPrior, scale = 1/betaPrior)))  \n    print('Posterior mean = ' + str(round(alphaPost/betaPost,3)))\n    print('Posterior standard deviation = '+ str(np.sqrt(    (alphaPrior+np.sum(y))/  ((betaPrior+n)**2)  )    ))\n    print('Equal tail 95% posterior interval: ' + str(sps.gamma.interval(0.95, a = alphaPost, scale = 1/betaPost)))  \n\n    if (thetaPriorGrid.any() != None):\n        fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n        h1, = ax[0].plot(thetaPriorGrid, priorDens, lw = 3);\n        ax[0].set_xlabel(r'$\\theta$');ax[0].set_ylabel('PDF');\n        ax[0].set_title('Prior distribution');\n\n        h2, = ax[1].plot(thetaPostGrid, postDens, lw = 3, color =\"orange\");\n        ax[1].set_xlabel(r'$\\theta$');ax[1].set_ylabel('PDF');\n        ax[1].set_title('Posterior distribution');\n\nalphaPrior = 2\nbetaPrior = 1/2\nPostPoisson(y = nBids, alphaPrior = 2, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,12,10000), thetaPostGrid = np.linspace(3.25,4,10000))\n\nNumber of data points = 1000\nSum of number of counts = 3635\nMean number of counts = 3.635\nPrior mean = 4.0\nPrior standard deviation = 2.8284271247461903\nEqual tail 95% prior interval: (0.48441855708793014, 11.143286781877796)\nPosterior mean = 3.635\nPosterior standard deviation = 0.06027740643004339\nEqual tail 95% posterior interval: (3.5179903738284697, 3.7542677655304297)\n\n\n\n\n\n\n\n\n\n\n\nFit of the Poisson model\nLet’s plot the data along with the fitted Poisson model. We’ll keep things simple and plot the fit for the posterior mean of \\(\\theta\\).\n\ndef plotPoissonFit(y, alphaPrior, betaPrior):\n    \n    # Plot data\n    maxY = np.max(y)\n    yGrid = np.arange(maxY)\n    probs = [np.sum(y==k)/len(y) for k in range(maxY)]\n    h1 = plt.bar(yGrid, probs, alpha = 0.3);\n    plt.xlabel('y');plt.ylabel('PMF');\n    plt.xticks(yGrid);\n    plt.title('Fitted Poisson model based on posterior mean estimate');\n    \n    # Compute posterior mean\n    n = len(y)\n    alphaPost = alphaPrior + np.sum(y)\n    betaPost = betaPrior + n\n    postMean = alphaPost/betaPost\n    \n    # Plot the fit based on the posterior mean of theta\n    poisFit = sps.poisson.pmf(yGrid, mu = postMean) \n    plt.plot(yGrid, poisFit, color = 'orange', lw = 3)\n\n\n# Plot the fit for all bids\nalphaPrior = 2\nbetaPrior = 1/2\nplotPoissonFit(y = nBids, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\n\n\n\n\nWow, that’s are terrible fit! This data does not look at all like a Poisson distribution. What can we do?\n\n\nAnalyzing the auction with low and high reservation prices separately.\nWe will later model the number of bids using a Poisson regression where we take into account several explanatory variables. But, for now, let’s split the auctions in two subsets:\ni) auctions with low reservation price in relation to the item’s book value (MinBidShare&lt;=0)\nii) auctions with high reservation price in relation to the item’s book value (MinBidShare&gt;0)\nLet’s start with the 550 auction with low reservation prices. The prior for the auction with low reservation prices is set to \\(\\theta \\sim \\mathrm{Gamma}(4,1/2)\\) to reflect a belief that belief that such auctions are likely to attract more bids.\n\n# Auctions with low reservation prices:\nnBidsLow = nBids[eBayData['MinBidShare']&lt;=0]\n\nPostPoisson(y = nBidsLow, alphaPrior = 4, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,25,10000), thetaPostGrid = np.linspace(4.8,5.8,10000))\n\nNumber of data points = 550\nSum of number of counts = 2927\nMean number of counts = 5.321818181818182\nPrior mean = 8.0\nPrior standard deviation = 4.0\nEqual tail 95% prior interval: (2.17973074725265, 17.534546139484647)\nPosterior mean = 5.324\nPosterior standard deviation = 0.0983446153216288\nEqual tail 95% posterior interval: (5.13322503650632, 5.518717305739481)\n\n\n\n\n\n\n\n\n\nAs expected, the posterior for the mean number of bids is concentrated on a larger number of bids. People like to bid on items where the seller’s reservation price is low.\nIs the first for these auctions improved? Yes it is, although there is still room for improvement:\n\n# Plot the fit for low bids\nplotPoissonFit(y = nBidsLow, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\n\n\n\n\nBelow are the results for the auction with high reservation bids. The prior is here set to \\(\\theta \\sim \\mathrm{Gamma}(1,1/2)\\) implying less on average.\n\n# Auctions with high reservation prices:\nnBidsHigh = nBids[eBayData['MinBidShare']&gt;0]\n\nPostPoisson(y = nBidsHigh, alphaPrior = 1, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,12,10000), thetaPostGrid = np.linspace(1.3,1.8,10000))\n\nNumber of data points = 450\nSum of number of counts = 708\nMean number of counts = 1.5733333333333333\nPrior mean = 2.0\nPrior standard deviation = 2.0\nEqual tail 95% prior interval: (0.050635615968579795, 7.377758908227871)\nPosterior mean = 1.574\nPosterior standard deviation = 0.05910555807189499\nEqual tail 95% posterior interval: (1.4600786825716714, 1.6917395497993104)\n\n\n\n\n\n\n\n\n\nAnd the fit is not perfect for these bids, but better than before.\n\n# Plot the fit for high bids\nplotPoissonFit(y = nBidsHigh, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\n\n\n\n\nSo, separating the bids into dataset with low and high reservation prices makes the Poisson model a lot better for the data. Later in the book, we will use a Poisson regression with reservation price as one of the features, which an even more fine grained analysis."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/Python_KalmanFilteringSmoothing.html",
    "href": "notebooks/KalmanFilteringSmoothing/Python_KalmanFilteringSmoothing.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nThe answer is 8"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/Julia_KalmanFilteringSmoothing.html",
    "href": "notebooks/KalmanFilteringSmoothing/Julia_KalmanFilteringSmoothing.html",
    "title": "Kalman filtering and Smoothing for the river nile data",
    "section": "",
    "text": "This notebook analyzes the River Nile data using the local level model with full implementation of the Kalman filter and smoother.\n\nusing Pkg\nPkg.add(\"Plots\");\n\n   Resolving package versions...\n  No Changes to `~/Dropbox/BayesBookWeb/BayesianLearningBook/notebooks/KalmanFilteringSmoothing/Project.toml`\n  No Changes to `~/Dropbox/BayesBookWeb/BayesianLearningBook/notebooks/KalmanFilteringSmoothing/Manifest.toml`\n\n\nDefine the Kalman filter update at a given time point:\n\nfunction kalmanfilter_update(μ, Ω, u, y, A, B, C, Σₑ, Σₙ)\n\n    # Prediction step - moving state forward without new measurement\n    μ̄ = A*μ .+ B*u\n    Ω̄ = A*Ω*A' + Σₙ\n\n    # Measurement update - updating the N(μ̄, Ω̄) prior with the new data point\n    K = Ω̄*C' / (C*Ω̄*C' .+ Σₑ) # Kalman Gain\n    μ = μ̄ + K*(y .- C*μ̄)\n    Ω = (I - K*C)*Ω̄\n    return μ, Ω\n\nend;\n\nDefining a function for the Kalman filter algorithm that calls on the kalmanfilter_update at each time-step:\n\nfunction kalmanfilter(U, Y, A, B, C, Σₑ, Σₙ, μ₀, Σ₀)\n    \n    T = size(Y,1)   # Number of time steps\n    n = length(μ₀)  # Dimension of the state vector      \n\n    # Storage for the mean and covariance state vector trajectory over time\n    μ_filter = zeros(T, n)   \n    Σ_filter = zeros(n, n, T)\n    \n    # The Kalman iterations\n    μ = μ₀\n    Σ = Σ₀\n    for t = 1:T\n        μ, Σ = kalmanfilter_update(μ, Σ, U[t,:]', Y[t,:]', A, B, C, Σₑ, Σₙ)\n        μ_filter[t,:] .= μ\n        Σ_filter[:,:,t] .= Σ\n    end\n\n    return μ_filter, Σ_filter\n\nend;"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/LocalTrendNileDataPython.html",
    "href": "notebooks/KalmanFilteringSmoothing/LocalTrendNileDataPython.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma, norm\n\nnile = pd.read_csv('nile.csv')\n\n\n\"\"\"\nUnivariate Local Linear Level Model\n\"\"\"\nclass LocalLinearLevel(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 1\n\n        # Initialize the statespace\n        super(LocalLinearLevel, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1])\n        self.ssm['transition'] = np.array([1])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*2\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearLevel, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]\n\n\nmod = LocalLinearLevel(nile['flow'])\ninitial_state = np.array([1000])  # Example: Modify based on your model's state dimension\ninitial_state_cov = np.eye(1) * 1000**2  # Small non-zero covariance for each state\n\n# Initialize the model with known values\nmod.initialize_known(initial_state, initial_state_cov)\n\nconstraints = {'sigma2.measurement': 100**2, 'sigma2.level': 100**2}\n# Fit the model with constraints\nres = mod.fit_constrained(constraints)\n\n#res = mod.fit(disp=False)\nprint(res.summary())\n\n                           Statespace Model Results                           \n==============================================================================\nDep. Variable:                   flow   No. Observations:                  100\nModel:               LocalLinearLevel   Log Likelihood                -636.763\nDate:                Thu, 29 Feb 2024   AIC                           1273.526\nTime:                        18:53:38   BIC                           1273.526\nSample:                             0   HQIC                          1273.526\n                                - 100                                         \nCovariance Type:                  opg                                         \n==============================================================================================\n                                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nsigma2.measurement (fixed)      1e+04        nan        nan        nan         nan         nan\nsigma2.level (fixed)            1e+04        nan        nan        nan         nan         nan\n===================================================================================\nLjung-Box (L1) (Q):                   2.16   Jarque-Bera (JB):                 0.40\nProb(Q):                              0.14   Prob(JB):                         0.82\nHeteroskedasticity (H):               0.65   Skew:                             0.13\nProb(H) (two-sided):                  0.22   Kurtosis:                         2.83\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method\n\n\nnsimulations = 50000\nsimulated_state_kfs = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=nile.year)\nsimulated_state_cfa = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=nile.year)\n\nfor i in range(nsimulations):\n    # Apply KFS simulation smoothing\n    sim_kfs.simulate()\n    # Save the KFS simulated state\n    simulated_state_kfs.iloc[:, i] = sim_kfs.simulated_state[0]\n\n    # Apply CFA simulation smoothing\n    sim_cfa.simulate()\n    # Save the CFA simulated state\n    simulated_state_cfa.iloc[:, i] = sim_cfa.simulated_state[0]\n\n\nsimulated_state_kfs.iloc[93,:].mean()\n\n1013.6584161831919\n\n\n\nsimulated_state_kfs.iloc[1,:].std()\n\n68.68731718279996"
  },
  {
    "objectID": "notebooks/SurveyMultinomial/multinomial.html",
    "href": "notebooks/SurveyMultinomial/multinomial.html",
    "title": "Bayesian analysis of multinomial data",
    "section": "",
    "text": "Prepared for the course: Bayesian Learning Author: Mattias Villani, Stockholm and Linköping University, http://mattiasvillani.com\n\nData\nA company has conduct a survey of mobile phone usage. 513 participants were asked the question: ‘What kind of mobile phone do you main use?’ with the four options:\n\niPhone\nAndroid Phone\nWindows Phone\nOther\n\nThe responses in the four categories were: 180, 230, 62, 41.\n\n\nModel\n\\[\n(y_1,\\ldots,y_4) \\vert \\theta_1,\\ldots,\\theta_4 \\sim \\mathrm{multinomial}(\\theta_1,\\ldots,\\theta_4)\n\\]\n\n\nPrior\nThe conjugate prior for multinomial data is the Dirichlet prior.\n\\[\n(\\theta_1,\\ldots,\\theta_K) \\sim \\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_K),\n\\] where the \\(\\alpha_k\\) are positive hyperparameters such that \\(\\mathbb{E}(\\theta_k) = \\alpha_k /\\sum_{j=1}^K \\alpha_j\\). Also, the sum of the \\(\\alpha\\)’s, \\(\\sum_{k=1}^K \\alpha_j\\), determines the precision (inverse variance) of the Dirichlet distribution. We will determine the prior hyperparameters from data from a similar survey that was conducted four year ago. The proportions in the four categories back then were: 30%, 30%, 20% and 20%. This was a large survey, but since time has passed and user patterns most likely has changed, we value the information in this older survey as being equivalent to a survey with only 50 participants. This gives us the prior: \\[\n(\\theta_1,\\ldots,\\theta_4) \\sim \\mathrm{Dirichlet}(\\alpha_1 = 15,\\alpha_2 = 15,\\alpha_3 = 10,\\alpha_4=10)\n\\]\nnote that \\(\\mathbb{E}(\\theta_1) = 15/50 = 0.3\\) and so on, so the prior mean is set equal to the proportions from the older survey. Also, \\(\\sum_{k=1}^4 \\alpha_k = 50\\), so the prior information is equivalent to a survey based on 50 respondents, as required.\n\n\nPosterior\n\\[(\\theta_1,\\ldots,\\theta_K) \\vert \\mathbf{y} \\sim \\mathrm{Dirichlet}(\\alpha_1 + y_1,\\ldots,\\alpha_K + y_K)\\] We can easily simulate from a Dirichlet distribution since if \\(x_k \\sim \\mathrm{Gamma}(\\alpha_k,1)\\) for \\(k=1,\\ldots,K\\), then the vector \\((z_1,\\ldots,z_K)\\) where \\(z_k = y_k /\\sum_{j=1}^K y_k\\), can be shown to follow the \\(\\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_K)\\) distribution. The code below in a (inefficient) implementation of this simulator.\n\nSimDirichlet &lt;- function(nIter, param){     \n  nCat &lt;- length(param)     \n  thetaDraws &lt;- as.data.frame(matrix(NA, nIter, nCat)) # Storage.   \n  for (j in 1:nCat){        \n    thetaDraws[,j] &lt;- rgamma(nIter,param[j],1)  \n  }     \n  for (i in 1:nIter){       \n    thetaDraws[i,] = thetaDraws[i,]/sum(thetaDraws[i,]) \n  }     \n  return(thetaDraws) \n}\n\nOk, let’s use this piece of code on the survey data to obtain a sample from the posterior.\n\n# Data and prior\nset.seed(123) # Set the seed for reproducibility\ny &lt;- c(180,230,62,41) # The cell phone survey data (K=4)\nalpha &lt;- c(15,15,10,10) # Dirichlet prior hyperparameters \nnIter &lt;- 1000 # Number of posterior draws\nthetaDraws &lt;- SimDirichlet(nIter,y + alpha)\nnames(thetaDraws) &lt;- c('theta1','theta2','theta3','theta4')\nhead(thetaDraws)\n\n     theta1    theta2    theta3     theta4\n1 0.3530702 0.4539865 0.1079301 0.08501313\n2 0.3676441 0.4020532 0.1406601 0.08964270\n3 0.3347276 0.4782798 0.1030357 0.08395696\n4 0.3395357 0.4487847 0.1217003 0.08997931\n5 0.3769253 0.4175829 0.1180137 0.08747814\n6 0.3607870 0.4186397 0.1275708 0.09300241\n\n\nSo thetaDraws is a nIter-by-4 matrix, where the \\(k\\)th column holds the posterior draws for \\(\\theta_k\\). We can now approximate the marginal posterior of \\(\\theta_k\\) by a histogram or a a kernel density estimate.\n\npar(mfrow = c(1,2)) # Splits the graphical window in four parts\nhist(thetaDraws[,1], breaks = 25, xlab = 'Fraction IPhone users', main ='iPhone', freq = FALSE)  \nlines(density(thetaDraws[,1]), col = \"blue\", lwd = 2)\nhist(thetaDraws[,2], breaks = 25, xlab = 'Fraction Android users', main ='Android', freq = FALSE)\nlines(density(thetaDraws[,2]), col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\nWe can also compute the probability that Android has the largest market share by simply the proportion of posterior draws where Android is largest. You can for example see that this was the case in the first six draws shown above. This code does this calculation.\n\n# Computing the posterior probability that Android is the largest\nPrAndroidLargest &lt;- sum(thetaDraws[,2]&gt;apply(thetaDraws[,c(1,3,4)],1,max))/nIter\nmessage(paste('Pr(Android has the largest market share) = ', PrAndroidLargest))\n\nPr(Android has the largest market share) =  0.991"
  },
  {
    "objectID": "exercises/ch2/weibull_post_censored.html",
    "href": "exercises/ch2/weibull_post_censored.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.3\nThis exercise continues the analysis of the lung cancer data in Exercise 2.2\nAssume that the survival time \\(X\\) of the lung cancer patients in Exercise 2.2 are independent Weibull distributed \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] The value of \\(k\\) determines how the failure rate changes with time:\n\n\\(k=1\\) gives a failure (death) rate that is constant over time and corresponds to the special case of a exponential distribution \\(\\mathrm{Expon}(\\theta=1/\\lambda)\\) used in Exercise 2.2. Note that (following Wikipedia) the exponential distribution is parameterized with a rate (inverse scale) parameter \\(\\theta\\), while the Weibull is parameterized with a scale parameter \\(\\lambda= 1/\\theta\\) 🤷\n\\(k&lt;1\\) gives a decreasing failure rate over time\n\\(k&gt;1\\) gives an increasing failure rate over time.\n\n\nPlot the posterior distribution of \\(\\lambda\\) conditional on \\(k=1\\), \\(k=3/2\\) and \\(k=2\\). For all \\(k\\), use the prior \\(\\lambda \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) with \\(\\alpha=3\\) and \\(\\beta=1/50\\) (which a similar prior for \\(\\theta=1/\\lambda\\) as in Exercise 2.2). Hint: the posterior distribution for \\(k\\neq 1\\) is intractable, so use numerical evaluation of the posterior over a grid of \\(\\lambda\\)-values.\nPlot the time variable as a histogram and overlay the fitted model for the three different \\(k\\)-values; use the posterior mode for \\(\\theta\\) in each model when plotting the fitted model density.\nUse stan to sample from the posterior distribution of \\(\\lambda\\) for a given \\(k=3/2\\). This should replicate your results in (a). Read this part of the Stan User Guide on how to implement censoring in the model before starting. The example in the User Guide has the same censoring point for all patients, which is not the case in the lung dataset. So you need to generalize that to a vector of censoring points, one for each patient.\n\n\n\n\n\n\n\nSolution Exercise 2.3a\n\n\n\n\n\nSimilar to Exercise 2.2b, the likelihood can be computed with separate treatment of the uncensored and censored observations: \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\lambda, k) & = \\prod_{i=1}^n p(x_i \\vert \\lambda, k) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\lambda, k) \\prod_{c \\in \\mathcal{C}} \\Big(1 - F(x_c \\vert \\lambda, k)\\Big)\n\\end{align}\n\\] where \\(p(x \\vert \\lambda, k)\\) is the pdf of a Weibull variable \\[\np(x \\vert \\lambda, k) = \\frac{k}{\\lambda}\\Big( \\frac{x}{\\lambda} \\Big)^{k-1}e^{-(x/\\lambda)^k}\\quad\\text{ for }x&gt;0\n\\] which is implemented in R as dweibull. The cdf of the Weibull distribution is of rather simple form \\[\nF(x \\vert \\lambda, k) = 1 - e^{-(x/\\lambda)^k}\n\\] and is implemented in R as pweibull.\nThe code below plots the prior and posterior distribution for \\(\\lambda\\) for the three different \\(k\\)-values. We could have inserted the mathematical expressions for the pdf and cdf and simplified the final likelihood expression; we will instead use the dweibull and pweibull functions without simplifications since it gives a more general template that can be used for any distribution, not just the Weibull model. For numerical stability we usually compute the posterior distribution on the log scale \\[\n\\log p(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j)\n\\] for a grid of equally spaced \\(\\lambda\\)-values: \\(\\lambda^{(1)}\\ldots,\\lambda^{(J)}\\). The \\(\\propto\\) sign now means that there is a missing additive constant \\(\\log p(x_1,\\ldots,x_n)\\) which does not depend on the unknown parameter \\(\\lambda\\). When we have computed \\(\\log p(\\lambda \\vert x_1,\\ldots,x_n)\\) over a grid of \\(\\lambda\\) values we compute the posterior on the original scale by \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j) \\Big)\n\\] and then divide all numbers with the normalizing constant to make sure that the posterior integrates to one. This is done numerically by approximating the integral by a Riemann rectangle sum \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) =\n\\frac{\\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(j)}) + \\log p(\\lambda^{(j)}) \\Big)}\n{\\sum_{h=1}^J \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(h)}) + \\log p(\\lambda^{(h)}) \\Big) \\Delta}\n\\] where \\(\\Delta\\) is the spacing between the grid points of \\(\\lambda\\)-values: \\(\\lambda^{(1)}, \\ldots, \\lambda^{(J)}\\).\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet up prior hyperparameters\n\nalpha_prior &lt;- 3     # shape parameter\nbeta_prior &lt;- 1/50   # rate parameter\n\nSet up function that computes the likelihood for any \\(\\lambda\\) value:\n\n# Make a function that computes the likelihood\nweibull_loglike &lt;- function(lambda, x, censored, k){\n  loglik_uncensored = sum(dweibull(x[-censored], shape = k, scale = lambda, \n                                   log = TRUE))\n  loglik_censored = sum(pweibull(x[censored], shape = k, scale = lambda, \n                                 lower.tail = FALSE, log.p = TRUE))\n  return(loglik_uncensored + loglik_censored)\n}\n\nSet up a function that computes the posterior density over a grid of \\(\\lambda\\):\n\nweibull_posterior &lt;- function(lambdaGrid, x, censored, k, alpha_prior, beta_prior){\n  Delta = lambdaGrid[2] - lambdaGrid[1] # Grid step size\n  logPrior &lt;- dgamma(lambdaGrid, shape = alpha_prior, rate = beta_prior, log = TRUE)\n  logLike &lt;- sapply(lambdaGrid, weibull_loglike, x, censored, k)\n  logPost &lt;- logLike + logPrior\n  logPost &lt;- logPost - max(logPost) # subtract constant to avoid overflow\n  post &lt;- exp(logPost)/(sum(exp(logPost))*Delta) # original scale and normalize\n  logLike &lt;- logLike - max(logLike)\n  likeNorm &lt;- exp(logLike)/(sum(exp(logLike))*Delta) # normalized likelihood\n  return(list(post = post, prior = exp(logPrior), likeNorm = likeNorm))\n}\n\n\n# Plot the prior and posterior densities\n\nlambdaGrid &lt;- seq(200, 800, length.out = 1000)\n# Compute to get the prior\npostRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k = 1, \n                             alpha_prior, beta_prior)\ndf &lt;- data.frame(\n  lambdaGrid = lambdaGrid, \n  prior = postRes$prior\n)\n\n# Compute for all selected k values\npostModes = c()\nfor (k in c(1, 3/2, 2)){\n  postRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k, alpha_prior, beta_prior)\n  df[str_glue(\"posterior k={k}\")] &lt;- postRes$post\n  postModes = c(postModes, lambdaGrid[which.max(postRes$post)])\n}\n\ndf_long &lt;- df %&gt;% pivot_longer(-lambdaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = lambdaGrid, y = density, color = density_type) +\n  geom_line() +\n  xlim(250,600) +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior k=1\", \"posterior k=1.5\", \"posterior k=2\"), \n    values = c(colors[2], colors[1], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.3\", x = expression(lambda), y = \"Density\", color = \"\") + \n  theme_minimal()\n\nWarning: Removed 1668 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 2.3b\n\n\n\n\n\nThe fit of the three Weibull models are plotted below. The best fit seems to be for \\(k=3/2\\), but it is still not very good. In a later exercise you will be asked to freely estimate both \\(\\lambda\\) and \\(k\\), and even later to fit a Weibull regression model with covariates.\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dweibull, args = list(shape = 1, scale = postModes[1]), lwd = 1, \n                aes(color = \"Weibull fit k = 1\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 3/2, scale = postModes[2]), lwd = 1, \n                aes(color = \"Weibull fit k = 3/2\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 2, scale = postModes[3]), lwd = 1, \n                aes(color = \"Weibull fit k = 2\"),\n  ) +\n  labs(title = \"Weibull model fits\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[6]) +\n  scale_color_manual(\"\", values = c(colors[1], colors[3], colors[4])) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 2.3c\n\n\n\n\n\nThe code below defines the iid Weibull survival model with censored data in stan. The code here extends this example in the Stan User Guide to the case with different censoring points for each patient. Note the target += construction where the censored data points are added to the target (the log posterior) after the initial uncensored (observed) data are included in the log posterior with the y_obs ~ weibull(k, lambda) statement. The weibull_lccdf function in stan is a convenience function that computes the survival probability \\(\\mathrm{Pr}(X &gt;= x) = 1 - F(x)\\), where \\(F()\\) is the cdf of the Weibull distribution. There are _lccdf versions of all distribution in stan.\n\nweibull_survivalmodel &lt;- '\ndata {\n\n  // Data\n  int&lt;lower=0&gt; N_obs;\n  int&lt;lower=0&gt; N_cens;\n  array[N_obs] real y_obs;\n  array[N_cens] real y_cens;\n  \n  // Model setting\n  real&lt;lower=0&gt; k;\n  \n  // Prior hyperparameters theta ~ Gamma(alpha, beta)\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; beta;\n}\nparameters {\n  real lambda;\n}\nmodel {\n  lambda ~ gamma(alpha, beta); // specifies the prior\n  y_obs ~ weibull(k, lambda);  // add the observed (non-censored) data\n  target += weibull_lccdf(y_cens | k, lambda); // add censored. lccdf is 1-cdf\n}\n'\n\nWe set up the data and prior lists that will be supplied to stan:\n\nk = 3/2\ny_obs &lt;- lung %&gt;% filter(status == 2) %&gt;% pull(time)\ny_cens &lt;- lung %&gt;% filter(status == 1) %&gt;% pull(time)\n\ndata &lt;- list(N_obs = length(y_obs), N_cens = length(y_cens), \n             y_obs = y_obs, y_cens = y_cens, k = k)\nprior &lt;- list(alpha = alpha_prior, beta = beta_prior)\n\nLoad rstan and set some options\n\n#install.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', \n#                                    getOption(\"repos\")))\nsuppressMessages(library(rstan))\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nSample from the posterior distribution using HMC in stan\n\nnDraws = 5000\nfit = stan(model_code = weibull_survivalmodel, data = c(data, prior), iter = nDraws)\n\nTrying to compile a simple C file\n\n\nRunning /usr/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‘gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0’\ngcc -I\"/usr/share/R/include\" -DNDEBUG   -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/Rcpp/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/unsupported\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/BH/include\" -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/src/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppParallel/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1       -fpic  -g -O2 -ffile-prefix-map=/build/r-base-6tgf7J/r-base-4.4.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2  -c foo.c -o foo.o\nIn file included from /home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/Eigen/Core:19,\n                 from /home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/Eigen/Dense:1,\n                 from /home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [/usr/lib/R/etc/Makeconf:195: foo.o] Error 1\n\n\nSummarize the results from the posterior sampling. The number of effective draws n_eff is not much lower than the \\(5000\\) nominal number of draws, so the HMC sampling is efficient. The Rhat is also close to one, suggesting that the different runs gave similar results.\n\ns &lt;- summary(fit, pars = \"lambda\", probs = c(0.025, 0.975))\ns$summary  # results from all the different runs (chains) merged.\n\n           mean   se_mean       sd     2.5%    97.5%    n_eff     Rhat\nlambda 415.9198 0.3630227 21.31495 376.6366 460.8922 3447.474 1.000285\n\n\nCompare the posterior from HMC sampling with the gridded version above, as a bug check. Hmm, they should agree. Can’t seem to find the bug now, will fix later. Well, you get the idea.\n\n# Plot histogram from stan draws\npostsamples &lt;- extract(fit, pars = c(\"lambda\"))\nhist(postsamples$lambda, 50, freq = FALSE, col = colors[5], \n     xlab = expression(lambda), ylab = \"posterior density\", \n     main = expression(lambda), ylim = c(0,0.025))\n\n# Adding the gridded version from above\nlambdaGrid &lt;- seq(200, 800, length.out = 1000)\npostRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k = k, \n                             alpha_prior, beta_prior)\nlines(lambdaGrid, postRes$post, col = colors[3], lw = 2)\n\nlegend(x = \"topright\", inset=.05, legend = c(\"Stan\", \"Gridded\"), lty = c(0,1),\n         fill = c(colors[5], NA), border = c(1,0),\n         col = c(colors[5], colors[3]), box.lty=1\n  )"
  },
  {
    "objectID": "exercises/ch2/expon_post_censored.html",
    "href": "exercises/ch2/expon_post_censored.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.2\nThe dataset lung in the R package survival contains data on 228 patients with advanced lung cancer. We will here analyze the survival time in days for the patients which is recorded by the variable time. The variable status is a binary variable with status = 1 if the survival time of the patient is censored (patient still alive at the end of the study) and status = 2 if the survival time was uncensored (patient dead before the end of the study).\n\nConsider first only the uncensored patients (status = 2). Assume that the survival time \\(X\\) of the patients are independent exponentially distributed with a common rate parameter \\(\\theta\\) such that \\(\\mathbb{E}(X \\vert \\theta) = 1/\\theta\\). Assume the conjugate prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\). A doctor tells you that the expected time until death (\\(1/\\theta\\)) for this population is around \\(200\\) days. It can be shown that setting \\(\\alpha=3\\) and \\(\\beta=300\\) implies that the prior mean for \\(\\mathbb{E}(X \\vert \\theta) = 1/\\theta\\) is \\(200\\) days, so use that prior. Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values.\nNow consider all patients, both censored and uncensored, using the same prior as in (a). Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values.\nHint: The posterior is no longer tractable due to contributions of the censored patients to the likelihood. For the censored patients we only know that they lived at least the number of days recorded in the dataset. The likelihood contribution \\(p(x_c \\vert \\theta)\\) for the \\(c\\)th censored patient with recorded time \\(x_c\\) is therefore \\(p(X \\geq x_c \\vert \\theta) = e^{-\\theta x_c}\\), which follows from the distribution function of the exponential distribution \\(p(X \\leq x \\vert \\theta) = 1 - e^{-\\theta x}\\).\nPlot a histogram of time and overlay the pdf of the exponential model with the parameter \\(\\theta\\) estimated with the posterior mode.\n\n\n\n\n\n\n\nSolution Exercise 2.2a\n\n\n\n\n\nFrom Exercise 2.1, we know that the posterior distribution is \\[\\theta \\sim \\mathrm{Gamma}(\\alpha + n_u, \\beta + \\sum\\nolimits_{u \\in \\mathcal{U}} x_u),\\] where \\(n_u\\) is the number of uncensored observations and \\(\\mathcal{U}\\) is the set of observation indices for the uncensored data.\nThe following code plots the prior, likelihood (normalized) and posterior over a grid of values for \\(\\theta\\). Note that the data is so much stronger than the prior that the posterior is virtually identical to the likelihood, which is why the normalized likelihood is not visible in the plot.\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\n\n# Summarize the data needed for the posterior, filter out censored data\ndata_summary &lt;- lung %&gt;% filter(status == 2) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters\nalpha_post &lt;- alpha_prior + data_summary$n  \nbeta_post &lt;- beta_prior + data_summary$sum_x   \n\n\n# Plot the prior and posterior densities, and the (normalized) likelihood as a bon \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nlikelihood_density &lt;- dgamma(thetaGrid, shape = data_summary$n, rate = data_summary$sum_x)\nposterior_density &lt;- dgamma(thetaGrid, shape = alpha_post, rate = beta_post)\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  likelihood = likelihood_density,\n  posterior = posterior_density\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"likelihood\", \"posterior\"), \n    values = c(colors[2], colors[1], colors[3])) +\n  labs(title = \"Exercise 2.2\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 2.2b\n\n\n\n\n\nThe likelihood for all data, censored and uncensored, is \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\theta) & = \\prod_{i=1}^n p(x_i \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} p(x_c \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} \\left(1 - F(x_c \\vert \\theta)\\right)\n\\end{align}\n\\] where \\(\\mathcal{U}\\) and \\(\\mathcal{C}\\) are the sets of observation indicies for the uncensored and censored data, respectively. The likelihood for the uncensored data (the first product) is the same as before \\[\n\\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) = \\prod_{u \\in \\mathcal{U}} \\theta e^{-\\theta x_u} = \\theta^{n_u} e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u},\n\\] where \\(n_u\\) is the number of uncensored observations. The likelihood contribution for each observation in the censored set (the second product) is the survival function \\[\n\\mathrm{Pr}(X \\geq x_c) = 1 - F(x_c \\vert \\theta),\n\\] where \\(F(x_c \\vert \\theta) = 1 - e^{-x_c \\theta}\\) is the cumulative distribution function of the exponential distribution evaluated at \\(x_c\\).\nSo the likelihood function is \\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\theta^n e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u} \\times e^{-\\theta \\sum_{u \\in \\mathcal{U}} x_c} = \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i}.\n\\] where one should note that \\(\\theta\\) is raised to the number of uncensored observations, \\(n_u\\) while the sum in the exponential term includes both uncensored and censored observations.\nBy Bayes’ theorem, the posterior distribution is again a Gamma distribution \\[\n\\begin{align}\np(\\theta \\vert x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n& \\propto \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i} \\times \\theta^{\\alpha-1}e^{-\\beta\\theta} \\\\\n& = \\theta^{\\alpha + n_u - 1} e^{ -\\theta(\\beta + \\sum_{i = 1}^n x_i)},\n\\end{align}\n\\] which we recognize as proportional to the following Gamma distribution \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n_u,\\beta + \\sum\\nolimits_{i=1}^n x_i).\n\\]\nThe code below plots both:\n\nthe posterior from the previous exercise (a) with only the uncensored data and\nthe posterior from with all data.\n\nThe posterior with all data is more informative and concentrates on smaller \\(\\theta\\) values. Since smaller \\(\\theta\\) values correspond to longer expected survival times, this is makes sense since the censored patients were still alive at the end of the study.\n\n# Summarize the data needed for the posterior, grouped by `status`:\ndata_summary &lt;- lung %&gt;% group_by(status) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters - only uncensored data\nalpha_post_u &lt;- alpha_prior + data_summary$n[2] # second row is uncensored data (status = 2)  \nbeta_post_u &lt;- beta_prior + data_summary$sum_x[2] # sum over uncensored observations\n\n# Compute posterior hyperparameters - all data\nalpha_post_all &lt;- alpha_prior + data_summary$n[2] # note: this is still n_u \nbeta_post_all &lt;- beta_prior + sum(data_summary$sum_x) # sum over all observations   \n\n\n# Plot the prior and the two posterior densities \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nposterior_density_u &lt;- dgamma(thetaGrid, shape = alpha_post_u, rate = beta_post_u)\nposterior_density_all &lt;- dgamma(thetaGrid, shape = alpha_post_all, rate = beta_post_all)\n\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  posterior_uncensored = posterior_density_u,\n  posterior_all = posterior_density_all\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior_uncensored\", \"posterior_all\"), \n    values = c(colors[2], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.2\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 2.2 c\n\n\n\n\n\nThe code below plots the histogram and the pdf of the exponential model with the parameter \\(\\theta\\) set equal to the posterior mode. It is clear that the exponential model with its monotonically decreasing density is not fitting the data well.\n\npostMode = df$thetaGrid[which.max(df$posterior_all)]\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dexp, args = list(rate = postMode), lwd = 1, \n                aes(color = \"Exponential fit\"),\n  ) +\n  labs(title = \"Exercise 2.2c - Exponential model fit to lung cancer survival\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[5]) +\n  scale_color_manual(\"\", values = colors[3]) +\n  theme_minimal()"
  },
  {
    "objectID": "exercises/ch2solutions.html",
    "href": "exercises/ch2solutions.html",
    "title": "Chapter 2 - Single-parameter models: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution.\n\nExercise 2.1\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\) be iid exponentially distributed data. Show that the Gamma distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood from an iid sample from \\(\\mathrm{Expon}(\\theta)\\) is \\[\np(x_1,\\ldots,x_n \\vert \\theta)= \\prod_{i=1}^n p(x_i \\vert \\theta) =\n  \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\n\\] The density of the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) prior is \\[\np(\\theta) =  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\theta^{\\alpha-1}e^{-\\beta\\theta}\n             \\propto \\theta^{\\alpha-1}e^{-\\beta\\theta}\n\\]\nBy Bayes’ theorem, the posterior distribution is \\[\n\\begin{align}\n  p(\\theta \\vert x_1,\\ldots,x_n) &\\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta)   \\\\\n& \\propto \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\\theta^{\\alpha-1}e^{-\\beta\\theta}  \\\\\n& =  \\theta^{\\alpha + n - 1} e^{ -\\theta(\\beta + \\sum_{i=1}^n x_i)},\n\\end{align}\n\\] which can be recognized as proportional to the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha +n,\\beta + \\sum\\nolimits_{i=1}^n x_i)\\) distribution. Since the prior and posterior belongs to the same (Gamma) distributional family, the Gamma prior is indeed conjugate to the exponential likelihood.\n\n\n\n\n\n\nExercise 2.2\nThe dataset \\(\\texttt{lung}\\) in the R package \\(\\texttt{survival}\\) contains data on 228 patients with advanced lung cancer. We will here analyze the survival time of the patient in days (\\(\\texttt{time}\\)). The variable \\(\\texttt{status}\\) is a binary variable with \\(\\texttt{status}=1\\) if the survival time of the patient is censored (patient was still alive at the end of the study) and \\(\\texttt{status}=2\\) if the survival time was uncensored (patient was dead before the end of the study).\nIn this exercise we will only analyze the uncensored patients; Exercise below asks you to analyze all patients. Assume that the survival times \\(X_1,\\ldots,X_n\\) of the uncensored patients are iid \\(\\mathrm{Expon}(\\theta)\\) distributed. Use the conjugate prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha=3,\\beta=300)\\), which can be shown to imply that the expected survival time \\(\\mathbb{E}(X \\vert \\theta) = 1/\\theta\\) for this population is around \\(200\\) days. Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom Exercise 2.1, we know that the posterior distribution is \\[\\theta \\sim \\mathrm{Gamma}(\\alpha + n_u, \\beta + \\sum\\nolimits_{u \\in \\mathcal{U}} x_u),\\] where \\(n_u\\) is the number of uncensored observations and \\(\\mathcal{U}\\) is the set of observation indices for the uncensored data.\nThe following code plots the prior, likelihood (normalized) and posterior over a grid of values for \\(\\theta\\). Note that the data is so much stronger than the prior that the posterior is virtually identical to the likelihood, which is why the normalized likelihood is not visible in the plot.\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\n\n# Summarize the data needed for the posterior, filter out censored data\ndata_summary &lt;- lung %&gt;% filter(status == 2) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters\nalpha_post &lt;- alpha_prior + data_summary$n  \nbeta_post &lt;- beta_prior + data_summary$sum_x   \n\n\n# Plot the prior and posterior densities, and the (normalized) likelihood \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nlikelihood_density &lt;- dgamma(thetaGrid, shape = data_summary$n, rate = data_summary$sum_x)\nposterior_density &lt;- dgamma(thetaGrid, shape = alpha_post, rate = beta_post)\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  likelihood = likelihood_density,\n  posterior = posterior_density\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"likelihood\", \"posterior\"), \n    values = c(colors[2], colors[1], colors[3])) +\n  labs(title = \"Survival lung cancer - uncensored patients\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.3\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Geom}(\\theta)\\) be iid from a geometric distribution with parameter \\(0&lt;\\theta&lt;1\\). Show that the Beta distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe geometric distribution has probability mass function \\[\np(x) = (1-\\theta)^{x}\\theta, \\quad \\text{ for }x=0,1,2,...\n\\] The likelihood from a sample of \\(n\\) observations is therefore \\[\np(x_{1},\\ldots,x_{n}\\vert\\theta) = \\prod_{i=1}^n p(x_i \\vert \\theta)= (1-\\theta)^{\\sum_{i=1}^n x_i}\\theta^{n}\n\\] The posterior distribution when using a \\(\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\) prior is then by Bayes’ theorem \\[\np(\\theta\\vert x_{1},\\ldots,x_n) \\propto\n(1-\\theta)^{\\sum_{i=1}^n x_i} \\theta^{n} \\cdot \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n=\\theta^{\\alpha + n-1}(1-\\theta)^{\\beta + \\sum_{i=1}^n x_i-1}\n\\] which is proportional to the \\(\\mathrm{Beta}(\\alpha+n,\\beta+\\sum_{i=1}^n x_i)\\) distribution. Since the posterior is in the same Beta family as the prior, the prior \\(\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\) is a conjugate prior to the geometric model.\n\n\n\n\n\n\nExercise 2.4\nLet \\(X_1,\\ldots,X_n\\) be an iid sample from a distribution with density function \\[\np(x) \\propto \\theta^2 x \\exp (-x\\theta)\\quad \\text{ for } x&gt;0 \\text{ and } \\theta&gt;0.\n\\] Find the conjugate prior for this distribution and derive the posterior distribution from an iid sample \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood function from a sample \\(x_1,\\ldots,x_n\\) is\n\\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\prod_{i=1}^n\\theta^2 x_i \\exp (-x_i\\theta) \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\n\\]\nThis likelihood resembles a Gamma distribution, so a good guess for a conjugate prior would be the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) distribution; to see that this is indeed a reasonable guess, note that the particular form of the Gamma density (a power of \\(\\theta\\) times an exponential in \\(\\theta\\)) makes it closed under multiplication. The posterior distribution is then\n\\[\n\\begin{align}\np(\\theta|x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n      & \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\\theta^{\\alpha-1}\\exp(-\\theta\\beta) \\\\\n      & \\propto \\theta^{\\alpha + 2n - 1}\\exp\\Big(-\\theta (\\beta+\\sum_{i=1}^n x_i) \\Big)\n\\end{align}\n\\] and the posterior is therefore \\(\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + 2n,\\beta + \\sum_{i=1}^n x_i)\\). Since the posterior belongs to the same (Gamma) family as the prior, the Gamma prior is indeed conjugate to this likelihood.\n\n\n\n\n\n\nExercise 2.5\na) Let \\(x_{1},\\ldots,x_{10}\\) be a sample with mean \\(\\bar{x}=1.873\\). Assume the model \\(X_1,\\ldots,X_n \\overset{\\mathrm{iid}}{\\sim} N(\\theta,1)\\). Use the prior \\(\\theta \\sim N(0,5)\\). Note that the second parameter of the normal distribution is a variance, not a standard deviation. Compute the posterior distribution of \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\(n=10,\\bar{x}=1.873,\\sigma^{2}=1,\\mu_{0}=0,\\tau_{0}^{2}=5\\). The posterior is normal with \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{1}}{\\frac{10}{1}+\\frac{1}{5}}=\\frac{50}{51}\\approx0.98039 \\\\\n      \\mu_{n}   &= \\frac{50}{51}\\cdot1.873+\\frac{1}{51}\\cdot0=1.8363 \\\\\n      \\tau_{n}^{2}  &= \\left(\\frac{10}{1}+\\frac{1}{5}\\right)^{-1}=\\frac{5}{51}.\n    \\end{align*}\\]\n\n\n\nb) You now get hold of a second sample \\(Y_1,\\ldots,Y_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta ,2)\\), where \\(\\theta\\) is the same quantity as in (a) but the measurements have a larger variance. The sample mean in this second sample is \\(\\bar{y}=0.582\\). Compute the posterior distribution of \\(\\theta\\) using both samples (the \\(x\\)’s and the \\(y\\)’s) under the assumption that the two samples are independent.\n\\(\\textit{Hint}\\): batch learning.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe easiest way to do this is use the posterior from the first sample as a prior for the second sample. That is, for this second sample we use the prior \\[\\begin{equation*}\n      \\theta\\sim N\\left(1.836,\\frac{5}{51}\\right),\n    \\end{equation*}\\] which gives the posterior \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{2}}{\\frac{10}{2}+\\frac{1}{5/51}}=\\frac{25}{76} \\\\\n\\mu_{n} &= \\frac{25}{76}\\cdot0.582+\\left(1-\\frac{25}{76}\\right)\\cdot1.836=1.4237\\\\\n\\tau_{n}^{2}    &= \\left(\\frac{10}{2}+\\frac{1}{5/51}\\right)^{-1}=\\frac{5}{76}.\n    \\end{align*}\\]\n\n\n\nc) You finally obtain a third sample \\(Z_{1},\\ldots,Z_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta,3)\\), with mean \\(\\bar{z}=1.221\\). Unfortunately, the measuring device for this latter sample was defective and any measurement above \\(3\\) was recorded as exactly \\(3\\). There were two such measurements. Give an expression for the unnormalized posterior distribution (likelihood times prior) for \\(\\theta\\) based on all three samples (\\(x,y\\) and \\(z\\)). Plot this unnormalized posterior over a grid of \\(\\theta\\) values.\n\\(\\textit{Hint}\\): the posterior distribution is not normal anymore when the measurements are censored at \\(3\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet us do as before, using the posterior from the first two samples (obtained in problem b) above) as the prior. The prior is therefore \\(\\theta\\sim N\\left(1.4237,\\frac{5}{76}\\right)\\).\nLet us first use these eight observations to update the posterior, and then afterwards add the information from the two measurements that where censored at \\(3\\). The mean of the eight measurements which were correctly recorded is \\(\\frac{1.221\\cdot10-3\\cdot 2}{8} = 0.77625\\). The eight correctly recorded observations gives the following updating of the \\(N\\left(1.4237,\\frac{5}{76}\\right)\\) prior: \\[\\begin{align*}\n      w &=\\frac{\\frac{8}{3}}{\\frac{8}{3}+\\frac{1}{5/76}}=\\frac{10}{67} \\\\\n      \\mu_{n}   &=\\frac{10}{67}\\cdot0.77625+\\left(1-\\frac{10}{67}\\right)\\cdot1.4237=1.3271 \\\\\n      \\tau_{n}^{2} &=\\left(\\frac{8}{3}+\\frac{1}{5/76}\\right)^{-1}=\\frac{15}{268}=0.05597.\n    \\end{align*}\\] Note that most of the weight is now given to the prior, i.e. the posterior after the updates in (a) and (b). This is reasonable since the \\(8\\) new observations have a relatively large variance, \\(\\sigma^{2}=3\\), and the prior at this step is based on \\(20\\) previous observations.\nWe are now ready for the final piece of information: the two censored observations. We do not know their exact values, but we know that they were equal to or larger than \\(3\\). This is important information which we cannot ignore. The likelihood from these two observations (let’s call them \\(z_{1}\\) and \\(z_{2}\\)) is \\[\\begin{align*}\n      p(z_{1},z_{2}\\vert\\theta) &=\\mathrm{Pr}(z_{1}\\geq3\\vert\\theta)\\mathrm{Pr}(z_{2}\\geq3\\vert\\theta) \\\\\n      &=\\Big(1-\\mathrm{Pr}(z_{1}\\leq 3\\vert\\theta)\\Big) \\Big(1-\\mathrm{Pr}(z_{2}\\leq3\\vert\\theta)\\Big) \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]  \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\n    \\end{align*}\\] where \\(\\Phi(\\cdot)\\) is the CDF of the \\(\\mathrm{N}(0,1)\\) distribution. The posterior based on all \\(30\\) data points is now obtained by multiplying this likelihood with the prior at this stage, that is the posterior based on the first \\(28\\) correctly recorded data: \\(N(1.3271,0.05597)\\). The posterior is therefore proportional to \\[\\begin{equation*}\n      \\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\\exp\\left[-\\frac{1}{2\\cdot0.05597}\\left(\\mu-1.3271\\right)^{2}\\right].\n    \\end{equation*}\\]\nThe code below plots the prior (which is the posterior based on the \\(28\\) correctly recorded observations), the (normalized) likelihood from the two censored observations and the posterior based on all \\(30\\) data points. Note the form of the likelihood function from the two censored observations: the probability of observing them increases monotonically with \\(\\theta\\) since all we know about these observations is that they are larger or equal to \\(3\\).\n\ntheta_grid = seq(0.5, 2.5, length = 1000)\nbin_width = theta_grid[2] - theta_grid[1]\nlike = (1 - pnorm(3, theta_grid, sqrt(3)))^2 \nprior = dnorm(theta_grid, 1.3271, sqrt(0.05597))\npost = like * prior # unnormalized\npost = post / sum(post * bin_width)\nlike = like / sum(like * bin_width)\nplot(theta_grid, prior, type = \"l\", col = \"orange\", lwd = 3, xlab = expression(theta), ylab = \"density\")\nlines(theta_grid, like, col = \"steelblue\", lwd = 3)\nlines(theta_grid, post, col = \"indianred\", lwd = 3)\nlegend(\"topleft\", legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),\n       col = c(\"orange\", \"steelblue\", \"indianred\"), lwd = 3, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.6\nDerive the posterior distribution for the normal model with a normal prior.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.7\na) Let \\(X_1,\\dots,X_n |\\theta \\sim \\mathrm{Uniform}(\\theta-1/2,\\theta+1/2)\\) for \\(-\\infty &lt; \\theta &lt;\\infty\\). The estimator \\(\\hat\\theta= \\bar X\\) is unbiased for \\(\\theta\\). Calculate for the sampling variance of \\(\\hat\\theta\\).\n\\(\\textit{Note}\\): A more efficient estimator is the mid-range \\(\\hat\\theta = (X_{\\min} + X_{\\max})/2\\), but we use the sample mean here for simplicity.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe variance of a sample mean is always of the form \\[\n\\mathbb{V}(\\bar X) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of each observation \\(X_i \\sim \\mathrm{Uniform}(\\theta-1/2,\\theta+1/2)\\). The variance of \\(\\mathrm{Uniform}(a,b)\\) variable is \\(\\frac{(b-a)^2}{2}\\), so here we have \\(\\sigma^2 = \\frac{1}{12}\\) and the sampling variance of the estimator is \\[\n\\mathbb{V}(\\bar X) = \\frac{1}{12n}\n\\]\n\n\n\nb) Derive the posterior distribution for \\(\\theta\\) assuming a uniform prior distribution over the whole real line.\n\\(\\textit{Hint}\\): once you have observed some data, some values for \\(\\theta\\) are no longer possible.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe density for each observation in the sample is \\[\np(x \\vert \\theta) = I\\Big(\\theta- \\frac{1}{2} \\leq x \\leq \\theta + \\frac{1}{2}\\Big).\n\\] where \\(I()\\) is an indicator function that returns the value one when the condition inside the parenthesis is true, and zero otherwise. The likelihood based a single observation is therefore zero whenever \\(\\theta \\geq x-\\frac{1}{2}\\) or when \\(\\theta \\leq x + \\frac{1}{2}\\). So, to highlight that we care about \\(p(x\\vert\\theta)\\) as function of \\(\\theta\\), we can write \\[\np(x \\vert \\theta) = I\\Big(x - \\frac{1}{2} \\leq \\theta \\leq x + \\frac{1}{2}\\Big).\n\\]\nThe likelihood from an iid sample of \\(n\\) observation can therefore be written \\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\prod_{i=1}^n p(x_i \\vert \\theta) = \\prod_{i=1}^n I\\Big(x_i - \\frac{1}{2} \\leq \\theta \\leq x_i + \\frac{1}{2}\\Big)\n\\] The plots below illustrate how certain \\(\\theta\\) values makes the single data point \\(x_1 = 2.1\\) impossible.\n\n\nCode\nxi = 2.1\npar(mfrow = c(3,1))\n\n# Adding the uniform density for an impossible theta\ntheta = 1.2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 1.2 is too low - distribution misses the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"orange\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"orange\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"orange\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"orange\")\n\n# Adding the uniform density for an impossible theta\ntheta = 2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 2 is OK - distribution captures the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"cornflowerblue\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"cornflowerblue\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"cornflowerblue\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"cornflowerblue\")\n\n# Adding the uniform density for an impossible theta\ntheta = 3.2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 3.2 is too high - distribution misses the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"green\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"green\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"green\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"green\")\n\n\n\n\n\n\n\n\n\nThe likelihood is non-zero only for the \\(\\theta\\) values where \\(x_i - \\frac{1}{2} \\leq \\theta \\leq x_i + \\frac{1}{2}\\) for all data observations \\(i=1,2,\\ldots,n\\). This means that the likelihood is non-zero only when \\(x_\\max - \\frac{1}{2} \\leq \\theta \\leq x_\\min + \\frac{1}{2}\\), where \\(x_\\min\\) and \\(x_\\max\\) are the minimum and maximum of the sample. With a uniform prior on \\(\\theta\\), the posterior is proportional to the likelihood, so \\[\np(\\theta \\vert x_1,\\ldots,x_n) \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\propto I\\Big( x_\\max - \\frac{1}{2} \\leq \\theta \\leq x_\\min + \\frac{1}{2} \\Big)\n\\] So the posterior distribution is \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Uniform}\\Big(x_\\max - \\frac{1}{2} , \\leq x_\\min + \\frac{1}{2} \\Big)\n\\]\n\n\n\nc) Assume that you have observed three data observations: \\(x_1 = 1.1, x_2 = 2.05, x_3 = 1.21\\). What would a frequentist conclude about \\(\\theta\\)? What would a Bayesian conclude? Discuss.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe frequentist with \\(\\bar X\\) as the estimator of \\(\\theta\\) obtains the estimate \\(\\bar x \\approx 1.453\\). The sampling standard deviation is \\(\\mathbb{S}(\\bar X) = \\sqrt{\\frac{1}{12n}} = \\sqrt{\\frac{1}{3\\cdot 12}} \\approx 0.1666\\), which is the variability of the sample mean estimator on average over all possible datasets of size \\(n=3\\). The variability is rather large since we only have three observations, so we can easily obtain a sample with three extreme (all small or all large) observations. Here is a plot of the simulated sampling distribution for \\(\\bar X\\) from a sample with three observations:\n\ntheta = 1  # any value is ok, it will be the center of the sampling distr.\nnRep = 50000\nn = 3\nxbar = rep(0, nRep)\nfor (i in 1:nRep){\n  x_rep = runif(n, min = theta - 0.5, max = theta + 0.5)\n  xbar[i] = mean(x_rep)\n}\nhist(xbar, 50, freq = FALSE, \n     main = \"sampling distribution of the sample mean\", \n     xlab = \"sample mean\", ylab = \"density\", col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n(As a side-note: note how fast the central limit theorem is here, the sampling distribution is already close to normal as \\(n=3\\))\nFor the given dataset we have \\(x_\\min = 1.1\\) and \\(x_\\max = 2.05\\) and the posterior is therefore \\[\n\\theta \\vert x_1,x_2,x_3 \\sim \\mathrm{Uniform}\\big(1.55 , 1.60 \\big)\n\\] Since we were lucky to obtain a range of the data close to \\(1\\) (the range is the difference between the maximum and minimum observations: \\(2.05-1.1 = 0.95\\)), the Bayesian gets a tight posterior which is uniform between \\(1.55\\) and \\(1.60\\). The posterior is plotted here:\n\n\nCode\nx = c(1.10, 2.05, 1.21)\nplot(x = NA, y = NA, xlim = c(1,2), ylim = c(-1,1), \n     xlab = expression(theta), ylab = \"posterior density\")\npost_low = max(x) -0.5\npost_high = min(x) + 0.5\nlines(c(post_low, post_high), c(1, 1), col = \"orange\")\nlines(c(post_low, post_low), c(0, 1), col = \"orange\")\nlines(c(post_high, post_high), c(1, 0), col = \"orange\")\nabline(h=0, lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nThe difference between the frequentist and Bayesian solutions is that the Bayesian solution conditions on the observed data, while the frequentist inferences are unconditional on the data, measuring the variability of the estimator over all possible datasets. We got lucky with a wide range in the actually observed data, so the Bayesian can provide a tight posterior for \\(\\theta\\) with little uncertainty.\n\n\n\n\n\n\nExercise 2.8\nExercise modelled the survival times of uncensored lung cancer patients with an iid exponential model. In this exercise we will extend that analysis to include also the censored patients, using the same prior as in Exercise . Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values. Finally, assess the fit of the exponential model by plotting a histogram of \\(\\texttt{time}\\) and overlay the pdf of the exponential model with the parameter \\(\\theta\\) estimated with the posterior mode.\n\\(\\textit{Hint}\\): The posterior is no longer tractable due to contributions of the censored patients to the likelihood. For the censored patients we only know that they lived the number of days recorded in the dataset. The likelihood contribution \\(p(x_c \\vert \\theta)\\) for the \\(c\\)th censored patient with recorded time \\(x_c\\) is therefore \\(p(X \\geq x_c \\vert \\theta) = e^{-\\theta x_c}\\), which follows from the distribution function of the exponential distribution \\(p(X \\leq x \\vert \\theta) = 1 - e^{-\\theta x}\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nThe likelihood for all data, censored and uncensored, is \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\theta) & = \\prod_{i=1}^n p(x_i \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} p(x_c \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} \\left(1 - F(x_c \\vert \\theta)\\right)\n\\end{align}\n\\] where \\(\\mathcal{U}\\) and \\(\\mathcal{C}\\) are the sets of observation indicies for the uncensored and censored data, respectively. The likelihood for the uncensored data (the first product) is the same as before \\[\n\\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) = \\prod_{u \\in \\mathcal{U}} \\theta e^{-\\theta x_u} = \\theta^{n_u} e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u},\n\\] where \\(n_u\\) is the number of uncensored observations. The likelihood contribution for each observation in the censored set (the second product) is the survival function \\[\n\\mathrm{Pr}(X \\geq x_c) = 1 - F(x_c \\vert \\theta),\n\\] where \\(F(x_c \\vert \\theta) = 1 - e^{-x_c \\theta}\\) is the cumulative distribution function of the exponential distribution evaluated at \\(x_c\\).\nSo the likelihood function is \\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\theta^n e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u} \\times e^{-\\theta \\sum_{u \\in \\mathcal{U}} x_c} = \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i}.\n\\] where one should note that \\(\\theta\\) is raised to the number of uncensored observations, \\(n_u\\) while the sum in the exponential term includes both uncensored and censored observations.\nBy Bayes’ theorem, the posterior distribution is again a Gamma distribution \\[\n\\begin{align}\np(\\theta \\vert x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n& \\propto \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i} \\times \\theta^{\\alpha-1}e^{-\\beta\\theta} \\\\\n& = \\theta^{\\alpha + n_u - 1} e^{ -\\theta(\\beta + \\sum_{i = 1}^n x_i)},\n\\end{align}\n\\] which we recognize as proportional to the following Gamma distribution \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n_u,\\beta + \\sum\\nolimits_{i=1}^n x_i).\n\\]\nThe code below plots both:\n\nthe posterior from the previous exercise (a) with only the uncensored data and\nthe posterior from with all data.\n\nThe posterior with all data is more informative and concentrates on smaller \\(\\theta\\) values. Since smaller \\(\\theta\\) values correspond to longer expected survival times, this is makes sense since the censored patients were still alive at the end of the study.\n\n# Summarize the data needed for the posterior, grouped by `status`:\ndata_summary &lt;- lung %&gt;% group_by(status) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters - only uncensored data\nalpha_post_u &lt;- alpha_prior + data_summary$n[2] # second row is uncensored data (status = 2)  \nbeta_post_u &lt;- beta_prior + data_summary$sum_x[2] # sum over uncensored observations\n\n# Compute posterior hyperparameters - all data\nalpha_post_all &lt;- alpha_prior + data_summary$n[2] # note: this is still n_u \nbeta_post_all &lt;- beta_prior + sum(data_summary$sum_x) # sum over all observations   \n\n\n# Plot the prior and the two posterior densities \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nposterior_density_u &lt;- dgamma(thetaGrid, shape = alpha_post_u, rate = beta_post_u)\nposterior_density_all &lt;- dgamma(thetaGrid, shape = alpha_post_all, rate = beta_post_all)\n\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  posterior_uncensored = posterior_density_u,\n  posterior_all = posterior_density_all\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior_uncensored\", \"posterior_all\"), \n    values = c(colors[2], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.2\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThe code below plots the histogram and the pdf of the exponential model with the parameter \\(\\theta\\) set equal to the posterior mode. It is clear that the exponential model with its monotonically decreasing density is not fitting the data well.\n\npostMode = df$thetaGrid[which.max(df$posterior_all)]\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dexp, args = list(rate = postMode), lwd = 1, \n                aes(color = \"Exponential fit\"),\n  ) +\n  labs(title = \"Exercise 2.2c - Exponential model fit to lung cancer survival\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[5]) +\n  scale_color_manual(\"\", values = colors[3]) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.10\nShow that the \\(N(\\mu,1)\\) distribution belongs to the exponential family.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.10\nExercise modelled the survival times of uncensored lung cancer patients with an iid exponential model. Here we extend that analysis to the more general Weibull distribution: \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] The value of \\(k\\) determines how the failure rate changes with time:\nPlot the posterior distribution of \\(\\lambda\\) conditional on \\(k=1\\), \\(k=3/2\\) and \\(k=2\\). For all \\(k\\), use the prior \\(\\lambda \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) with \\(\\alpha=3\\) and \\(\\beta=1/50\\) (which is a similar prior for \\(\\theta=1/\\lambda\\) as in Exercise 2.2). Plot the time variable as a histogram and overlay the fitted model for the three different \\(k\\)-values; use the posterior mode for \\(\\theta\\) in each model when plotting the fitted model density.\n\\(\\textit{Hint}\\): the posterior distribution for \\(k\\neq 1\\) is intractable, so use numerical evaluation of the posterior over a grid of \\(\\lambda\\)-values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood can be computed with separate treatment of the uncensored and censored observations: \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\lambda, k) & = \\prod_{i=1}^n p(x_i \\vert \\lambda, k) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\lambda, k) \\prod_{c \\in \\mathcal{C}} \\Big(1 - F(x_c \\vert \\lambda, k)\\Big)\n\\end{align}\n\\] where \\(p(x \\vert \\lambda, k)\\) is the pdf of a Weibull variable \\[\np(x \\vert \\lambda, k) = \\frac{k}{\\lambda}\\Big( \\frac{x}{\\lambda} \\Big)^{k-1}e^{-(x/\\lambda)^k}\\quad\\text{ for }x&gt;0\n\\] which is implemented in R as dweibull. The cdf of the Weibull distribution is of rather simple form \\[\nF(x \\vert \\lambda, k) = 1 - e^{-(x/\\lambda)^k}\n\\] and is implemented in R as pweibull.\nThe code below plots the prior and posterior distribution for \\(\\lambda\\) for the three different \\(k\\)-values. We could have inserted the mathematical expressions for the pdf and cdf and simplified the final likelihood expression; we will instead use the dweibull and pweibull functions without simplifications since it gives a more general template that can be used for any distribution, not just the Weibull model. For numerical stability we usually compute the posterior distribution on the log scale \\[\n\\log p(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j)\n\\] for a grid of equally spaced \\(\\lambda\\)-values: \\(\\lambda^{(1)}\\ldots,\\lambda^{(J)}\\). The \\(\\propto\\) sign now means that there is a missing additive constant \\(\\log p(x_1,\\ldots,x_n)\\) which does not depend on the unknown parameter \\(\\lambda\\). When we have computed \\(\\log p(\\lambda \\vert x_1,\\ldots,x_n)\\) over a grid of \\(\\lambda\\) values we compute the posterior on the original scale by \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j) \\Big)\n\\] and then divide all numbers with the normalizing constant to make sure that the posterior integrates to one. This is done numerically by approximating the integral by a Riemann rectangle sum \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) =\n\\frac{\\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(j)}) + \\log p(\\lambda^{(j)}) \\Big)}\n{\\sum_{h=1}^J \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(h)}) + \\log p(\\lambda^{(h)}) \\Big) \\Delta}\n\\] where \\(\\Delta\\) is the spacing between the grid points of \\(\\lambda\\)-values: \\(\\lambda^{(1)}, \\ldots, \\lambda^{(J)}\\).\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet up prior hyperparameters\n\nalpha_prior &lt;- 3     # shape parameter\nbeta_prior &lt;- 1/50   # rate parameter\n\nSet up function that computes the likelihood for any \\(\\lambda\\) value:\n\n# Make a function that computes the likelihood\nweibull_loglike &lt;- function(lambda, x, censored, k){\n  loglik_uncensored = sum(dweibull(x[-censored], shape = k, scale = lambda, \n                                   log = TRUE))\n  loglik_censored = sum(pweibull(x[censored], shape = k, scale = lambda, \n                                 lower.tail = FALSE, log.p = TRUE))\n  return(loglik_uncensored + loglik_censored)\n}\n\nSet up a function that computes the posterior density over a grid of \\(\\lambda\\):\n\nweibull_posterior &lt;- function(lambdaGrid, x, censored, k, alpha_prior, beta_prior){\n  Delta = lambdaGrid[2] - lambdaGrid[1] # Grid step size\n  logPrior &lt;- dgamma(lambdaGrid, shape = alpha_prior, rate = beta_prior, log = TRUE)\n  logLike &lt;- sapply(lambdaGrid, weibull_loglike, x, censored, k)\n  logPost &lt;- logLike + logPrior\n  logPost &lt;- logPost - max(logPost) # subtract constant to avoid overflow\n  post &lt;- exp(logPost)/(sum(exp(logPost))*Delta) # original scale and normalize\n  logLike &lt;- logLike - max(logLike)\n  likeNorm &lt;- exp(logLike)/(sum(exp(logLike))*Delta) # normalized likelihood\n  return(list(post = post, prior = exp(logPrior), likeNorm = likeNorm))\n}\n\n\n# Plot the prior and posterior densities\n\nlambdaGrid &lt;- seq(200, 800, length.out = 1000)\n# Compute to get the prior\npostRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k = 1, \n                             alpha_prior, beta_prior)\ndf &lt;- data.frame(\n  lambdaGrid = lambdaGrid, \n  prior = postRes$prior\n)\n\n# Compute for all selected k values\npostModes = c()\nfor (k in c(1, 3/2, 2)){\n  postRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k, alpha_prior, beta_prior)\n  df[str_glue(\"posterior k={k}\")] &lt;- postRes$post\n  postModes = c(postModes, lambdaGrid[which.max(postRes$post)])\n}\n\ndf_long &lt;- df %&gt;% pivot_longer(-lambdaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = lambdaGrid, y = density, color = density_type) +\n  geom_line() +\n  xlim(250,600) +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior k=1\", \"posterior k=1.5\", \"posterior k=2\"), \n    values = c(colors[2], colors[1], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.3\", x = expression(lambda), y = \"Density\", color = \"\") + \n  theme_minimal()\n\nWarning: Removed 1668 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe fit of the three Weibull models are plotted below. The best fit seems to be for \\(k=3/2\\), but it is still not very good. In a later exercise you will be asked to freely estimate both \\(\\lambda\\) and \\(k\\), and even later to fit a Weibull regression model with covariates.\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dweibull, args = list(shape = 1, scale = postModes[1]), lwd = 1, \n                aes(color = \"Weibull fit k = 1\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 3/2, scale = postModes[2]), lwd = 1, \n                aes(color = \"Weibull fit k = 3/2\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 2, scale = postModes[3]), lwd = 1, \n                aes(color = \"Weibull fit k = 2\"),\n  ) +\n  labs(title = \"Weibull model fits\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[6]) +\n  scale_color_manual(\"\", values = c(colors[1], colors[3], colors[4])) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.11\nLet \\[\nX_1,\\ldots,X_n \\overset{iid}{\\sim } \\mathrm{Uniform}(0,\\theta).\n\\] Show that the Pareto prior \\(\\theta \\sim \\mathrm{Pareto}(\\alpha, \\beta)\\), is conjugate to the Uniform model by deriving the posterior distribution. See Box~ for a definition of the Pareto distribution and properties; note the support of the Pareto distribution.\n\\(\\textit{Hint}\\): Do not forget to include an indicator function when you write up the likelihood function since the \\(\\mathrm{Uniform}(0,\\theta)\\) distribution is zero for outcomes larger than \\(\\theta\\). Also, the Pareto prior is zero for values of \\(\\theta &lt; \\beta\\), so you need another indicator function for that.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe pdf of the \\(\\mathrm{Uniform}(0,\\theta)\\) distribution is \\[\\begin{equation*}\n  p(x) = \\begin{cases}\n          \\frac{1}{\\theta}  & \\text{if } x \\leq  \\theta \\\\\n          0                 & \\text{otherwise }\n          \\end{cases}\n\\end{equation*}\\] which can be written with an indicator function as \\[\np(x) = \\frac{1}{\\theta} I(x_i \\leq \\theta).\n\\] The likelihood function is therefore of the form \\[\\begin{equation*}\n   p(x_1,\\ldots,x_n\\vert\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} I(x_i \\leq \\theta) = \\frac{1}{\\theta^n} \\prod_{i=1}^n I(x_i \\leq \\theta).\n\\end{equation*}\\] The factor \\(\\prod_{i=1}^n I(x_i \\leq \\theta)\\) is only non-zero if all \\(x_1,\\ldots,x_n\\) are smaller or equal to \\(\\theta\\), i.e. if \\(x_{\\mathrm{max}} := \\mathrm{max}(x_1, \\dots,x_n)\\) is smaller or equal to \\(\\theta\\). We can therefore write the likelihood as \\[\\begin{equation*}\n   p(x_1,\\ldots,x_n\\vert\\theta) = \\frac{1}{\\theta^n}  I(x_{\\mathrm{max}} \\leq \\theta).\n\\end{equation*}\\]\nThe Pareto prior can also be written with an indicator function \\[\\begin{align*}\n    p(\\theta) = \\frac{\\alpha \\beta^\\alpha }{\\theta^{\\alpha+1}}\\cdot I(\\beta \\leq \\theta),\n\\end{align*}\\] to explicitly include that \\(p(\\theta)=0\\) if \\(\\theta &lt; \\beta\\).\nBy Bayes’ theorem, the posterior is then \\[\\begin{align*}\n    p(\\theta \\vert x_1, \\dots,x_n) &\\propto p(x_1, \\dots,x_n \\vert \\theta)p(\\theta) \\\\\n                       &= \\frac{1}{\\theta^n} I(x_{\\mathrm{max}} \\leq \\theta)\n                            \\frac{\\alpha \\beta^\\alpha }{\\theta^{\\alpha+1}}\\cdot I(\\beta \\leq \\theta) \\\\\n                       &=  \\frac{\\alpha \\beta^\\alpha }{\\theta^{n+\\alpha+1}} \\cdot  \n                            I\\big(\\tilde \\beta \\leq \\theta \\big)        \n\\end{align*}\\] where \\(\\tilde \\beta = \\mathrm{max}(x_{\\mathrm{max}},\\beta)\\). This is proportional to a \\(\\mathrm{Pareto}\\big(\\alpha + n, \\tilde \\beta \\big)\\) density. Since the prior and posterior are both in the Pareto family, the Pareto prior is conjugate to the \\(\\mathrm{Uniform}(0,\\theta)\\) model.\n\n\n\n\n\n\nExercise 2.12\nThe number of patients per day in need of an ambulance to a small regional hospital is random following a Poisson distribution with unknown mean \\(\\theta\\). The hospital has collected data from the past \\(n=7\\) days, resulting in the following daily counts of ambulance trips: \\(8, 12,  6,  9,  9,  9,  5\\). Before collecting the data, the head of the ambulance drivers believed that the mean number of ambulance trips per day is about \\(12\\) and that a mean of less than \\(5\\) trips is very unlikely, “like winning a lottery with only one winning ticket per hundred tickets”.\na)\nCompute the posterior distribution for \\(\\theta\\) using a Gamma prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) with parameters \\(\\alpha\\) and \\(\\beta\\) set to reflect the head’s beliefs. Provide a plot of the prior distribution, the normalized likelihood and the posterior distribution.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first need to determine the two hyperparameters \\(\\alpha\\) and \\(\\beta\\) of the \\(\\mathrm{Gamma}(\\alpha,\\beta)\\) prior. From properties of the gamma distribution we have \\[\n\\mathbb{E}(\\theta) = \\frac{\\alpha}{\\beta} = 12\n\\] to match the prior mean of the head. We must therefore have \\(\\alpha = 12\\beta\\), leaving the single hyperparameter \\(\\beta\\) to be determined from the second condition: \\[\n\\mathrm{Pr}(\\theta &lt; 5) = 0.01\n\\] Let \\(F(\\theta \\vert \\alpha,\\beta)\\) denote the cumulative distribution function (cdf) of the \\(\\mathrm{Gamma}(\\alpha,\\beta)\\) prior. Since \\(\\alpha = 12\\beta\\) we need to solve the following equation for \\(\\beta\\): \\[\nF(5 \\vert 12\\beta,\\beta) = 0.01\n\\] In R code, we would like to find the beta this pgamma(5, shape = 12*beta, rate = beta) = 0.01 This cannot be (easily) solved analytically. We can either find the \\(\\beta\\) that satisfies pgamma(5, shape = 12*beta, rate = beta) = 0.01 by trial and error (that is, try different values of beta until you get really close to pgamma(5, shape = 12*beta, rate = beta) = 0.01). A faster way is to use a numerical method (for example Newton’s method) that finds roots to equations \\(f(x) = 0\\) (a so called root-finding algorithm), since our problem can be formulated as finding the \\(\\beta\\) that solves the equation \\(F(5 \\vert 12\\beta,\\beta) - 0.01 = 0\\). The uniroot function in R does the job:\n\nf &lt;- function(x){\n  return(pgamma(5, shape = 12*x, rate = x) - 0.01)\n}\nresults = uniroot(f, c(0.001,100)) # second argument is a search interval\nbetaPrior = results$root \nalphaPrior = 12*betaPrior\n\nThe value of \\(\\beta\\) that satisfies the equation \\(F(5 \\vert 12\\beta,\\beta) = 0.01\\) is \\(\\beta = 0.8472998\\). We can check that this \\(\\beta\\) indeed gives the correct prior probability \\(\\mathrm{Pr}(\\theta &lt; 5) = 0.01\\):\n\npgamma(5, shape = alphaPrior, rate = betaPrior)\n\n[1] 0.01000101\n\n\nBingo!\nSince \\(\\alpha = 12\\beta\\), the complete prior is \\(\\theta \\sim \\mathrm{Gamma}(\\alpha = 10.168, 0.847)\\). Here is a plot:\n\nthetaGrid = seq(0, 30, length = 1000)\nplot(thetaGrid, dgamma(thetaGrid, shape = alphaPrior, rate = betaPrior), \n     type = \"l\", ylab = \"prior density\", xlab = expression(theta), \n     col = \"orange\", lwd = 2)\n\n\n\n\n\n\n\n\nSanity check: the mean of \\(12\\) seems right, and also that \\(\\mathrm{Pr}(\\theta &lt; 5) = 0.01\\).\nFrom Chapter 2 of the Bayesian Learning book, we know that the posterior distribution is \\[\n\\theta \\vert \\boldsymbol{x} \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n x_i, \\beta + n)\n\\] So, since \\(\\sum_{i=1}^7 x_i = 58\\) we have\n\nx = c(8,12,6,9,9,9,5)\nn = length(x)\nalphaPost = alphaPrior + sum(x)\nbetaPost = betaPrior + n\nnTheta = 1000\nthetaGrid = seq(0, 15, length = nTheta)\nbinWidth = thetaGrid[2] - thetaGrid[1] # used to normalize likelihood\npriorDens = dgamma(thetaGrid, alphaPrior, betaPrior)\npostDens = dgamma(thetaGrid, alphaPost, betaPost)\nlike = rep(0, nTheta)\nfor (i in 1:nTheta){\n  like[i] = prod(dpois(x, thetaGrid[i]))\n}\nlike_norm = like/sum(binWidth*like)\nplot(thetaGrid, dgamma(thetaGrid, alphaPost, betaPost), \n     xlab = expression(theta), ylab = \"density\", type = \"l\", \n     col = \"orange\", lwd = 3)\nlines(thetaGrid, priorDens, col = \"orange\", lwd = 3)\nlines(thetaGrid, like_norm, col = \"steelblue\", lwd = 3)\nlines(thetaGrid, postDens, col = \"indianred\", lwd = 3)\nlegend(\"topleft\", legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),\n       col = c(\"orange\", \"steelblue\", \"indianred\"), lwd = 3, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\nb)\nCompare the posterior mean, median and mode, and summarize the posterior distribution by a 95% credible interval.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince the posterior is Gamma, we can look formulas for the mean and mode of a Gamma distribution. If \\(X\\sim \\mathrm{Gamma}(\\alpha, \\beta)\\), then \\[\n\\begin{aligned}\n\\mathbb{E}(X) &= \\frac{\\alpha}{\\beta} \\\\\n\\mathrm{Mode}(X) &= \\frac{\\alpha -1}{\\beta} \\text{ for } \\alpha \\geq 1, \\text{ zero otherwise}\n\\end{aligned}  \n\\] The median is not available in closed from, but R gladly computes it with the \\(qgamma\\) function. Here is posterior mean, median and mode in the ambulance data example:\n\npostMean = alphaPost/betaPost\npostMedian = qgamma(0.5, alphaPost, betaPost)\npostMode = (alphaPost - 1)/betaPost\nmessage(\"The posterior mean is \", round(postMean,3))\n\nThe posterior mean is 8.687\n\nmessage(\"The posterior median is \", round(postMedian,3))\n\nThe posterior median is 8.644\n\nmessage(\"The posterior mode is \", round(postMode,3))\n\nThe posterior mode is 8.559\n\n\nSince the posterior distribution is fairly symmetric, these three measures of posterior location are similar.\nFinally, a 95% credible interval can be computed by the following code (there are many ways of doing this). The code assumes for simplicity that the HPD interval is truly an interval, and not several disjoint intervals, as would happen when the posterior is multimodal. We know from the plot above that the posterior is unimodal, so the code is safe to use here.\n\n# first, sort the density values from highest to lowest\npostDensOrdered = sort(postDens, decreasing = TRUE)  \n# reorder the thetaValues so that they still match the density values\nthetaOrdered = thetaGrid[order(postDens, decreasing = TRUE)] \ncumsumPostDens = cumsum(binWidth*postDensOrdered) # posterior cdf \ninHPD = which(cumsumPostDens &lt; 0.95) # find highest pdf vals up to 95% quota.\nhpd = c(min(thetaOrdered[inHPD]), max(thetaOrdered[inHPD]))\nmessage(paste0(\"The 95% HPD interval for theta is (\", hpd[1], \",\", hpd[2],\")\"))\n\nThe 95% HPD interval for theta is (6.68168168168168,10.7657657657658)\n\n\nPlot the HPD as a blue line:\n\nplot(thetaGrid, postDens, type = \"l\", col = \"indianred\", lwd = 2, \n     xlab = expression(theta), ylab = \"posterior density\") \nlines(hpd, c(0,0), col = \"steelblue\", lwd = 3)"
  },
  {
    "objectID": "exercises/ch2/expon_post.html",
    "href": "exercises/ch2/expon_post.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.1\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\) be iid exponentially distributed data. Show that the Gamma distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution Exercise 2.1\n\n\n\n\n\nThe likelihood from an iid sample from \\(\\mathrm{Expon}(\\theta)\\) is \\[\np(x_1,\\ldots,x_n \\vert \\theta)= \\prod_{i=1}^n p(x_i \\vert \\theta) =\n  \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\n\\] The density of the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) prior is \\[\np(\\theta) =  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\theta^{\\alpha-1}e^{-\\beta\\theta}\n             \\propto \\theta^{\\alpha-1}e^{-\\beta\\theta}\n\\]\nBy Bayes’ theorem, the posterior distribution is \\[\n\\begin{align}\n  p(\\theta \\vert x_1,\\ldots,x_n) &\\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta)   \\\\\n& \\propto \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\\theta^{\\alpha-1}e^{-\\beta\\theta}  \\\\\n& =  \\theta^{\\alpha + n - 1} e^{ -\\theta(\\beta + \\sum_{i=1}^n x_i)},\n\\end{align}\n\\] which can be recognized as proportional to the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha +n,\\beta + \\sum\\nolimits_{i=1}^n x_i)\\) distribution. Since the prior and posterior belongs to the same (Gamma) distributional family, the Gamma prior is indeed conjugate to the exponential likelihood."
  },
  {
    "objectID": "notebooks/SpamBern/SpamBernR.html",
    "href": "notebooks/SpamBern/SpamBernR.html",
    "title": "Analyzing email spam data with a Bernoulli model",
    "section": "",
    "text": "a notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nThe SpamBase dataset from the UCI repository consists of \\(n=4601\\) emails that have been manually classified as spam (junk email) or ham (non-junk email).\nThe dataset also contains a vector of covariates/features for each email, such as the number of capital letters or $-signs; this information can be used to build a spam filter that automatically separates spam from ham.\nThis notebook analyzes only the proportion of spam emails without using the covariates.\n\n\nGetting started\nFirst, load libraries and setting up colors.\n\noptions(repr.plot.width=16, repr.plot.height=5, lwd = 4)\nlibrary(\"RColorBrewer\") # for pretty colors\nlibrary(\"tidyverse\")    # for string interpolation to print variables in plots.\nlibrary(\"latex2exp\")    # the TeX() function makes it possible to print latex math\ncolors = brewer.pal(12, \"Paired\")[c(1,2,7,8,3,4,5,6,9,10)];\n\n\n\nData\n\ndata = read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\", sep=\",\", header = TRUE)\nspam = data$X1 # This is the binary data where spam = 1, ham = 0.\nn = length(spam)\nspam = sample(spam, size = n) # Randomly shuffle the data.\n\n\n\nModel, Prior and Posterior\nModel \\[ X_1,\\ldots,X_n | \\theta \\sim \\mathrm{Bern}(\\theta)\\]\nPrior \\[\\theta\\sim\\mathrm{Beta}(\\alpha,\\beta)\\]\nPosterior \\[\\theta | x_1,\\ldots,x_n \\sim\\mathrm{Beta}(\\alpha+s,\\beta+f),\\]\nwhere \\(s=\\sum_{i=1}^n\\) is the number of ‘successes’ (spam) and \\(f=n-s\\) is the number of ‘failures’ (ham).\nLet us define a function that computes the posterior and plots it.\n\nBernPost &lt;- function(x, alphaPrior, betaPrior, legend = TRUE){\n    thetaGrid = seq(0,1, length = 1000)\n    n = length(x)\n    s = sum(x)\n    f = n - s\n    alphaPost = alphaPrior + s\n    betaPost = betaPrior + f\n    priorPDF = dbeta(thetaGrid, alphaPrior, betaPrior)\n    normLikePDF = dbeta(thetaGrid, s + 1, f + 1) # Trick to get the normalized likelihood\n    postPDF = dbeta(thetaGrid, alphaPost, betaPost)\n    \n    plot(1, type=\"n\", axes=FALSE, xlab = expression(theta), ylab = \"\", \n         xlim=c(min(thetaGrid),max(thetaGrid)), \n         ylim = c(0,max(priorPDF,postPDF,normLikePDF)), \n         main = TeX(sprintf(\"Prior: $\\\\mathrm{Beta}(\\\\alpha = %0.0f, \\\\beta = %0.0f)\", alphaPrior, betaPrior)))\n    axis(side = 1)\n    lines(thetaGrid, priorPDF, type = \"l\", lwd = 4, col = colors[6])\n    lines(thetaGrid, normLikePDF, lwd = 4, col = colors[2])\n    lines(thetaGrid, postPDF, lwd = 4, col = colors[4])\n    if (legend){\n        legend(x = \"topleft\", inset=.05,\n           legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),  \n           lty = c(1, 1, 1), pt.lwd = c(3, 3, 3), \n           col = c(colors[6], colors[2], colors[4]))\n    }\n    cat(\"Posterior mean is \", round(alphaPost/(alphaPost + betaPost),3), \"\\n\")\n    cat(\"Posterior standard deviation is \", round(sqrt(  alphaPost*betaPost/( (alphaPost+betaPost)^2*(alphaPost+betaPost+1))),3), \"\\n\")\n    return(list(\"alphaPost\" = alphaPrior + s, \"betaPost\" = betaPrior + f))\n}\n\nLet start by analyzing only the first 10 data points.\n\nn = 10\nx = spam[1:n]\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nSince we only have \\(n=10\\) data points, the posteriors for the three different priors differ a lot. Priors matter when the data are weak. Let’s try with the \\(n=100\\) first observations.\n\nn = 100\nx = spam[1:n]\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nThe effect of the prior is now almost gone. Finally let’s use all \\(n=4601\\) observations in the dataset:\n\nx = spam\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nWe see two things: * The effect of the prior is completely gone. All three prior give identical posteriors. We have reached a subjective consensus among the three persons. * We are quite sure now that the spam probability \\(\\theta\\) is around \\(0.4\\).\nA later notebook will re-analyze this data using for example logistic regression."
  },
  {
    "objectID": "notebooks/SpamLogisticReg/SpamLogisticReg.html",
    "href": "notebooks/SpamLogisticReg/SpamLogisticReg.html",
    "title": "Posterior approximation - logistic regression",
    "section": "",
    "text": "Load packages\n\n# install.packages(\"mvtnorm\") \n# install.packages(\"RColorBrewer\") \nlibrary(mvtnorm) # package with multivariate normal density\nlibrary(RColorBrewer) # just some fancy colors for plotting\nprettyCol = brewer.pal(10,\"Paired\")\n\n\n\nSettings\n\nchooseCov &lt;- c(1:16) # covariates to include in the model\ntau &lt;- 10;           # Prior std beta~N(0,tau^2*I)\n\n\n\nReading data and setting up the prior\n\nData&lt;-read.table(\"https://raw.githubusercontent.com/mattiasvillani/BayesLearnCourse/master/Notebooks/R/SpamReduced.dat\",header=TRUE) # Reduced spambase data (http://archive.ics.uci.edu/ml/datasets/Spambase/)\ncovNames &lt;- names(Data)[2:length(names(Data))]; # Read off the covariate names\ny &lt;- as.vector(Data[,1]); \nX &lt;- as.matrix(Data[,2:17]);\nX &lt;- X[,chooseCov];                             # Pick out the chosen covariates \ncovNames &lt;- covNames[chooseCov];                # ... and their names\nnPara &lt;- dim(X)[2];\n\n# Setting up the prior\nmu &lt;- as.vector(rep(0,nPara)) # Prior mean vector\nSigma &lt;- tau^2*diag(nPara);\n\n\nCoding up the log posterior function\n\nLogPostLogistic &lt;- function(betaVect,y,X,mu,Sigma){\n  nPara &lt;- length(betaVect);\n  linPred &lt;- X%*%betaVect;\n  logLik &lt;- sum( linPred*y -log(1 + exp(linPred)));\n  logPrior &lt;- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);\n  return(logLik + logPrior)\n}\n\n\n\nFinding the mode and observed information using optim\n\ninitVal &lt;- as.vector(rep(0,nPara)); \nOptimResults&lt;-optim(initVal,LogPostLogistic,gr=NULL,y,X,mu,Sigma,\n  method=c(\"BFGS\"), control=list(fnscale=-1),hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covariance matrix\npostStd &lt;- sqrt(diag(postCov)) # Computing approximate stdev\nnames(postMode) &lt;- covNames      # Naming the coefficient by covariates\nnames(postStd) &lt;- covNames # Naming the coefficient by covariates\n\n\n\n\nThe posterior mode is\n\nprint(postMode)\n\n          our          over        remove      internet          free \n 0.4182337582  1.1753728476  2.9209159589  0.9696191548  1.2944179828 \n          hpl            X.          X..1     CapRunMax   CapRunTotal \n-1.3114765304  0.5673271835  8.2721841199  0.0118045995  0.0005570864 \n        const            hp        george         X1999            re \n-1.4278739763 -2.0411544503 -6.0021765135 -0.4565997686 -0.8577822552 \n          edu \n-1.6854611460 \n\n\n\n\nThe posterior standard deviations are computed from the covariance\n\nprint(postStd)\n\n         our         over       remove     internet         free          hpl \n0.0730320059 0.2321086478 0.3302456199 0.1671111765 0.1412670451 0.4017479109 \n          X.         X..1    CapRunMax  CapRunTotal        const           hp \n0.0947016268 0.6851475429 0.0017545736 0.0001418867 0.0847302222 0.2998192165 \n      george        X1999           re          edu \n1.1494146395 0.1902088194 0.1476136565 0.2554459768 \n\n\n\n\nPlot the marginal posterior of \\(\\beta\\) for the free and hpl covariates\n\npar(mfrow=c(1,2))\ngridVals = seq(postMode['free']-3*postStd['free'], postMode['free']+3*postStd['free'], \n               length = 100)\nplot(gridVals, dnorm(gridVals, mean = postMode['free'], sd = postStd['free']), \n     xlab = expression(beta), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n     lwd = 2, col = prettyCol[2], main = expression(beta[free]))\ngridVals = seq(postMode['hpl']-3*postStd['hpl'], postMode['hpl']+3*postStd['hpl'], \n               length = 100)\nplot(gridVals, dnorm(gridVals, mean = postMode['hpl'], sd = postStd['hpl']), \n     xlab = expression(beta), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n     lwd = 2, col = prettyCol[2], main = expression(beta[hpl]))\n\n\n\n\n\n\n\n\n\n\nSimulate from normal approximation and make prediction at mean covariate\n\nxStar = colMeans(X)\nnSim = 1000\nprobSpam = rep(0,nSim)\nspamPred = rep(0,nSim)\nfor (i in 1:nSim){\n  betaDraw = as.vector(rmvnorm(1, postMode, postCov)) # Simulate a beta draw from approx post\n  linPred = t(xStar)%*%betaDraw\n  probSpam[i] = exp(linPred)/(1+exp(linPred)) # draw from posterior of Pr(spam|x)\n  spamPred[i] = rbinom(n=1,size=1,probSpam[i]) # draw from model given probSpam[i]\n}\npar(mfrow=c(1,2))\nhist(probSpam, freq = FALSE, xlab = expression(theta[i]), ylab= \"\", col = prettyCol[3],\n     main = \"Posterior distribution for Pr(spam|x)\", cex.main = 0.7)\nbarplot(c(sum(spamPred==0),sum(spamPred==1))/nSim, names.arg  = c(\"ham\",\"spam\"), col = prettyCol[7],\n     main = \"Predictive distribution spam\", cex.main = 0.7)"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/LocalLevelNileDataPython-Copy1.html",
    "href": "notebooks/KalmanFilteringSmoothing/LocalLevelNileDataPython-Copy1.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "%matplotlib inline\n\nfrom importlib import reload\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import invwishart, invgamma, norm\n\n# Get the macro dataset\nnile = pd.read_csv('nile.csv')\n\n\n\"\"\"\nUnivariate Local Linear Trend Model\n\"\"\"\nclass LocalLinearTrend(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 2\n\n        # Initialize the statespace\n        super(LocalLinearTrend, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1, 0])\n        self.ssm['transition'] = np.array([[1, 1],\n                                       [0, 1]])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level', 'sigma2.trend']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*3\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearTrend, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]\n\n\n\"\"\"\nUnivariate Local Linear Level Model\n\"\"\"\nclass LocalLinearLevel(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 1\n\n        # Initialize the statespace\n        super(LocalLinearLevel, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1])\n        self.ssm['transition'] = np.array([[1]])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*2\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearLevel, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]\n\n\nmod = LocalLinearLevel(nile['flow'])\ninitial_state = np.array([1000])  # Example: Modify based on your model's state dimension\ninitial_state_cov = np.eye(1) * 1000**2  # Small non-zero covariance for each state\n\n# Initialize the model with known values\nmod.initialize_known(initial_state, initial_state_cov)\n\nconstraints = {'sigma2.measurement': 100**2, 'sigma2.level': 100**2}\n# Fit the model with constraints\nres = mod.fit_constrained(constraints)\n\n#res = mod.fit(disp=False)\nprint(res.summary())\n\n                           Statespace Model Results                           \n==============================================================================\nDep. Variable:                   flow   No. Observations:                  100\nModel:               LocalLinearLevel   Log Likelihood                -636.763\nDate:                Thu, 29 Feb 2024   AIC                           1273.526\nTime:                        18:47:39   BIC                           1273.526\nSample:                             0   HQIC                          1273.526\n                                - 100                                         \nCovariance Type:                  opg                                         \n==============================================================================================\n                                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nsigma2.measurement (fixed)      1e+04        nan        nan        nan         nan         nan\nsigma2.level (fixed)            1e+04        nan        nan        nan         nan         nan\n===================================================================================\nLjung-Box (L1) (Q):                   2.16   Jarque-Bera (JB):                 0.40\nProb(Q):                              0.14   Prob(JB):                         0.82\nHeteroskedasticity (H):               0.65   Skew:                             0.13\nProb(H) (two-sided):                  0.22   Kurtosis:                         2.83\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nmod = LocalLinearLevel(nile['flow'])\n\nres = mod.fit(disp=False)\nprint(res.summary())\n\n                           Statespace Model Results                           \n==============================================================================\nDep. Variable:                   flow   No. Observations:                  100\nModel:               LocalLinearLevel   Log Likelihood                -632.538\nDate:                Thu, 29 Feb 2024   AIC                           1269.075\nTime:                        18:33:54   BIC                           1274.266\nSample:                             0   HQIC                          1271.175\n                                - 100                                         \nCovariance Type:                  opg                                         \n======================================================================================\n                         coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nsigma2.measurement  1.513e+04   2591.445      5.837      0.000       1e+04    2.02e+04\nsigma2.level        1461.9955    843.753      1.733      0.083    -191.730    3115.721\n===================================================================================\nLjung-Box (L1) (Q):                   1.36   Jarque-Bera (JB):                 0.05\nProb(Q):                              0.24   Prob(JB):                         0.98\nHeteroskedasticity (H):               0.61   Skew:                            -0.03\nProb(H) (two-sided):                  0.16   Kurtosis:                         3.08\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Construct a local level model for inflation\nmod = sm.tsa.UnobservedComponents(nile.flow, 'llevel')\n\n# Fit the model's parameters (sigma2_varepsilon and sigma2_eta)\n# via maximum likelihood\nres = mod.fit()\nprint(res.params)\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method\n\nsigma2.irregular    15078.011658\nsigma2.level         1478.811445\ndtype: float64\n\n\n\nnsimulations = 20\nsimulated_state_kfs = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=dta.index)\nsimulated_state_cfa = pd.DataFrame(\n    np.zeros((mod.nobs, nsimulations)), index=dta.index)\n\nfor i in range(nsimulations):\n    # Apply KFS simulation smoothing\n    sim_kfs.simulate()\n    # Save the KFS simulated state\n    simulated_state_kfs.iloc[:, i] = sim_kfs.simulated_state[0]\n\n    # Apply CFA simulation smoothing\n    sim_cfa.simulate()\n    # Save the CFA simulated state\n    simulated_state_cfa.iloc[:, i] = sim_cfa.simulated_state[0]\n\n\nmod\n\n&lt;statsmodels.tsa.statespace.structural.UnobservedComponents at 0x70a3e514c130&gt;"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "",
    "text": "This tutorial gives a very brief introduction to state-space models, along with inference methods like Kalman filtering, smoothing and forecasting. The methods are illustrated using the R package dlm , exemplified with the local level model fitted to the well-known Nile river data. The tutorial is also sprinkled with some cool interactivity in Javascript."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#piecewise-constant-model",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#piecewise-constant-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Piecewise constant model",
    "text": "Piecewise constant model\nAn extremely simple model for a time series is to treat the observations as independent normally distributed with the same mean \\(\\mu\\) and variance \\(\\sigma_\\varepsilon\\)\n\\[\ny_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\n\nShow the code\n#install.packages(\"latex2exp\")\nlibrary(latex2exp)\nn = 200\nmu = 2\nsigma_eps = 1\ny = rnorm(n, mean = mu, sd = sigma_eps)\nplot(seq(1,n), y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y_t\", lwd = 1.5,\n    main = \"Simulated data from the naive iid model\")\nlines(seq(1,n), rep(mu,n), type = \"l\", col = \"orange\")\nlegend(\"topright\", legend = c(TeX(\"$y_t$\"), TeX(\"$\\\\mu$\")), lty = 1, lwd = 1.5, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nThis model is of course not something to write home about, it basically ignores the time series nature of the data. Let us start to make it a little more interesting by allowing the mean to vary of time. This means that we will have a time-varying parameter model where the mean \\(\\mu_t\\) changes (abruptly) at certain time points \\(t_1, t_2, \\dots, t_K\\):\n\\[\ny_t = \\mu_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)\n\\]\n\\[\n\\begin{align}   \n\\mu_t &=\n\\begin{cases}            \n  \\mu_1 & \\text{if $1 \\leq t \\leq t_1$} \\\\\n  \\mu_2 & \\text{if $t_1 &lt; t \\leq t_2$} \\\\            \n  \\vdots & \\vdots \\\\\n  \\mu_K & \\text{if $t_{K-1} &lt; t \\leq T$}. \\\\          \n\\end{cases}\n\\end{align}\n\\]\nHere is a widget that lets you simulate data from the piecewise constant model1."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#local-level-model",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#local-level-model",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Local level model",
    "text": "Local level model\nThe piecewise constant model has a few abrupt changes in the mean, but what if the mean changes more gradually? The local level model has a constantly changing mean following a random walk model:\n\\[y_t = \\mu_t + \\varepsilon_t,\\qquad \\varepsilon_t \\sim N(0,\\sigma_\\varepsilon^2)\\]\n\\[\\mu_t = \\mu_{t-1} + \\eta_t,\\qquad \\eta_t \\sim N(0,\\sigma_\\eta^2)\\]\nwhich models the observed time series \\(y_t\\) as a mean \\(\\mu_t\\) plus a random measurement error or disturbance \\(\\varepsilon_t\\). The mean \\(\\mu_t\\) evolves over time as a random walk driven by innovations \\(\\eta_t\\).\nHere is a widget that simulates data from the model. Go ahead, experiment with the measurement/noise \\(\\sigma_\\varepsilon\\) and the standard deviation of the innovations to the mean process, \\(\\sigma_\\eta\\). For example, drive \\(\\sigma_\\eta\\) toward zero and note how the mean becomes close to constant over time."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#regression-with-time-varying-parameters",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#regression-with-time-varying-parameters",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Regression with time-varying parameters",
    "text": "Regression with time-varying parameters\nThe usual simple linear time series regression model is\n\\[\ny_t = \\alpha + \\beta x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2) \\qquad t=1,\\ldots,T\n\\]\nwhere \\(y_t\\) is a time series response variable (for example electricity price) that is being explained by the explanatory variable \\(x_t\\) (for example temperature). This model assumes that the parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma_\\varepsilon\\) are constant in time, that the relationship between electricity price and temperature has remained the same throughout the whole observed time period.\nIt sometimes makes sense to let the parameters vary with time. Here is one such model, the time-varying regression model:\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nwhere the intercept \\(\\alpha\\) now has a time \\(t\\) subscript and evolves in time following a random walk process\n\\[\\alpha_{t} = \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)\\]\nso that in every time period, the intercept changes by adding on an innovation \\(\\eta_t\\) drawn from a normal distribution with standard deviation \\(\\sigma_\\alpha\\). This standard deviation therefore controls how much the intercept changes over time. The slope \\(\\beta\\) changes over time in a similar fashion, with the speed of change determined by \\(\\sigma_\\beta\\).\nHere is a widget that simulates data from the time-varying regression above. By moving the slider (show regline at time) you can plot the regression line \\(\\alpha_t + \\beta_t x_t\\) at any time period \\(t\\). The plot highlights (darker blue) data points that are closer in time to the time chosen by the slider. To the left you can see the whole time path of the simulated \\(\\alpha\\) and \\(\\beta\\) with the current parameters highlighted by dots."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#state-space-model---filtering-smoothing-and-forecasting",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#state-space-model---filtering-smoothing-and-forecasting",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "State-space model - filtering, smoothing and forecasting",
    "text": "State-space model - filtering, smoothing and forecasting\n\nThe state space model\nAll of the models above, and many, many, many more can be written as a so called state-space model. A state-space model for a univariate time series \\(y_t\\) with a state vector \\(\\boldsymbol{\\theta}_t\\) can be written as\n\\[\n\\begin{align}\ny_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + v_t,\\hspace{1.5cm} v_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\] where we have written the multivariate distribution \\(N(\\boldsymbol{0},\\boldsymbol{V})\\) for \\(v_t\\), even though it is actually a scalar here, to be consistent with the notation used later.\nFor example, the local level model is a state-space model with a single scalar state variable \\(\\boldsymbol{\\theta}_t = \\mu_t\\) and parameters\n\\[\n\\begin{align}\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\nu^2\n\\end{align}\n\\]\nWe learn about the state \\(\\mu_t\\) from the observed time series \\(y_t\\) . The first equation is often called the observation or measurement model since it gives the connection between the unobserved state and the observed measurements. The measurements can also be a vector, but we will use a single measurement in this tutorial. The second equation is called the state transition model since it determines how the state evolves over time.\nWe can even let the state-space parameters \\(\\boldsymbol{F}, \\boldsymbol{G}, \\boldsymbol{V}, \\boldsymbol{W}\\) be different in every time period. This is in fact needed if we want to write the time-varying regression model in state-space form. Recall the time varying regression model\n\\[\n\\begin{align}  \ny_t &= \\alpha_{t} + \\beta_{t} x_t  + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\  \n\\alpha_{t} &= \\alpha_{t-1} + \\eta_t, \\qquad \\quad \\eta_t \\sim N(0, \\sigma_\\alpha^2)   \\\\  \n\\beta_{t} &= \\beta_{t-1} + \\nu_t, \\qquad \\quad \\nu_t \\sim N(0, \\sigma_\\beta^2)\n\\end{align}\n\\]\nWe can tuck the two time-varying parameters in a vector \\(\\boldsymbol{\\beta}=(\\alpha_t,\\beta_t)^\\top\\) and also write the models as\n\\[\n\\begin{align}  \ny_t &= \\boldsymbol{x}_t^\\top\\boldsymbol{\\beta}_{t}   + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\sim N(0, \\sigma_\\varepsilon^2)  \\\\    \n\\boldsymbol{\\beta}_{t} &= \\boldsymbol{\\beta}_{t-1} + \\boldsymbol{w}_t, \\quad \\quad \\nu_t \\sim N(0, \\boldsymbol{W})\n\\end{align}\n\\]\nwhere\n\\[\n\\begin{align}  \n\\boldsymbol{x}_t &= (1,x_t)^\\top  \\\\    \n\\boldsymbol{w}_t &= (\\eta_t,\\nu_t)^\\top  \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\eta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nNote that this is a state-space model with\n\\[\n\\begin{align}\n\\boldsymbol{F}_t &= \\boldsymbol{x}_t\\\\\n\\boldsymbol{G} &=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix} \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &=\n\\begin{pmatrix}\n\\sigma_\\alpha^2 & 0 \\\\\n0               & \\sigma_\\beta^2\n\\end{pmatrix}\n\\end{align}\n\\]\nand note now that \\(\\boldsymbol{F}\\) changes in every time period, hence the subscript \\(t\\).\nFinally, we can also have multivariate response vector \\(\\boldsymbol{y}_t\\)\nas\n\\[\n\\begin{align}\n\\boldsymbol{y}_t &= \\boldsymbol{F} \\boldsymbol{\\theta}_t + \\boldsymbol{v}_t,\\hspace{1.5cm} \\boldsymbol{v}_t \\sim N(\\boldsymbol{0},\\boldsymbol{V})  \\\\\n\\boldsymbol{\\theta}_t &= \\boldsymbol{G} \\boldsymbol{\\theta}_{t-1} + \\boldsymbol{w}_t, \\qquad \\boldsymbol{w}_t \\sim N(\\boldsymbol{0},\\boldsymbol{W})\n\\end{align}\n\\]\n\n\nFiltering and smoothing\nThere are two different types of relevant inferences in state-space models: filtering and smoothing:\n\nThe filtered estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(t\\).\nThe smoothed estimate \\(\\hat{\\boldsymbol{\\theta}}_{t|T}\\) of the state \\(\\boldsymbol{\\theta}_t\\) uses data up to time \\(T\\), the end of the time series.\n\nThe filtered estimate is therefore the instantaneous estimate, giving the best estimate of the current state. The smoothed estimate is the retrospective estimate that looks back in time and gives us the best estimate using all the data.\nFiltering means to compute the sequence of instantaneous estimates of the unobserved state at every time point \\(t=1,2,\\ldots,T\\)\n\\[\n\\hat{\\boldsymbol{\\theta}}_{1|1},\\hat{\\boldsymbol{\\theta}}_{2|2},\\ldots,\\hat{\\boldsymbol{\\theta}}_{T|T}\n\\]\nWe will take a time series and compute the filtered estimates for the whole time series, but it is important to understand that filtering is often done in real-time, which means it is a continuously ongoing process that returns filtered estimates of the state \\(\\boldsymbol{\\theta}_t\\) as time progresses and new measurements \\(y_t\\) come in. Think about a self-driving car that is continuously trying to understand the environment (people, other cars, the road conditions etc). The environment is the state and the car uses its sensors to collect measurements. The filtering estimates tells the car about the best guess for the environment at every point in time.\nFor state-space models of the type discussed here (linear measurement equation and linear evolution of the state, with independent Normal measurement errors and state innovations), the filtered estimates can be computed with one of the most famous algorithms in statistics: the Kalman filter.\nThe Kalman filter is a little messy to write up if you are shaky on vectors and matrices, but we will do it for completeness. We will however use a package for it so don’t worry if the linear algebra is intidimating. We will use the notation $\\(\\boldsymbol{\\mu}_{t|t}\\) instead of \\(\\hat{\\boldsymbol{\\theta}}_{t|t}\\), but they really mean the same.\n\ntime \\(t = 0\\). The Kalman filter starts with mean \\(\\boldsymbol{\\mu}_{0|0}\\) and covariance matrix \\(\\boldsymbol{\\Omega}_{0|0}\\) for the state at time \\(t=0\\). Think about \\(\\boldsymbol{\\mu}_{0|0}\\) as the best guess \\(\\boldsymbol{\\theta}_0\\) of the state vector at time \\(t=0\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\) representing how sure we can be about this guess2.\ntime \\(t = 1\\). The Kalman filter then uses the first measurement \\(y_1\\) to update \\(\\boldsymbol{\\mu}_{0|0} \\rightarrow \\boldsymbol{\\mu}_{1|1}\\) and \\(\\boldsymbol{\\Omega}_{0|0} \\rightarrow \\boldsymbol{\\Omega}_{1|1}\\) to represent the estimate and the uncertainty for \\(\\boldsymbol{\\theta}_1\\), the state at time \\(t=1\\).\ntime \\(t = 2,...,T\\). It then continues in this fashion using the next measurement \\(y_2\\) to compute \\(\\boldsymbol{\\mu}_{2|2}\\) and \\(\\boldsymbol{\\Omega}_{2|2}\\) and so on all the way to the end of the time series to finally get \\(\\boldsymbol{\\mu}_{T|T}\\) and \\(\\boldsymbol{\\Omega}_{T|T}\\).\n\nHere is the Kalman filter algorithm:\n\n\nInitialization: set \\(\\boldsymbol{\\mu}_{0|0}\\) and \\(\\boldsymbol{\\Omega}_{0|0}\\)\nfor \\(t=1,\\ldots,T\\) do\n\nPrediction update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t-1} &= \\boldsymbol{G} \\boldsymbol{\\mu}_{t-1|t-1} \\\\  \n\\boldsymbol{\\Omega}_{t|t-1} &= \\boldsymbol{G}\\boldsymbol{\\Omega}_{t-1|t-1}  \\boldsymbol{G}^\\top + \\boldsymbol{W}\n\\end{align}\n\\]\nMeasurement update\\[\n\\begin{align}\n\\boldsymbol{\\mu}_{t|t} &= \\boldsymbol{\\mu}_{t|t-1} + \\boldsymbol{K}_t ( y_t - \\boldsymbol{F} \\boldsymbol{\\mu}_{t|t-1}  )  \\\\  \n\\boldsymbol{\\Omega}_{t|t} &= (\\boldsymbol{I} - \\boldsymbol{K}_t \\boldsymbol{F} )\\boldsymbol{\\Omega}_{t|t-1}\n\\end{align}\n\\]\n\n\nwhere \\[\\boldsymbol{K}_t = \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top ( \\boldsymbol{F} \\boldsymbol{\\Omega}_{t|t-1}\\boldsymbol{F}^\\top + \\boldsymbol{V})^{-1}\\] is the Kalman Gain.\n\nThe widget below lets you experiment with the Kalman filter for the local level model fitted to the Nile river data. In the widget we infer (filter) the local levels \\(\\mu_1,\\mu_2,\\ldots,\\mu_T\\) and can experiment with the measurement standard deviation \\(\\sigma_\\varepsilon\\), the standard deviation of the innovations to the local mean \\(\\sigma_\\eta\\), and also the initial guess for \\(\\mu_0\\) and the standard deviation \\(\\sigma_0\\) of that guess.\nHere are few things to try out in the widget below:\n\nIncrease the measurement standard deviation \\(\\sigma_\\varepsilon\\) and note how the filtered mean pays less and less attention to changes in the data (because the model believes that the data is very poor quality (noisy) and tells us basically nothing about the level). Then move \\(\\sigma_\\varepsilon\\) to smaller values and note how the filtered mean starts chasing the data (because the model believes that the data are super informative about the level).\nMake the standard deviation for the initial level \\(\\sigma_0\\) very small and then change the initial mean \\(\\mu_0\\) to see how this affects the filtered mean at the first part of the time series.\nMove the standard deviation of the innovations to the level \\(\\sigma_\\eta\\) small and note how the filtered mean becomes smoother and smoother over time."
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#the-dlm-package-in-r",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#the-dlm-package-in-r",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "The dlm package in R",
    "text": "The dlm package in R\nThe dlm package is a user-friendly R package for analyzing some state-space models. The package has a nice vignette that is worth reading if you plan to use the package more seriously.\n\nFiltering\nLet’s first do some filtering in the dlm package. Start by loading the dlm package:\n\n#install.packages(\"dlm\") # uncomment the first time to install.\nlibrary(dlm)\n\nWe now need to tell the dlm package what kind of state-space model we want to estimate. The means setting up the matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\). We will keep it simple and use the local level model as example, where all parameter matrices \\(\\boldsymbol{F}\\), \\(\\boldsymbol{G}\\), \\(\\boldsymbol{V}\\) and \\(\\boldsymbol{W}\\) are scalars (single numbers). As we have seen above, the local level model corresponds to a state-space model with parameters\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}_t &= \\mu_t \\\\\n\\boldsymbol{F} &= 1 \\\\\n\\boldsymbol{G} &= 1  \\\\\n\\boldsymbol{V} &= \\sigma_\\varepsilon^2 \\\\\n\\boldsymbol{W} &= \\sigma_\\eta^2\n\\end{align}\n\\]\nSo we only need to set \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) to start the fun. We will for now set \\(\\sigma_\\varepsilon^2 = 100^2\\) and \\(\\sigma_\\eta^2 = 100^2\\), and return to this when we learn how the dlm package can find maximum likelihood estimates for these parameters. Here is how you setup the local level model in the dlm package:\n\nmodel = dlm(FF = 1, V = 100^2, GG = 1, W = 100^2, m0 = 1000, C0 = 1000^2)\n\nCompute the filtering estimate using the Kalman filter and plot the result\n\nnileFilter &lt;- dlmFilter(Nile, model)\nplot(Nile, type = 'l', col = \"steelblue\")\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\n\nParameter estimation by maximum likelihood\nThe parameters \\(\\sigma_\\varepsilon^2\\) and \\(\\sigma_\\eta^2\\) were just set to some values above. Let’s instead estimate them by maximum likelihood. The function dlmMLE does this for us, but we need to set up a model build object so the dlm package knows which parameter to estimate. We reparameterize the two variances using the exponential function to ensure that the estimated variances are positive.\n\n modelBuild &lt;- function(param) {\n   dlm(FF = 1, V = exp(param[1]), GG = 1, W = exp(param[2]), m0 = 1000, C0 = 1000^2)\n }\n fit &lt;- dlmMLE(Nile, parm = c(10,10), build = modelBuild)\n\nWe need to take the exponential of the estimates to get the estimated variance parameters.\n\n exp(fit$par)\n\n[1] 15101.488  1467.014\n\n\nor the square roots, to get the maximum likelihood estimates of the standard deviations\n\nsqrt(exp(fit$par))\n\n[1] 122.88811  38.30162\n\n\nWe can redo the filter, this time using the maximum likelihood estimates of the parameters:\n\nmodel_mle = dlm(FF = 1, V = exp(fit$par[1]), GG = 1, W = exp(fit$par[2]), m0 = 1000, C0 = 1000^2)\nnileFilter &lt;- dlmFilter(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\"), lwd = 1.5, lty = 1, \n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\n\nSmoothing\nWe can also use the dlm package to compute the smoothed retrospective estimates of the local level \\(\\mu_t\\) at time \\(t\\) using all the data from \\(t=1\\) until the end of the time series \\(T\\). We haven’t showed the mathematical algorithm for smoothing, but you can look it up in many books. Anyway, here is the smoothing results for the Nile data, using the function dlmSmooth from the dlm package. The filtered estimates are also shown.\n\nnileSmooth &lt;- dlmSmooth(Nile, model_mle)\nplot(Nile, type = 'l', col = \"steelblue\", lwd = 1.5)\nlines(dropFirst(nileFilter$m), type = 'l', col = \"orange\", lwd = 1.5)\nlines(dropFirst(nileSmooth$s), type = 'l', col = \"red\", lwd = 1.5)\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Filtered\",\"Smoothed\"), lty = 1, lwd = 1.5, col = c(\"steelblue\", \"orange\", \"red\"))\n\n\n\n\n\n\n\n\n\n\nForecasting\nWe can also use state-space models for forecasting. Here is how it is done in the dlm package.\n\nnileFore &lt;- dlmForecast(nileFilter, nAhead = 5)\nsqrtR &lt;- sapply(nileFore$R, function(x) sqrt(x))\npl &lt;- nileFore$a[,1] + qnorm(0.05, sd = sqrtR)\npu &lt;- nileFore$a[,1] + qnorm(0.95, sd = sqrtR)\nx &lt;- ts.union(window(Nile, start = c(1900, 1)),\n              window(nileSmooth$s, start = c(1900, 1)), \n              nileFore$a, pl, pu)\n\nplot(x, plot.type = \"single\", type = 'o', pch = c(NA, NA, NA, NA, NA), lwd = 1.5,\n     col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"),\n     ylab = \"River flow\")\nlegend(\"bottomleft\", legend = c(\"Observed\", \"Smoothed\", \"Forecast\", \n    \"90% probability limit\"), bty = 'n', pch = c(NA, NA, NA, NA, NA), lty = 1, lwd = 1.5,\n    col = c(\"steelblue\", \"red\", \"brown\", \"gray\", \"gray\"))"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#non-gaussian-state-space-models",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#non-gaussian-state-space-models",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Non-Gaussian state-space models",
    "text": "Non-Gaussian state-space models\n\nPoisson time series model\nA useful model for time series of counts \\(Y \\in \\{0,1,2,\\ldots \\}\\) is a Poisson distribution with time-varying intensity \\(\\lambda_t = \\exp(z_t)\\), where \\(z_t\\) is some continuous stochastic process with autocorrelation, most commonly a random walk:\n\\[\n\\begin{align} y_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= z_{t-1} + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nNote that because of the exponential function \\(\\lambda_t = \\exp(z_t)\\) is guaranteed to be positive for all \\(t\\), as required for the Poisson distribution. It is easily to simulate data from the Poisson time series model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimPoisTimeSeries &lt;- function(T, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the Poisson variables with different intensities, lambda_t = exp(z_t) for each time\n  lambda = exp(z)\n  return (rpois(T, lambda = lambda[2:(T+1)]))\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1) \nT = 100\nsigma_eta = 0.1\ny = simPoisTimeSeries(T, sigma_eta)\nplot(y, type = \"o\", pch = 19, col = \"steelblue\", yaxt = \"n\", xlab = \"time, t\", ylab = \"counts, y\", \n      main = paste(\"A simulated Poisson time series with sigma_eta =\", sigma_eta))\naxis(side = 2, at = seq(0,max(y)))\n\n\n\n\n\n\n\n\n\nWhat’s life without widgets? Here is one for a slightly more general Poisson time series model where the random walk is replaced by an autoregressive process of order 1:\n\\[\n\\begin{align}\ny_t \\vert z_t &\\overset{\\mathrm{indep}}{\\sim} \\mathrm{Pois}(\\exp(z_t)) \\\\  \nz_t &= \\mu + \\phi(z_{t-1} -\\mu) + \\eta_t, \\qquad \\eta_t \\sim N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\n\nThe Poisson time series model is an example of a non-linear (the observations are not linear functions of the state) and non-Gaussian (well, Poisson is not Gaussian) and can therefore not be analyzed with the Kalman filter. There are (approximate) extensions of the Kalman filter and also purely simulation based filtering methods called particle methods. But that is stuff for another day.\n\n\nStochastic volatility models\nMany time series, particularly in the finance, has a variance that is changing over time. Furthermore, it is common to find volatility clustering in the data, meaning that once the the variance is high (turbulent stock market) it tends to remain high for a while and vice versa. The basic stochastic volatility (SV) model tries to capture this:\n\\[\n\\begin{align}\ny_t &= \\mu + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= z_{t-1} + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere we have for simplicity assumed just a constant mean \\(\\mu\\), but we can extend this with and autoregressive process, or basically any model of your preference. The thing that set the SV model apart from the other model presented so far is that the variance of the measurement errors \\(Var(y_t)=Var(\\varepsilon_t) = \\exp(z_t)\\) is heteroscedastic, that is, it varies over time. The variance is driven by the \\(z_t\\) process, which here is modeled as a random walk, which will induce volatility clustering. Note again that we use the exponential function to ensure that the variance is positive for all \\(t\\). The model is Gaussian (only normal distributions), but non-Gaussian (the variance is an exponential of the state) so the Kalman filter can not be used, and we would to resort to other methods for the filtering and smoothing. Here is code to simulate from this basic stochastic volatility model:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVol &lt;- function(T, mu, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = z[t-1] + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigma²_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\nLet’s use that function to simulate a time series and plot it:\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(2) \nT = 100\nmu = 3\nsigma_eta = 1\nsimuldata = simStochVol(T, mu, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nWe can replace the random walk for the \\(z_t\\) with a more well-behaved AR(1) process:\n\\[\n\\begin{align}\ny_t &= \\mu_y + \\varepsilon_t, \\hspace{0.8cm} \\varepsilon_t \\overset{\\mathrm{indep}}{\\sim}N(0,\\exp(z_t)), \\\\\nz_t &= \\mu_z + \\phi(z_{t-1} - \\mu_z) + \\eta_t, \\quad \\eta_t \\overset{\\mathrm{iid}}{\\sim}N(0, \\sigma^2_\\eta)\n\\end{align}\n\\]\nwhere \\(\\mu_y\\) is the mean of the time series \\(y\\) and \\(\\mu_z\\) is the mean of the (log) variance process \\(z_t\\). The parameter \\(\\mu_z\\) therefore determines how much variance \\(y_t\\) has on average and \\(\\phi\\) determines how much volatility clustering there is. A \\(\\phi\\) close to 1 gives long periods of persistently large or small variance. Here is the code:\n\n# Set up the simulation function, starting the z_t process at zero.\nsimStochVolAR &lt;- function(T, mu_y, mu_z, phi, sigma_eta){\n  \n  # Simulate the z_t process\n  z = rep(0,T+1)\n  for (t in 2:(T+1)){\n    z[t] = mu_z + phi*(z[t-1] - mu_z) + rnorm(1, mean = 0, sd = sigma_eta)\n  }\n  \n  # Simulate the y_T with a different variance in for each sigma²_t = exp(z_t) for each t\n  sigma2eps = exp(z)\n  y = rnorm(T+1, mean = mu_y, sd = sqrt(sigma2eps))\n  return ( list(y = y[2:(T+1)], sigmaeps = sqrt(sigma2eps)[2:(T+1)])  )\n}\n\n\n\nShow the code\n# Simulate and plot the time series\nset.seed(1)\nT = 1000\nmu_y = 3\nmu_z = -1\nphi = 0.95\nsigma_eta = 1\nsimuldata = simStochVolAR(T, mu_y, mu_z, phi, sigma_eta)\nplot(simuldata$y, type = \"l\", col = \"steelblue\", xlab = \"time, t\", ylab = \"y\", \n     main = paste(\"A simulated stochastic volatility process with sigma_eta =\", sigma_eta),\n     ylim = c(min(simuldata$y,simuldata$sigmaeps), max(simuldata$y,simuldata$sigmaeps)), lwd = 2)\nlines(simuldata$sigmaeps, col = \"orange\", lwd = 2)\nlegend(\"bottomright\", legend = c(\"time series\", \"standard deviation\"), lty = 1, lwd = c(2,2),\n    col = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nWidget time!"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#bonus-implementing-the-kalman-filter-from-scratch",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#bonus-implementing-the-kalman-filter-from-scratch",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Bonus: Implementing the Kalman filter from scratch",
    "text": "Bonus: Implementing the Kalman filter from scratch\nFor the curious, the code below implements the Kalman filter from scratch in R. Let us first implement a function kalmanfilter_update that does the update for a single time step:\n\nkalmanfilter_update &lt;- function(mu, Omega, y, G, C, V, W) {\n  \n  # Prediction step - moving state forward without new measurement\n  muPred &lt;- G %*% mu\n  omegaPred &lt;- G %*% Omega %*% t(G) + W\n  \n  # Measurement update - updating the N(muPred, omegaPred) prior with the new data point\n  K &lt;- omegaPred %*% t(F) / (F %*% omegaPred %*% t(F) + V) # Kalman Gain\n  mu &lt;- muPred + K %*% (y - F %*% muPred)\n  Omega &lt;- (diag(length(mu)) - K %*% F) %*% omegaPred\n  \n  return(list(mu, Omega))\n}\n\nThen we implement a function that does all the Kalman iterations, using the kalmanfilter_update function above:\n\nkalmanfilter &lt;- function(Y, G, F, V, W, mu0, Sigma0) {\n  T &lt;- dim(Y)[1]  # Number of time steps\n  n &lt;- length(mu0)  # Dimension of the state vector\n  \n  # Storage for the mean and covariance state vector trajectory over time\n  mu_filter &lt;- matrix(0, nrow = T, ncol = n)\n  Sigma_filter &lt;- array(0, dim = c(n, n, T))\n  \n  # The Kalman iterations\n  mu &lt;- mu0\n  Sigma &lt;- Sigma0\n  for (t in 1:T) {\n    result &lt;- kalmanfilter_update(mu, Sigma, t(Y[t, ]), G, F, V, W)\n    mu &lt;- result[[1]]\n    Sigma &lt;- result[[2]]\n    mu_filter[t, ] &lt;- mu\n    Sigma_filter[,,t] &lt;- Sigma\n  }\n  \n  return(list(mu_filter, Sigma_filter))\n}\n\nLet’s try it out on the Nile river data:\n\n# Analyzing the Nile river data\nprettycolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\")\ny = as.vector(Nile)\nV = 100^2\nW = 100^2\nmu0 = 1000\nSigma0 = 1000^2\n\n# Set up state-space model for local level model\nT = length(y)\nG = 1\nF = 1\nY = matrix(0,T,1)\nY[,1] = y\nfilterRes = kalmanfilter(Y, G, F, V, W, mu0, Sigma0)\nmeanFilter = filterRes[[1]]\nstd_filter = sqrt(filterRes[[2]][,,, drop =TRUE])\n\nplot(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\npolygon(c(seq(1:T), rev(seq(1:T))), \n        c(meanFilter - 1.96*std_filter, rev(meanFilter + 1.96*std_filter)), \n        col = \"#F0F0F0\", border = NA)\nlines(seq(1:T), y, type = \"l\", col = prettycolors[1], lwd = 1.5, xlab = \"time, t\")\nlines(seq(1:T), meanFilter, type = \"l\", col = prettycolors[3], lwd = 1.5)\nlegend(\"topright\", legend = c(\"time series\", \"filter mean\", \"95% intervals\"), lty = 1, lwd = 1.5,\n    col = c(prettycolors[1], prettycolors[3], \"#F0F0F0\"))"
  },
  {
    "objectID": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#footnotes",
    "href": "notebooks/KalmanFilteringSmoothing/R_KalmanFilteringSmoothing.html#footnotes",
    "title": "State-space models - filtering, smoothing and forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClicking on the  below the widget will take you to the Observable notebook of the widget where you can also change the locations of the thresholds, \\(t_1,t_2,\\ldots,t_{K-1}\\), if you are really into that sort of thing.↩︎\nIts all about that Bayes\nThe Kalman filter is often presented from a frequentist point of view in statistics, where the Kalman filtered estimates are the optimal estimates in the mean square error sense.\n\nThe Kalman filter can also be derived as simple Bayesian updating, using Bayes’ theorem to update the information about the state as a new measurement comes in. The \\(\\boldsymbol{\\mu_{0|0}}\\) and \\(\\boldsymbol{\\Omega_{0|0}}\\) can be seen as the prior mean and prior covariance matrix summarizing your prior information about the state before collecting any measurements.\n\nThe Kalman filter is great. When something is great, Bayes usually lurks in the background! 😜↩︎"
  },
  {
    "objectID": "notebooks/ebayPoissonOneParam/eBayPoissonR.html",
    "href": "notebooks/ebayPoissonOneParam/eBayPoissonR.html",
    "title": "Modeling the number of Bids in eBay coin auctions",
    "section": "",
    "text": "a notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nWe want learn about the number of bidders in a eBay internet auction. The dataset below contains information on the number of bidders and covariates/features that can be used to predict the number of bidders. We will later use a Poisson regression to build a prediction model, but we will here only analyze the number of bids using an simple iid Poission model without covariates.\n\n\nImport modules and load the data\n\nset.seed(123) # Set the seed for reproducibility\noptions(repr.plot.width=15, repr.plot.height=6, lwd = 4)\n#install.packages(\"RColorBrewer\")\nlibrary(\"RColorBrewer\")\ncolors = brewer.pal(12, \"Paired\")\n\n# Load the data\neBayData = read.csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData$nBids\n\n\n\nData\nThe dataset contains data from 1000 auctions of collector coins. The dataset was collected and first analyzed in the article Bayesian Inference in Structural Second-Price Common Value Auctions. Let’s read in the full dataset and extract the variable of interest, the number of bids (nBids):\n\neBayData = read.csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData$nBids\n\n\n\nPrior-to-Posterior updating\nWe will model these data using a Poisson distribution: \\[y_1,...,y_n \\vert \\theta \\overset{iid}{\\sim} \\mathrm{Poisson}(\\theta)\\]\nwith a conjugate Gamma prior\n\\[\\theta  \\sim \\mathrm{Gamma}(\\alpha, \\beta)\\]\nso that the posterior is also Gamma: \\[\\theta \\vert y_1,\\ldots,y_n \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n y_i, \\beta + n)\\]\n\nPostPoisson &lt;- function(y, alphaPrior, betaPrior, thetaPriorGrid = NA, thetaPostGrid = NA){\n\n    # Compute Prior density and posterior\n    priorDens = dgamma(thetaPriorGrid, shape = alphaPrior, rate = betaPrior)\n    n = length(y)\n    alphaPost = alphaPrior + sum(y)\n    betaPost = betaPrior + n\n    postDens = dgamma(thetaPostGrid, shape = alphaPost, rate = betaPost)\n    \n    message(paste('Mean number of counts = ', mean(y)))\n    message(paste('Prior mean = ', alphaPrior/betaPrior))\n    message(paste('Posterior mean = ', round(alphaPost/betaPost,3)))\n    message(paste('Prior standard deviation = ', sqrt(alphaPrior/(betaPrior**2))))\n    message(paste('Posterior standard deviation = ', sqrt( (alphaPrior+sum(y))/((betaPrior+n)**2)) ))\n    message(paste('Equal tail 95% prior interval: (' ,qgamma(0.025, shape = alphaPrior, rate = betaPrior),',',qgamma(0.975, shape = alphaPrior, rate = betaPrior),')'))\n    message(paste('Equal tail 95% posterior interval: (' ,qgamma(0.025, shape = alphaPost, rate = betaPost),',',qgamma(0.975, shape = alphaPost, rate = betaPost),')'))\n\n    if ( any(is.na(thetaPriorGrid)) != TRUE){\n        par(mfrow = c(1,2))\n        plot(thetaPriorGrid, priorDens, type = \"l\", lwd = 3, col = colors[2], xlab = expression(theta), ylab = \"PDF\", main = 'Prior distribution')\n        plot(thetaPostGrid, postDens, type = \"l\", lwd = 3, col = colors[8], xlab = expression(theta), ylab = \"PDF\", main = 'Posterior distribution')\n    }\n}\n\nalphaPrior = 2\nbetaPrior = 1/2\nPostPoisson(y = nBids, alphaPrior = 2, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 12, length = 10000), thetaPostGrid = seq(3.25, 4, length = 10000))\n\n\n\nFit of the Poisson model\nLet’s plot the data along with the fitted Poisson model. We’ll keep things simple and plot the fit for the posterior mean of \\(\\theta\\).\n\nplotPoissonFit &lt;- function(y, alphaPrior, betaPrior){\n    \n    # Compute empirical distribution of the data\n    n = length(y)\n    yGrid = seq(0, max(y))\n    probs = rep(NA,max(y)+1)\n    for (i in yGrid){\n        probs[i+1] = sum(y == i)/n\n    }\n    \n    # Compute posterior mean and Poisson model fit\n    alphaPost = alphaPrior + sum(y)\n    betaPost = betaPrior + n\n    postMean = alphaPost/betaPost\n    \n    # Plot the data and model fit\n    poisFit = dpois(yGrid, lambda = postMean) \n    plot(yGrid, probs, type = \"o\", lwd = 6, xlab = \"y\", ylab = \"PMF\", col = colors[1], main = 'Fitted Poisson model', \n           ylim = c(0,max(probs, poisFit)))\n    lines(yGrid, poisFit, col = colors[2], lwd = 6, type = \"o\")\n    legend(x = \"topright\", inset=.05, legend = c(\"Data distribution\", \"Poisson fit\"), pch = c(19,19), cex = c(1,1),\n       lty = c(1, 1), pt.lwd = c(3,3), col = c(colors[1], colors[2]))\n}\n\n\nalphaPrior = 2\nbetaPrior = 1/2\nplotPoissonFit(y = nBids, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\nWow, that’s are terrible fit! This data does not look at all like a Poisson distribution. What can we do?\n\n\nAnalyzing only the auctions with low reservation prices\nWe will later model the number of bids using a Poisson regression where we take into account several explanatory variables. But, for now, let’s split the auctions in two subsets:\ni) auctions with low reservation price in relation to the item’s book value (MinBidShare&lt;=0)\nii) auctions with high reservation price in relation to the item’s book value (MinBidShare&gt;0)\nLet’s start with the 550 auction with low reservation prices. The prior for the auction with low reservation prices is set to \\(\\theta \\sim \\mathrm{Gamma}(4,1/2)\\) to reflect a belief that belief that such auctions are likely to attract more bids.\n\n# Auctions with low reservation prices:\nnBidsLow = nBids[eBayData$MinBidShare&lt;=0]\nPostPoisson(y = nBidsLow, alphaPrior = 4, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 25,length = 10000), thetaPostGrid = seq(4.8, 5.8, length = 10000))\n\nAs expected, the posterior for the mean number of bids is concentrated on a larger number of bids. People like to bid on items where the seller’s reservation price is low.\nIs the first for these auctions improved? Yes it is, although there is still room for improvement:\n\n# Plot the fit for low bids\nplotPoissonFit(y = nBidsLow, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\nAnalyzing the auctions with high reservation prices\nBelow are the results for the auction with high reservation bids. The prior is here set to \\(\\theta \\sim \\mathrm{Gamma}(1,1/2)\\) implying less on average.\n\n# Auctions with high reservation prices:\nnBidsHigh = nBids[eBayData$MinBidShare&gt;0]\nPostPoisson(y = nBidsHigh, alphaPrior = 1, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 12, length = 10000), thetaPostGrid = seq(1.3, 1.8,length = 10000))\n\nAnd the fit is not perfect for these bids, but better than before.\n\n# Plot the fit for high bids\nplotPoissonFit(y = nBidsHigh, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\nSo, separating the bids into dataset with low and high reservation prices makes the Poisson model a lot better for the data. Later in the book, we will use a Poisson regression with reservation price as one of the features, which an even more fine grained analysis."
  },
  {
    "objectID": "notebooks/Untitled.html",
    "href": "notebooks/Untitled.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "a = 4\n\n4\n\n\n\nb = 4\n\n4\n\n\n\nprintln(\"This is the value of a= $a\")\n\nThis is the value of a= 4"
  },
  {
    "objectID": "notebooks/FossilNonlinearReg/FossilPolyRegStan.html",
    "href": "notebooks/FossilNonlinearReg/FossilPolyRegStan.html",
    "title": "Bayesian nonlinear regression in RStan",
    "section": "",
    "text": "Model\nThis notebook illustrate how to do a Bayesian analysis using rstan for the polynomial regression model \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_p x^p + \\varepsilon, \\quad \\varepsilon \\overset{\\mathrm{iid}}{\\sim} N(0,\\sigma^2).\\]\nPrior\nAn L2-prior (ridge) is used to prevent overfitting \\[\\beta_j \\vert \\sigma^2 \\overset{\\mathrm{iid}}{\\sim} N\\Big(0,\\frac{\\sigma^2}{\\lambda^2}\\Big),  \\] where the regularization parameter \\(\\lambda\\) is learned from the data. The regularization parameter \\(\\lambda\\) is a precision (inverse variance) and we will here instead parameterize it as a variance: \\(\\psi^2 = 1/\\lambda\\) in the sampling and use the prior \\[ \\psi^2 \\sim \\mathrm{Inv-}\\chi^2(\\omega_0,\\psi_0^2).\\] The intercept is not regularized and is given a separate prior \\[\\beta_0 \\sim N(0,\\sigma_{0,\\mathrm{intercept}}^2)\\] The noise variance is assign the usual scaled inverse chi-squared prior \\[ \\sigma^2 \\sim \\mathrm{Inv-}\\chi^2(\\nu_0,\\sigma_0^2).\\]"
  },
  {
    "objectID": "notebooks/FossilNonlinearReg/FossilPolyRegStan.html#footnotes",
    "href": "notebooks/FossilNonlinearReg/FossilPolyRegStan.html#footnotes",
    "title": "Bayesian nonlinear regression in RStan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChaudhuri, P. and J. S. Marron (1999). Sizer for exploration of structures in curves. Journal of the American Statistical Association↩︎"
  },
  {
    "objectID": "notebooks/ebayCountRegression/ebayCountRegression.html",
    "href": "notebooks/ebayCountRegression/ebayCountRegression.html",
    "title": "Bayesian Count regression in RStan",
    "section": "",
    "text": "Model\nThis notebook illustrate how to do a Bayesian analysis using rstan for the Poisson regression model for count data response \\(y\\) conditional on \\(p\\) covariates \\(\\mathbf{x}=(x_1,\\dots,x_p)^\\top\\): \\[y \\vert \\mathbf{x} \\sim \\mathrm{Poisson}(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p)\\]\nPrior\nThere is no conjugate prior for the Poisson regression and we use \\[\\boldsymbol{\\beta} \\sim N(\\mathbf{0},\\tau^2 \\mathbf{I}_p),  \\] where the hyperparameter \\(\\tau\\) can be fixed by the user or learned from the data. In the latter case we use a half-Cauchy prior for \\({\\tau \\sim C^+(0,1)}\\)."
  },
  {
    "objectID": "notebooks/ebayCountRegression/ebayCountRegression.html#footnotes",
    "href": "notebooks/ebayCountRegression/ebayCountRegression.html#footnotes",
    "title": "Bayesian Count regression in RStan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWegmann, B. and Villani, M. (1999). Sizer for exploration of structures in curves. Journal of the American Statistical Association↩︎"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "This page contains a set of notebooks in Julia, R and Python for some of the data analyses presented in the book.\n\nChapter 1 - The Bayesics\n\n\nChapter 2 - One-parameter models\n\n\n\nBernoulli model for spam data\n\n\n\n\n\nNormal model for internet download speed data\n\n\n\n\n\nPoisson for number of eBay bidders\n\n\n\n\n\n\n\n\nChapter 3 - Multi-parameter models\n\n\n\nMultinomial model for survey data\n\n\n\n\n\n\n\n\n\n\n\nChapter 4 - Priors\n\n\nChapter 5 - Regression\n\n\nChapter 6 - Prediction and Decision making\n\n\nChapter 7 - Normal posterior approximation\n\n\nChapter 8 - Classification\n\n\n\nLogistic regression for spam data\n\n\n\n\n\nLogistic regression for Titanic data\n\n\n\n\n\n\n\n\nChapter 9 - Gibbs sampling\n\n\nChapter 10 - Markov Chain Monte Carlo simulation\n\n\nChapter 11 - Variational inference\n\n\nChapter 12 - Regularization\n\n\n\nPolynomial regression fossil data\n\n\n\n\n\n\n\n\nChapter 13 - Mixture models and Bayesian nonparametrics\n\n\nChapter 14 - Model comparison and variable selection\n\n\nChapter 15 - Gaussian processes\n\n\nChapter 16 - Interaction models\n\n\nChapter 17 - Dynamic models and sequential inference\n\n\n\nFiltering and smoothing of the Nile river data"
  },
  {
    "objectID": "halloferrors.html",
    "href": "halloferrors.html",
    "title": "Hall of Typos",
    "section": "",
    "text": "The following people 🙏 have kindly reported typos and errors (ordered by the number of typos found, weighted by severity. Affiliation/student university when error was found):\nBayesian Learning\n\nAlice Jonason, Stockholm University\nTea Unnebäck, Stockholm University\nSune Karlsson, Örebro University\nJoachim Tscherpel, Stockholm University\nFederico M. Stefanini, University of Milan\nDagmar Kallenberg, Stockholm University\nMona Sfaxi, Stockholm University\nMichael Sederlin, KTH Royal Institute of Technology\nJinzhe Yang, Stockholm University\nPer Gösta Andersson, Stockholm University\nPatrik Mirzai\nToni Dumitriu, Stockholm University\n\nBayesian Learning - the prequel\n\nJoachim Tscherpel, Stockholm University\n\nPlease email me any typos and errors at my obvious gmail address. Thanks!"
  },
  {
    "objectID": "exercises/ch7/weibull_post_censored_optim.html",
    "href": "exercises/ch7/weibull_post_censored_optim.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.3\nThis exercise continues the analysis of the lung cancer data in Exercise 2.2\nAssume that the survival time \\(X\\) of the lung cancer patients in Exercise 2.2 are independent Weibull distributed \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] The value of \\(k\\) determines how the failure rate changes with time:\n\n\\(k=1\\) gives a failure (death) rate that is constant over time and corresponds to the special case of a exponential distribution \\(\\mathrm{Expon}(\\theta=1/\\lambda)\\) used in Exercise 2.2. Note that (following Wikipedia) the exponential distribution is parameterized with a rate (inverse scale) parameter \\(\\theta\\), while the Weibull is parameterized with a scale parameter \\(\\lambda= 1/\\theta\\) 🤷\n\\(k&lt;1\\) gives a decreasing failure rate over time\n\\(k&gt;1\\) gives an increasing failure rate over time.\n\n\nPlot the posterior distribution of \\(\\lambda\\) conditional on \\(k=1\\), \\(k=3/2\\) and \\(k=2\\). For all \\(k\\), use the prior \\(\\lambda \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) with \\(\\alpha=3\\) and \\(\\beta=1/50\\) (which a similar prior for \\(\\theta=1/\\lambda\\) as in Exercise 2.2). Hint: the posterior distribution for \\(k\\neq 1\\) is intractable, so use numerical evaluation of the posterior over a grid of \\(\\lambda\\)-values.\nPlot the time variable as a histogram and overlay the fitted model for the three different \\(k\\)-values; use the posterior mode for \\(\\theta\\) in each model when plotting the fitted model density.\n\n\n\n\n\n\n\nSolution Exercise 2.3a\n\n\n\n\n\nSimilar to Exercise 2.2b, the likelihood can be computed with separate treatment of the uncensored and censored observations: \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\lambda, k) & = \\prod_{i=1}^n p(x_i \\vert \\lambda, k) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\lambda, k) \\prod_{c \\in \\mathcal{C}} \\Big(1 - F(x_c \\vert \\lambda, k)\\Big)\n\\end{align}\n\\] where \\(p(x \\vert \\lambda, k)\\) is the pdf of a Weibull variable \\[\np(x \\vert \\lambda, k) = \\frac{k}{\\lambda}\\Big( \\frac{x}{\\lambda} \\Big)^{k-1}e^{-(x/\\lambda)^k}\\quad\\text{ for }x&gt;0\n\\] which is implemented in R as dweibull. The cdf of the Weibull distribution is of rather simple form \\[\nF(x \\vert \\lambda, k) = 1 - e^{-(x/\\lambda)^k}\n\\] and is implemented in R as pweibull.\nThe code below plots the prior and posterior distribution for \\(\\lambda\\) for the three different \\(k\\)-values. We could have inserted the mathematical expressions for the pdf and cdf and simplified the final likelihood expression; we will instead use the dweibull and pweibull functions without simplifications since it gives a more general template that can be used for any distribution, not just the Weibull model. For numerical stability we usually compute the posterior distribution on the log scale \\[\n\\log p(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j)\n\\] for a grid of equally spaced \\(\\lambda\\)-values: \\(\\lambda^{(1)}\\ldots,\\lambda^{(J)}\\). The \\(\\propto\\) sign now means that there is a missing additive constant \\(\\log p(x_1,\\ldots,x_n)\\) which does not depend on the unknown parameter \\(\\lambda\\). When we have computed \\(\\log p(\\lambda \\vert x_1,\\ldots,x_n)\\) over a grid of \\(\\lambda\\) values we compute the posterior on the original scale by \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j) \\Big)\n\\] and then divide all numbers with the normalizing constant to make sure that the posterior integrates to one. This is done numerically by approximating the integral by a Riemann rectangle sum \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) =\n\\frac{\\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(j)}) + \\log p(\\lambda^{(j)}) \\Big)}\n{\\sum_{h=1}^J \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(h)}) + \\log p(\\lambda^{(h)}) \\Big) \\Delta}\n\\] where \\(\\Delta\\) is the spacing between the grid points of \\(\\lambda\\)-values: \\(\\lambda^{(1)}, \\ldots, \\lambda^{(J)}\\).\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet up prior hyperparameters\n\nalpha_prior &lt;- 3     # shape parameter\nbeta_prior &lt;- 1/300  # rate parameter\n\nSet up function that computes the likelihood for any \\(\\lambda\\) value:\n\n# Make a function that computes the likelihood\nweibull_loglike &lt;- function(lambda, x, censored, k){\n  loglik_uncensored = sum(dweibull(x[-censored], shape = k, scale = lambda, \n                                   log = TRUE))\n  loglik_censored = sum(pweibull(x[censored], shape = k, scale = lambda, \n                                 lower.tail = FALSE, log.p = TRUE))\n  return(loglik_uncensored + loglik_censored)\n}\n\nSet up a function that computes the posterior density over a grid of \\(\\lambda\\):\n\nweibull_posterior &lt;- function(lambdaGrid, x, censored, k, alpha_prior, beta_prior){\n  Delta = lambdaGrid[2] - lambdaGrid[1] # Grid step size\n  logPrior &lt;- dgamma(lambdaGrid, shape = alpha_prior, rate = beta_prior, log = TRUE)\n  logLike &lt;- sapply(lambdaGrid, weibull_loglike, x, censored, k)\n  logPost &lt;- logLike + logPrior\n  logPost &lt;- logPost - max(logPost) # subtract constant to avoid overflow\n  post &lt;- exp(logPost)/(sum(exp(logPost))*Delta) # original scale and normalize\n  logLike &lt;- logLike - max(logLike)\n  likeNorm &lt;- exp(logLike)/(sum(exp(logLike))*Delta) # normalized likelihood\n  return(list(post = post, prior = exp(logPrior), likeNorm = likeNorm))\n}\n\n\n# Plot the prior and posterior densities\n\nlambdaGrid &lt;- seq(1, 1000, length.out = 1000)\n# Compute to get the prior\npostRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k = 1, \n                             alpha_prior, beta_prior)\ndf &lt;- data.frame(\n  lambdaGrid = lambdaGrid, \n  prior = postRes$prior\n)\n\n# Compute for all selected k values\npostModes = c()\nfor (k in c(1, 3/2, 2)){\n  postRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k, alpha_prior, beta_prior)\n  df[str_glue(\"posterior k={k}\")] &lt;- postRes$post\n  postModes = c(postModes, lambdaGrid[which.max(postRes$post)])\n}\n\ndf_long &lt;- df %&gt;% pivot_longer(-lambdaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = lambdaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior k=1\", \"posterior k=1.5\", \"posterior k=2\"), \n    values = c(colors[2], colors[1], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.3\", x = expression(lambda), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 2.3b\n\n\n\n\n\nThe fit of the three Weibull models are plotted below. The best fit seems to be for \\(k=3/2\\), which is still not very good. In a later exercise you will be asked to freely estimate both \\(\\lambda\\) and \\(k\\).\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dweibull, args = list(shape = 1, scale = postModes[1]), lwd = 1, \n                aes(color = \"Weibull fit k = 1/2\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 3/2, scale = postModes[2]), lwd = 1, \n                aes(color = \"Weibull fit k = 1\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 2, scale = postModes[3]), lwd = 1, \n                aes(color = \"Weibull fit k = 2\"),\n  ) +\n  labs(title = \"Weibull model fits\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[6]) +\n  scale_color_manual(\"\", values = c(colors[1], colors[3], colors[4])) +\n  theme_minimal()"
  },
  {
    "objectID": "exercises/ch7/prob_weibull_lung_optim.html",
    "href": "exercises/ch7/prob_weibull_lung_optim.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 7.2\nThis exercise continues the analysis of the lung cancer data in Exercise 2.3\nFollowing Exercise 2.3, assume that the survival time \\(X\\) of the lung cancer patients are independent Weibull distributed \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] In Exercise 2.3 the value of \\(k\\) was fixed; here we will treat both \\(\\lambda\\) and \\(k\\) as unknown. Since both these parameters are positive we reparameterize first by taking logs: \\[\\tilde\\lambda := \\log\\lambda \\text{ and } \\tilde k := \\log k,\\] which usually improves the normal approximation. We assume prior independence between the two parameters and priors: \\[\\lambda \\sim \\mathrm{LogNormal}(5,1^2) \\text{ and } k \\sim \\mathrm{LogNormal}(0,2^2),\n\\] which by the definition of the log-normal distribution implies that \\[\\tilde\\lambda \\sim N(5,1^2) \\text{ and } \\tilde k \\sim N(0,2^2).\\]\n\nCompute a bivariate normal approximation of the joint posterior distribution in the log-parameterization \\(p(\\tilde\\lambda, \\tilde k \\vert x_1,\\ldots,x_n)\\) using numerical optimization.\nUse the results from (a) to obtain a Log-normal approximation for the two marginal posterior distributions \\(p(\\lambda \\vert x_1,\\ldots,x_n)\\) and \\(p(k \\vert x_1,\\ldots,x_n)\\) in the original parameterization. Plot the prior and posterior density for both parameters. hint: remember that the marginal distributions from a bivariat normal distribution are both normal. Also, recall the relationship between normal and log-normal distributions.\nIf \\(X \\sim \\mathrm{Weibull}(\\lambda, k)\\) then \\[\n\\mu := \\mathbb{E}(X)=\\lambda\\Gamma(1+1/k),\n\\] where \\(\\Gamma()\\) is the gamma function. Obtain an approximate posterior distribution for \\(\\mu\\) using the results from (a) and simulation.\n\n\n\n\n\n\n\nSolution Exercise 7.2a\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet data and set up prior hyperparameters\n\nx = lung$time\ncensored = (lung$status == 1)\nlog_lambda_mean &lt;- 5\nlog_lambda_sd &lt;- 1\nlog_k_mean &lt;- 0\nlog_k_sd &lt;- 2\n\nSet up function that computes the log posterior for any \\(\\boldsymbol{\\theta}=(\\tilde\\lambda,\\tilde k)\\) vector. The first argument of this function must be a vector containing all parameters.\n\n# Function for computing the log posterior for any given parameter vector \nlogpost_weibull &lt;- function(theta, x, censored){\n  \n  # Compute the parameters in the original scale\n  lambda = exp(theta[1])\n  k = exp(theta[2])\n  \n   # Compute the (log) joint prior density\n  logPrior = dnorm(theta[1], log_lambda_mean, log_lambda_sd, log = TRUE) +\n             dnorm(theta[2], log_k_mean, log_k_sd, log = TRUE)\n  \n  # Compute the log-likelihood\n  loglik_uncensored = sum(dweibull(x[-censored], shape = k, scale = lambda, \n                                   log = TRUE))\n  loglik_censored = sum(pweibull(x[censored], shape = k, scale = lambda, \n                                 lower.tail = FALSE, log.p = TRUE))\n  logLik = loglik_uncensored + loglik_censored\n  \n  # Return the log posterior\n  return(logLik + logPrior) \n}\n\nUse optim to find the posterior mode and the observed information matrix J\n\ninitVal &lt;- c(log_lambda_mean, log_k_mean) # Start optimizer at prior means\nOptimResults&lt;-optim(initVal, logpost_weibull, gr=NULL, x, censored,\n  method = c(\"BFGS\"), control=list(fnscale=-1), hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covar matrix\npostStd &lt;- sqrt(diag(postCov))         # Approximate stdev\n\nThe bivariate normal approximation for the transformed parameter vector \\(\\boldsymbol{\\theta}=(\\tilde\\lambda,\\tilde k)\\) has mean vector\n\npostMode\n\n[1] 6.0171524 0.3593135\n\n\nand covariance matrix\n\npostCov\n\n             [,1]         [,2]\n[1,] 0.0021649252 0.0002793462\n[2,] 0.0002793462 0.0026940478\n\n\nfrom which we can compute approximate posterior standard deviations for \\(\\tilde\\lambda\\) and \\(\\tilde k\\)\n\npostStd\n\n[1] 0.04652876 0.05190422\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 7.2b\n\n\n\n\n\nSince the marginal distributions from a bivariate normal distribution are both normal, we have that the following normal posterior approximations \\[\n\\begin{align}\n  \\tilde\\lambda \\vert x_1,\\ldots,x_n & \\sim N(6.017,0.047) \\\\\n  \\tilde k \\vert x_1,\\ldots,x_n  & \\sim N(0.359,0.052) \\\\\n\\end{align}\n\\] and therefore Log-Normal approximations on the original scale \\[\n\\begin{align}\n  \\lambda \\vert x_1,\\ldots,x_n  & \\sim \\mathrm{LogNormal}(6.017,0.047) \\\\\n  k \\vert x_1,\\ldots,x_n  & \\sim \\mathrm{LogNormal}(0.359,0.052) \\\\\n\\end{align}\n\\]\n\n# Plot the prior and posterior densities\nlambdaGrid &lt;- seq(1, 600, length = 1000)\nkGrid &lt;- seq(0.01, 3, length = 1000)\n\n# Plot the prior and posterior densities\nprior_dens_lambda &lt;- dlnorm(lambdaGrid, log_lambda_mean, log_lambda_sd)\npost_dens_lambda &lt;- dlnorm(lambdaGrid, postMode[1], postStd[1])\nprior_dens_k &lt;- dlnorm(kGrid, log_k_mean, log_k_sd)\npost_dens_k &lt;- dlnorm(kGrid, postMode[2], postStd[2])\n\ndf_lambda &lt;- data.frame(\n  paramGrid = lambdaGrid, \n  prior = prior_dens_lambda, \n  posterior = post_dens_lambda\n)\n\ndf_k &lt;- data.frame(\n  paramGrid = kGrid, \n  prior = prior_dens_k, \n  posterior = post_dens_k\n)\n\ndf_long_lambda &lt;- df_lambda %&gt;% pivot_longer(-paramGrid, names_to = \"density_type\", values_to = \"density\")\n\np_lambda = ggplot(df_long_lambda) +\n  aes(x = paramGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior\"), \n    values = c(colors[2], colors[3])) +\n  labs(title = \"\", x = expression(lambda), y = \"Density\", color = \"\") + \n  theme_minimal()\n\ndf_long_k &lt;- df_k %&gt;% pivot_longer(-paramGrid, names_to = \"density_type\", values_to = \"density\")\n\np_k = ggplot(df_long_k) +\n  aes(x = paramGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior\"), \n    values = c(colors[2], colors[3])) +\n  labs(title = \"\" , x = expression(k), y = \"Density\", color = \"\") + \n  theme_minimal()\n\ngridExtra::grid.arrange(p_lambda, p_k, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 7.2c\n\n\n\n\n\nWe can simulate from the multivariate normal posterior approximation of \\(\\tilde\\lambda,\\tilde k\\) from (a), transform the draws to the orginal parameterization \\(\\lambda,k\\) and the finally compute the mean \\(\\mu = \\lambda\\Gamma(1 + 1/k)\\) for each draw. Like this:\n\nlibrary(mvtnorm)\nnDraws = 5000\nthetaDraws = rmvnorm(nDraws, postMode, postCov)\nlambdaDraws = exp(thetaDraws[,1])\nkDraws = exp(thetaDraws[,2])\nmuDraws = lambdaDraws*gamma(1 + (1/kDraws))\nhist(muDraws, 50, col = colors[1], freq = FALSE, xlab = expression(mu), \n     ylab = \"density\", main = \"Posterior for the Weibull mean\")"
  },
  {
    "objectID": "exercises/ch7solutions.html",
    "href": "exercises/ch7solutions.html",
    "title": "Chapter 7 - Normal approximation: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution.\n\nExercise 7.2\nThis exercise continues the analysis of the lung cancer data in Exercise 2.3\nFollowing Exercise 2.3, assume that the survival time \\(X\\) of the lung cancer patients are independent Weibull distributed \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] In Exercise 2.3 the value of \\(k\\) was fixed; here we will treat both \\(\\lambda\\) and \\(k\\) as unknown. Since both these parameters are positive we reparameterize first by taking logs: \\[\\tilde\\lambda := \\log\\lambda \\text{ and } \\tilde k := \\log k,\\] which usually improves the normal approximation. We assume prior independence between the two parameters and priors: \\[\\lambda \\sim \\mathrm{LogNormal}(5,1^2) \\text{ and } k \\sim \\mathrm{LogNormal}(0,2^2),\n\\] which by the definition of the log-normal distribution implies that \\[\\tilde\\lambda \\sim N(5,1^2) \\text{ and } \\tilde k \\sim N(0,2^2).\\]\n\nCompute a bivariate normal approximation of the joint posterior distribution in the log-parameterization \\(p(\\tilde\\lambda, \\tilde k \\vert x_1,\\ldots,x_n)\\) using numerical optimization.\nUse the results from (a) to obtain a Log-normal approximation for the two marginal posterior distributions \\(p(\\lambda \\vert x_1,\\ldots,x_n)\\) and \\(p(k \\vert x_1,\\ldots,x_n)\\) in the original parameterization. Plot the prior and posterior density for both parameters. hint: remember that the marginal distributions from a bivariat normal distribution are both normal. Also, recall the relationship between normal and log-normal distributions.\nIf \\(X \\sim \\mathrm{Weibull}(\\lambda, k)\\) then \\[\n\\mu := \\mathbb{E}(X)=\\lambda\\Gamma(1+1/k),\n\\] where \\(\\Gamma()\\) is the gamma function. Obtain an approximate posterior distribution for \\(\\mu\\) using the results from (a) and simulation.\n\n\n\n\n\n\n\nSolution Exercise 7.2a\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet data and set up prior hyperparameters\n\nx = lung$time\ncensored = (lung$status == 1)\nlog_lambda_mean &lt;- 5\nlog_lambda_sd &lt;- 1\nlog_k_mean &lt;- 0\nlog_k_sd &lt;- 2\n\nSet up function that computes the log posterior for any \\(\\boldsymbol{\\theta}=(\\tilde\\lambda,\\tilde k)\\) vector. The first argument of this function must be a vector containing all parameters.\n\n# Function for computing the log posterior for any given parameter vector \nlogpost_weibull &lt;- function(theta, x, censored){\n  \n  # Compute the parameters in the original scale\n  lambda = exp(theta[1])\n  k = exp(theta[2])\n  \n   # Compute the (log) joint prior density\n  logPrior = dnorm(theta[1], log_lambda_mean, log_lambda_sd, log = TRUE) +\n             dnorm(theta[2], log_k_mean, log_k_sd, log = TRUE)\n  \n  # Compute the log-likelihood\n  loglik_uncensored = sum(dweibull(x[-censored], shape = k, scale = lambda, \n                                   log = TRUE))\n  loglik_censored = sum(pweibull(x[censored], shape = k, scale = lambda, \n                                 lower.tail = FALSE, log.p = TRUE))\n  logLik = loglik_uncensored + loglik_censored\n  \n  # Return the log posterior\n  return(logLik + logPrior) \n}\n\nUse optim to find the posterior mode and the observed information matrix J\n\ninitVal &lt;- c(log_lambda_mean, log_k_mean) # Start optimizer at prior means\nOptimResults&lt;-optim(initVal, logpost_weibull, gr=NULL, x, censored,\n  method = c(\"BFGS\"), control=list(fnscale=-1), hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covar matrix\npostStd &lt;- sqrt(diag(postCov))         # Approximate stdev\n\nThe bivariate normal approximation for the transformed parameter vector \\(\\boldsymbol{\\theta}=(\\tilde\\lambda,\\tilde k)\\) has mean vector\n\npostMode\n\n[1] 6.0171524 0.3593135\n\n\nand covariance matrix\n\npostCov\n\n             [,1]         [,2]\n[1,] 0.0021649252 0.0002793462\n[2,] 0.0002793462 0.0026940478\n\n\nfrom which we can compute approximate posterior standard deviations for \\(\\tilde\\lambda\\) and \\(\\tilde k\\)\n\npostStd\n\n[1] 0.04652876 0.05190422\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 7.2b\n\n\n\n\n\nSince the marginal distributions from a bivariate normal distribution are both normal, we have that the following normal posterior approximations \\[\n\\begin{align}\n  \\tilde\\lambda \\vert x_1,\\ldots,x_n & \\sim N(6.017,0.047) \\\\\n  \\tilde k \\vert x_1,\\ldots,x_n  & \\sim N(0.359,0.052) \\\\\n\\end{align}\n\\] and therefore Log-Normal approximations on the original scale \\[\n\\begin{align}\n  \\lambda \\vert x_1,\\ldots,x_n  & \\sim \\mathrm{LogNormal}(6.017,0.047) \\\\\n  k \\vert x_1,\\ldots,x_n  & \\sim \\mathrm{LogNormal}(0.359,0.052) \\\\\n\\end{align}\n\\]\n\n# Plot the prior and posterior densities\nlambdaGrid &lt;- seq(1, 600, length = 1000)\nkGrid &lt;- seq(0.01, 3, length = 1000)\n\n# Plot the prior and posterior densities\nprior_dens_lambda &lt;- dlnorm(lambdaGrid, log_lambda_mean, log_lambda_sd)\npost_dens_lambda &lt;- dlnorm(lambdaGrid, postMode[1], postStd[1])\nprior_dens_k &lt;- dlnorm(kGrid, log_k_mean, log_k_sd)\npost_dens_k &lt;- dlnorm(kGrid, postMode[2], postStd[2])\n\ndf_lambda &lt;- data.frame(\n  paramGrid = lambdaGrid, \n  prior = prior_dens_lambda, \n  posterior = post_dens_lambda\n)\n\ndf_k &lt;- data.frame(\n  paramGrid = kGrid, \n  prior = prior_dens_k, \n  posterior = post_dens_k\n)\n\ndf_long_lambda &lt;- df_lambda %&gt;% pivot_longer(-paramGrid, names_to = \"density_type\", values_to = \"density\")\n\np_lambda = ggplot(df_long_lambda) +\n  aes(x = paramGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior\"), \n    values = c(colors[2], colors[3])) +\n  labs(title = \"\", x = expression(lambda), y = \"Density\", color = \"\") + \n  theme_minimal()\n\ndf_long_k &lt;- df_k %&gt;% pivot_longer(-paramGrid, names_to = \"density_type\", values_to = \"density\")\n\np_k = ggplot(df_long_k) +\n  aes(x = paramGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior\"), \n    values = c(colors[2], colors[3])) +\n  labs(title = \"\" , x = expression(k), y = \"Density\", color = \"\") + \n  theme_minimal()\n\ngridExtra::grid.arrange(p_lambda, p_k, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 7.2c\n\n\n\n\n\nWe can simulate from the multivariate normal posterior approximation of \\(\\tilde\\lambda,\\tilde k\\) from (a), transform the draws to the orginal parameterization \\(\\lambda,k\\) and the finally compute the mean \\(\\mu = \\lambda\\Gamma(1 + 1/k)\\) for each draw. Like this:\n\nlibrary(mvtnorm)\nnDraws = 5000\nthetaDraws = rmvnorm(nDraws, postMode, postCov)\nlambdaDraws = exp(thetaDraws[,1])\nkDraws = exp(thetaDraws[,2])\nmuDraws = lambdaDraws*gamma(1 + (1/kDraws))\nhist(muDraws, 50, col = colors[1], freq = FALSE, xlab = expression(mu), \n     ylab = \"density\", main = \"Posterior for the Weibull mean\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.3\nThis exercise uses the lung cancer data presented in Exercise 2.3\nHere we model the survival times of the lung cancer patients in Exercise 2.3 as independent Weibull distributed with a scale parameter \\(\\lambda\\) that is a function of covariates, i.e. using a Weibull regression model. The response variable time is denoted by \\(y\\) and is modelled as a function of the three covariates age, sex and ph.ecog (ECOG performance score). The model for patient \\(i\\) is: \\[\ny_i \\vert \\mathbf{x}_i, \\boldsymbol{\\beta}, k \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Weibull}\\big(\\lambda_i = \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}),k\\big).\n\\] where \\(\\boldsymbol{\\beta}\\) is the vector with regression coefficients. Note that by the properties of the Weibull distribution, the conditional mean in this model is \\(\\mathbb{E}(y \\vert \\mathbf{x}_i) = \\lambda_i\\Gamma(1+1/k)\\), so the regression coefficients do not quite have the usual interpretation of the effect on the conditional mean. The three covariates are placed in a \\(n\\times p\\) matrix \\(\\mathbf{X}\\) with the first column being one for all observations to model the intercept. Use a multivariate normal prior for \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0},\\tau^2\\mathbf{I}_p)\\) with the non-informative choice \\(\\tau = 100\\). Reparameterize \\(\\tilde k := \\log k\\) and use the prior \\(\\tilde k \\sim N(0,2^2)\\). Remove the patients with missing values in the selected covariates.\n\nCompute a normal approximation of the joint posterior distribution \\(p(\\boldsymbol{\\beta}, \\tilde k \\vert \\mathbf{y}, \\mathbf{X})\\) using numerical optimization. Plot the marginal posteriors of each regression coefficient and the marginal posterior for \\(k\\) on the original scale.\nUse the result from (a) and Monte Carlo simulation to compute the predictive densities for the following two new patients:\n\n80-year female with an ECOG performance score of 0 (0=asymptomatic), i.e. \\(\\mathbf{x} = (1, 80, 1, 0)^\\top\\)\n80-year female with an ECOG performance score of 4 (4=bedbound) \\(\\mathbf{x} = (1, 80, 1, 4)^\\top\\).\n\nPlot the two predictive densities and compare. Compute the predictive probability of living for at least another 1000 days for both patients.\n\n\n\n\n\n\n\nSolution Exercise 7.3a\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(mvtnorm)\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet data and set up prior hyperparameters\n\nlibrary(survival) # loads the lung cancer data as `lung`\nlung &lt;- lung %&gt;% select(c(\"time\", \"status\", \"age\", \"sex\", \"ph.ecog\")) %&gt;% drop_na()\ny = lung$time\nX = cbind(1, lung$age, lung$sex == 2, lung$ph.ecog) # sex = 1 is female\np = dim(X)[2]\ncensored = (lung$status == 1)\nmu &lt;- rep(0,p)  # beta ~ N(mu, tau^2*I)\ntau &lt;- 100    \nlog_k_mean &lt;- 0\nlog_k_sd &lt;- 2\n\nSet up a function that computes the log posterior for any \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\beta}^\\top,\\tilde k)^\\top\\) vector. The first argument of this function must be a vector containing all parameters.\n\n# Function for computing the log posterior for any given parameter vector \nlogpost_weibullreg &lt;- function(theta, y, X, censored){\n  \n  p = dim(X)[2]\n  \n  # Compute the parameters in the original scale\n  beta_ = theta[1:p]\n  k = exp(theta[p+1])\n  \n   # Compute the (log) joint prior density\n  logPrior = dmvnorm(beta_, mu, tau^2*diag(p), log = TRUE) +\n             dnorm(theta[p+1], log_k_mean, log_k_sd, log = TRUE)\n  \n  # Compute the log-likelihood\n  lambda_uncensored = exp(X[-censored,]%*%beta_)\n  loglik_uncensored = sum(dweibull(y[-censored], shape = k, \n                                   scale = lambda_uncensored, log = TRUE))\n  lambda_censored = exp(X[censored,]%*%beta_)\n  loglik_censored = sum(pweibull(y[censored], shape = k, \n                                  scale = lambda_censored, \n                                  lower.tail = FALSE, log.p = TRUE))\n  logLik = loglik_uncensored + loglik_censored\n  \n  # Return the log posterior\n  return(logLik + logPrior) \n}\n\nUse optim to find the posterior mode and the observed information matrix J:\n\ninitVal &lt;- c(5, 0, 0, 0, log_k_mean) # Start optimizer at prior means\nOptimResults&lt;-optim(initVal, logpost_weibullreg, gr=NULL, y, X, censored,\n  method = c(\"BFGS\"), control=list(fnscale=-1), hessian=TRUE)\n\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\n\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covar matrix\n\nThe multivariate normal approximation for \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\beta},\\tilde k)\\) has mean vector\n\npostMode\n\n[1]  6.29535724 -0.00240418  0.23063114 -0.25847948  0.37972847\n\n\nand covariance matrix\n\npostCov\n\n              [,1]          [,2]          [,3]          [,4]          [,5]\n[1,]  0.1019922630 -1.589499e-03 -3.809323e-03  8.236358e-04 -8.249516e-04\n[2,] -0.0015894994  2.666312e-05  9.209196e-06 -8.162842e-05  1.683131e-05\n[3,] -0.0038093228  9.209196e-06  8.756587e-03 -3.349311e-04 -5.095982e-04\n[4,]  0.0008236358 -8.162842e-05 -3.349311e-04  4.673567e-03  2.946088e-04\n[5,] -0.0008249516  1.683131e-05 -5.095982e-04  2.946088e-04  2.656437e-03\n\n\nfrom which we can compute approximate posterior standard deviations for each parameter\n\npostStd &lt;- sqrt(diag(postCov))         # Approximate stdev\npostStd\n\n[1] 0.319362275 0.005163634 0.093576639 0.068363492 0.051540635\n\n\nSince the marginal distributions from a multivariate normal distribution are all normal, the marginal posterior for \\(\\tilde k\\) is normal and the posterior for \\(k\\) is therefore Log-Normal \\[\n\\begin{equation}\n  k \\vert \\mathbf{y},\\mathbf{X}  \\sim \\mathrm{LogNormal}(0.38,0.052) \\\\\n\\end{equation}\n\\] This prior and marginal posterior for \\(k\\) is plotted below\n\nkGrid &lt;- seq(0.01, 2, length = 1000)\nprior_dens_k &lt;- dlnorm(kGrid, log_k_mean, log_k_sd)\npost_dens_k &lt;- dlnorm(kGrid, postMode[p+1], postStd[p+1])\nplot(kGrid, prior_dens_k, col = colors[1], type = \"l\", ylim = c(0,5.5), lwd = 2)\nlines(kGrid, post_dens_k, col = colors[3], lwd  = 2)\nlegend(x = \"topleft\", inset=.05, legend = c(\"prior\", \"posterior\"), \n       lwd = 2, lty = c(1,1), col = c(colors[1], colors[3]), box.lty=1)\n\n\n\n\n\n\n\n\nThe marginal posteriors for the four beta coefficients are plotted below.\n\nvarNames = c(\"intercept\", \"age\", \"sex\", \"ph.ecog\")\npar(mfrow = c(2,2))\nfor (j in 1:p){\n  betaGrid = seq(postMode[j] - 4*postStd[j], postMode[j] + 4*postStd[j], \n                 length = 1000)\n  plot(betaGrid, dnorm(betaGrid, postMode[j], postStd[j]), col = colors[1], \n       xlab = expression(beta), ylab = \"density\", main = varNames[j], \n       type = \"l\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 7.3b\n\n\n\n\n\nThe predictive distribution for the lifetime of a new person \\(\\tilde y\\) with covariate vector \\(\\tilde{\\mathbf{x}}\\) given the training data \\((\\mathbf{y}, \\mathbf{X})\\) is: \\[\np(\\tilde y \\vert \\tilde{\\mathbf{x}}, \\mathbf{y}, \\mathbf{X}) = \\int p(\\tilde y \\vert \\tilde{\\mathbf{x}}, \\boldsymbol{\\beta}, k)\n                    p(\\boldsymbol{\\beta}, k \\vert \\mathbf{y}, \\mathbf{X})\\mathrm{d}\\boldsymbol{\\beta} \\mathrm{d} k,\n\\] where \\(p(\\boldsymbol{\\beta}, k \\vert \\mathbf{y}, \\mathbf{X})\\) is the posterior distribution and \\[p(\\tilde y \\vert \\tilde{\\mathbf{x}}, \\boldsymbol{\\beta}, k) = \\mathrm{Weibull}\\big(\\tilde y \\vert k, \\tilde{\\lambda}\\big)\\] is the Weibull density with \\(\\tilde{\\lambda} = \\exp(\\tilde{\\mathbf{x}}^\\top \\boldsymbol{\\beta})\\). A Monte Carlo evaluation of this integral is obtained by:\n\nsimulating \\(m\\) parameter draws \\(\\boldsymbol{\\beta}^{(i)}, \\tilde{k}^{(i)}\\) for \\(i=1,\\ldots,m\\) from the multivariate normal approximation in (a)\ncompute \\(k^{(i)} = \\exp\\big(\\tilde{k}^{(i)}\\big)\\) for each draw\nsimulate a predictive draw \\(\\tilde y^{(i)}\\) from the Weibull model given those parameters \\[\n\\tilde y^{(i)} \\vert \\tilde{\\mathbf{x}}, \\boldsymbol{\\beta}^{(i)}, k^{(i)} \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Weibull}\\big(\\lambda_i = \\exp(\\tilde{\\mathbf{x}}^\\top \\boldsymbol{\\beta}^{(i)}), k^{(i)} \\big).\n\\] The code below does exactly this for the two patients. Note that this can be done much efficiently by making all draws in one shot, but this code is connects better to the algorithm above.\n\n\nnDraws = 5000\nx1_tilde = c(1,80,1,0) # first patient\nx2_tilde = c(1,80,1,5) # second patient\ny1_tilde = rep(0,nDraws)\ny2_tilde = rep(0,nDraws)\nfor (i in 1:nDraws){\n  \n  # Simulate from multivariate normal posterior approximation\n  theta &lt;- rmvnorm(1, postMode, postCov)\n  beta_ = theta[1:p]\n  k = exp(theta[p+1])\n  \n  # Simulate predictive draws from the model\n  y1_tilde[i] = rweibull(1, shape = k, scale = exp(x1_tilde %*% beta_))\n  y2_tilde[i] = rweibull(1, shape = k, scale = exp(x2_tilde %*% beta_))\n}\n\n# Plot the predictive densities as kernel density estimates\nkde1 = density(y1_tilde)\nkde2 = density(y2_tilde)\nplot(kde1$x, kde1$y, col = colors[1], type = \"l\", , lwd = 2, ylim = c(0, 0.005),\n     xlab = \"days\", ylab = \"predictive density\",\n     main = \"predictive distributions for two patients\")\nlines(kde2$x, kde2$y, col = colors[3], lwd = 2)\nlegend(x = \"topright\", inset=.05, legend = c(\"asymptomatic\", \"bedbound\"), \n         lty = c(1,1), col = c(colors[1], colors[3]), box.lty=1)\n\n\n\n\n\n\n\n\nThe posterior predictive probability of living for at least another 1000 days is\n\nmessage(\"The probability that a 80 year female asymptomatic patient lives for at least 1000 days is \", mean(y1_tilde &gt;= 1000))  \n\nThe probability that a 80 year female asymptomatic patient lives for at least 1000 days is 0.1054\n\nmessage(\"The probability that a 80 year female bedbound patient lives for at least 1000 days is \", mean(y2_tilde &gt;= 1000))  \n\nThe probability that a 80 year female bedbound patient lives for at least 1000 days is 0"
  },
  {
    "objectID": "exercises/ch7/prob_weibullreg_lung_optim.html",
    "href": "exercises/ch7/prob_weibullreg_lung_optim.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 7.3\nThis exercise uses the lung cancer data presented in Exercise 2.3\nHere we model the survival times of the lung cancer patients in Exercise 2.3 as independent Weibull distributed with a scale parameter \\(\\lambda\\) that is a function of covariates, i.e. using a Weibull regression model. The response variable time is denoted by \\(y\\) and is modelled as a function of the three covariates age, sex and ph.ecog (ECOG performance score). The model for patient \\(i\\) is: \\[\ny_i \\vert \\mathbf{x}_i, \\boldsymbol{\\beta}, k \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Weibull}\\big(\\lambda_i = \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}),k\\big).\n\\] where \\(\\boldsymbol{\\beta}\\) is the vector with regression coefficients. Note that by the properties of the Weibull distribution, the conditional mean in this model is \\(\\mathbb{E}(y \\vert \\mathbf{x}_i) = \\lambda_i\\Gamma(1+1/k)\\), so the regression coefficients do not quite have the usual interpretation of the effect on the conditional mean. The three covariates are placed in a \\(n\\times p\\) matrix \\(\\mathbf{X}\\) with the first column being one for all observations to model the intercept. Use a multivariate normal prior for \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0},\\tau^2\\mathbf{I}_p)\\) with the non-informative choice \\(\\tau = 100\\). Reparameterize \\(\\tilde k := \\log k\\) and use the prior \\(\\tilde k \\sim N(0,2^2)\\). Remove the patients with missing values in the selected covariates.\n\nCompute a normal approximation of the joint posterior distribution \\(p(\\boldsymbol{\\beta}, \\tilde k \\vert \\mathbf{y}, \\mathbf{X})\\) using numerical optimization. Plot the marginal posteriors of each regression coefficient and the marginal posterior for \\(k\\) on the original scale.\nUse the result from (a) and Monte Carlo simulation to compute the predictive densities for the following two new patients:\n\n80-year female with an ECOG performance score of 0 (0=asymptomatic), i.e. \\(\\mathbf{x} = (1, 80, 1, 0)^\\top\\)\n80-year female with an ECOG performance score of 4 (4=bedbound) \\(\\mathbf{x} = (1, 80, 1, 4)^\\top\\).\n\nPlot the two predictive densities and compare. Compute the predictive probability of living for at least another 1000 days for both patients.\n\n\n\n\n\n\n\nSolution Exercise 7.3a\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(mvtnorm)\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet data and set up prior hyperparameters\n\nlibrary(survival) # loads the lung cancer data as `lung`\nlung &lt;- lung %&gt;% select(c(\"time\", \"status\", \"age\", \"sex\", \"ph.ecog\")) %&gt;% drop_na()\ny = lung$time\nX = cbind(1, lung$age, lung$sex == 2, lung$ph.ecog) # sex = 1 is female\np = dim(X)[2]\ncensored = (lung$status == 1)\nmu &lt;- rep(0,p)  # beta ~ N(mu, tau^2*I)\ntau &lt;- 100    \nlog_k_mean &lt;- 0\nlog_k_sd &lt;- 2\n\nSet up a function that computes the log posterior for any \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\beta}^\\top,\\tilde k)^\\top\\) vector. The first argument of this function must be a vector containing all parameters.\n\n# Function for computing the log posterior for any given parameter vector \nlogpost_weibullreg &lt;- function(theta, y, X, censored){\n  \n  p = dim(X)[2]\n  \n  # Compute the parameters in the original scale\n  beta_ = theta[1:p]\n  k = exp(theta[p+1])\n  \n   # Compute the (log) joint prior density\n  logPrior = dmvnorm(beta_, mu, tau^2*diag(p), log = TRUE) +\n             dnorm(theta[p+1], log_k_mean, log_k_sd, log = TRUE)\n  \n  # Compute the log-likelihood\n  lambda_uncensored = exp(X[-censored,]%*%beta_)\n  loglik_uncensored = sum(dweibull(y[-censored], shape = k, \n                                   scale = lambda_uncensored, log = TRUE))\n  lambda_censored = exp(X[censored,]%*%beta_)\n  loglik_censored = sum(pweibull(y[censored], shape = k, \n                                  scale = lambda_censored, \n                                  lower.tail = FALSE, log.p = TRUE))\n  logLik = loglik_uncensored + loglik_censored\n  \n  # Return the log posterior\n  return(logLik + logPrior) \n}\n\nUse optim to find the posterior mode and the observed information matrix J:\n\ninitVal &lt;- c(5, 0, 0, 0, log_k_mean) # Start optimizer at prior means\nOptimResults&lt;-optim(initVal, logpost_weibullreg, gr=NULL, y, X, censored,\n  method = c(\"BFGS\"), control=list(fnscale=-1), hessian=TRUE)\n\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\nWarning in dweibull(y[-censored], shape = k, scale = lambda_uncensored, : NaNs\nproduced\n\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covar matrix\n\nThe multivariate normal approximation for \\(\\boldsymbol{\\theta}=(\\boldsymbol{\\beta},\\tilde k)\\) has mean vector\n\npostMode\n\n[1]  6.29535724 -0.00240418  0.23063114 -0.25847948  0.37972847\n\n\nand covariance matrix\n\npostCov\n\n              [,1]          [,2]          [,3]          [,4]          [,5]\n[1,]  0.1019922630 -1.589499e-03 -3.809323e-03  8.236358e-04 -8.249516e-04\n[2,] -0.0015894994  2.666312e-05  9.209196e-06 -8.162842e-05  1.683131e-05\n[3,] -0.0038093228  9.209196e-06  8.756587e-03 -3.349311e-04 -5.095982e-04\n[4,]  0.0008236358 -8.162842e-05 -3.349311e-04  4.673567e-03  2.946088e-04\n[5,] -0.0008249516  1.683131e-05 -5.095982e-04  2.946088e-04  2.656437e-03\n\n\nfrom which we can compute approximate posterior standard deviations for each parameter\n\npostStd &lt;- sqrt(diag(postCov))         # Approximate stdev\npostStd\n\n[1] 0.319362275 0.005163634 0.093576639 0.068363492 0.051540635\n\n\nSince the marginal distributions from a multivariate normal distribution are all normal, the marginal posterior for \\(\\tilde k\\) is normal and the posterior for \\(k\\) is therefore Log-Normal \\[\n\\begin{equation}\n  k \\vert \\mathbf{y},\\mathbf{X}  \\sim \\mathrm{LogNormal}(0.38,0.052) \\\\\n\\end{equation}\n\\] This prior and marginal posterior for \\(k\\) is plotted below\n\nkGrid &lt;- seq(0.01, 2, length = 1000)\nprior_dens_k &lt;- dlnorm(kGrid, log_k_mean, log_k_sd)\npost_dens_k &lt;- dlnorm(kGrid, postMode[p+1], postStd[p+1])\nplot(kGrid, prior_dens_k, col = colors[1], type = \"l\", ylim = c(0,5.5), lwd = 2)\nlines(kGrid, post_dens_k, col = colors[3], lwd  = 2)\nlegend(x = \"topleft\", inset=.05, legend = c(\"prior\", \"posterior\"), \n       lwd = 2, lty = c(1,1), col = c(colors[1], colors[3]), box.lty=1)\n\n\n\n\n\n\n\n\nThe marginal posteriors for the four beta coefficients are plotted below.\n\nvarNames = c(\"intercept\", \"age\", \"sex\", \"ph.ecog\")\npar(mfrow = c(2,2))\nfor (j in 1:p){\n  betaGrid = seq(postMode[j] - 4*postStd[j], postMode[j] + 4*postStd[j], \n                 length = 1000)\n  plot(betaGrid, dnorm(betaGrid, postMode[j], postStd[j]), col = colors[1], \n       xlab = expression(beta), ylab = \"density\", main = varNames[j], \n       type = \"l\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise 7.3b\n\n\n\n\n\nThe predictive distribution for the lifetime of a new person \\(\\tilde y\\) with covariate vector \\(\\tilde{\\mathbf{x}}\\) given the training data \\((\\mathbf{y}, \\mathbf{X})\\) is: \\[\np(\\tilde y \\vert \\tilde{\\mathbf{x}}, \\mathbf{y}, \\mathbf{X}) = \\int p(\\tilde y \\vert \\tilde{\\mathbf{x}}, \\boldsymbol{\\beta}, k)\n                    p(\\boldsymbol{\\beta}, k \\vert \\mathbf{y}, \\mathbf{X})\\mathrm{d}\\boldsymbol{\\beta} \\mathrm{d} k,\n\\] where \\(p(\\boldsymbol{\\beta}, k \\vert \\mathbf{y}, \\mathbf{X})\\) is the posterior distribution and \\[p(\\tilde y \\vert \\tilde{\\mathbf{x}}, \\boldsymbol{\\beta}, k) = \\mathrm{Weibull}\\big(\\tilde y \\vert k, \\tilde{\\lambda}\\big)\\] is the Weibull density with \\(\\tilde{\\lambda} = \\exp(\\tilde{\\mathbf{x}}^\\top \\boldsymbol{\\beta})\\). A Monte Carlo evaluation of this integral is obtained by:\n\nsimulating \\(m\\) parameter draws \\(\\boldsymbol{\\beta}^{(i)}, \\tilde{k}^{(i)}\\) for \\(i=1,\\ldots,m\\) from the multivariate normal approximation in (a)\ncompute \\(k^{(i)} = \\exp\\big(\\tilde{k}^{(i)}\\big)\\) for each draw\nsimulate a predictive draw \\(\\tilde y^{(i)}\\) from the Weibull model given those parameters \\[\n\\tilde y^{(i)} \\vert \\tilde{\\mathbf{x}}, \\boldsymbol{\\beta}^{(i)}, k^{(i)} \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Weibull}\\big(\\lambda_i = \\exp(\\tilde{\\mathbf{x}}^\\top \\boldsymbol{\\beta}^{(i)}), k^{(i)} \\big).\n\\] The code below does exactly this for the two patients. Note that this can be done much efficiently by making all draws in one shot, but this code is connects better to the algorithm above.\n\n\nnDraws = 5000\nx1_tilde = c(1,80,1,0) # first patient\nx2_tilde = c(1,80,1,5) # second patient\ny1_tilde = rep(0,nDraws)\ny2_tilde = rep(0,nDraws)\nfor (i in 1:nDraws){\n  \n  # Simulate from multivariate normal posterior approximation\n  theta &lt;- rmvnorm(1, postMode, postCov)\n  beta_ = theta[1:p]\n  k = exp(theta[p+1])\n  \n  # Simulate predictive draws from the model\n  y1_tilde[i] = rweibull(1, shape = k, scale = exp(x1_tilde %*% beta_))\n  y2_tilde[i] = rweibull(1, shape = k, scale = exp(x2_tilde %*% beta_))\n}\n\n# Plot the predictive densities as kernel density estimates\nkde1 = density(y1_tilde)\nkde2 = density(y2_tilde)\nplot(kde1$x, kde1$y, col = colors[1], type = \"l\", , lwd = 2, ylim = c(0, 0.005),\n     xlab = \"days\", ylab = \"predictive density\",\n     main = \"predictive distributions for two patients\")\nlines(kde2$x, kde2$y, col = colors[3], lwd = 2)\nlegend(x = \"topright\", inset=.05, legend = c(\"asymptomatic\", \"bedbound\"), \n         lty = c(1,1), col = c(colors[1], colors[3]), box.lty=1)\n\n\n\n\n\n\n\n\nThe posterior predictive probability of living for at least another 1000 days is\n\nmessage(\"The probability that a 80 year female asymptomatic patient lives for at least 1000 days is \", mean(y1_tilde &gt;= 1000))  \n\nThe probability that a 80 year female asymptomatic patient lives for at least 1000 days is 0.1014\n\nmessage(\"The probability that a 80 year female bedbound patient lives for at least 1000 days is \", mean(y2_tilde &gt;= 1000))  \n\nThe probability that a 80 year female bedbound patient lives for at least 1000 days is 0"
  },
  {
    "objectID": "exercises/ch10/prob_weibullreg_lung_stan.html",
    "href": "exercises/ch10/prob_weibullreg_lung_stan.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 10.1\nThis exercise uses the lung cancer data first presented in Exercise 2.3 and performs a HMC sampling with stan from the same posterior distribution for the Weibull regression that was approximated by a normal distribution in Exercise 7.3\nHere we model the survival times of the lung cancer patients in Exercise 2.3 as independent Weibull distributed with a scale parameter \\(\\lambda\\) that is a function of covariates, i.e. using a Weibull regression model. The response variable time is denoted by \\(y\\) and is modelled as a function of the three covariates age, sex and ph.ecog (ECOG performance score). The model for patient \\(i\\) is: \\[\ny_i \\vert \\mathbf{x}_i, \\boldsymbol{\\beta}, k \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Weibull}\\big(\\lambda_i = \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}),k\\big).\n\\] where \\(\\boldsymbol{\\beta}\\) is the vector with regression coefficients. Note that by the properties of the Weibull distribution, the conditional mean in this model is \\(\\mathbb{E}(y \\vert \\mathbf{x}_i) = \\lambda_i\\Gamma(1+1/k)\\), so the regression coefficients do not quite have the usual interpretation of the effect on the conditional mean. The three covariates are placed in a \\(n\\times p\\) matrix \\(\\mathbf{X}\\) with the first column being one for all observations to model the intercept. Use a multivariate normal prior for \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0},\\tau^2\\mathbf{I}_p)\\) with the non-informative choice \\(\\tau = 100\\), and the prior \\(\\ k \\sim \\mathrm{logNormal}(0,2^2)\\). Remove the patients with missing values in the selected covariates.\nSample from the posterior distribution \\(p(\\boldsymbol{\\beta}, k \\vert \\mathbf{y}, \\mathbf{X})\\) using HMC in stan. Plot the marginal posterior for \\(k\\) and the marginal posteriors of each regression coefficient.\n\n\n\n\n\n\nSolution Exercise 10.1a\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(rstan)\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet data and set up prior hyperparameters\n\nlibrary(survival) # loads the lung cancer data as `lung`\nlung &lt;- lung %&gt;% select(c(\"time\", \"status\", \"age\", \"sex\", \"ph.ecog\")) %&gt;% drop_na()\ny = lung$time\nX = cbind(1, lung$age, lung$sex == 2, lung$ph.ecog) # sex = 1 is female\np = dim(X)[2]\ncensored = (lung$status == 1)\ny_obs = y[-censored]\ny_cens = y[censored]\nX_obs = X[-censored,]\nX_cens = X[censored,]\nmu &lt;- rep(0,p)  # beta ~ N(mu, tau^2*I)\ntau &lt;- 100    \nmu_k &lt;- 0       # k ~ LogNormal(mu_k, sigma_k^2)\nsigma_k &lt;- 2\n\nSet up the stan model for the Weibull regression.\n\nweibull_survivalreg &lt;- '\ndata {\n\n  // Data\n  int&lt;lower=0&gt; N_obs;\n  int&lt;lower=0&gt; N_cens;\n  int&lt;lower=1&gt; p;\n  array[N_obs] real y_obs;\n  array[N_cens] real y_cens;\n  matrix[N_obs,p] X_obs;\n  matrix[N_cens,p] X_cens;\n  \n  // Prior hyperparameters k ~ LogNormal(mu_k, sigma_k) and beta_ ~ N(0, tau^2*I)\n  real mu_k;\n  real&lt;lower=0&gt; sigma_k;\n  real&lt;lower = 0&gt; tau;\n}\nparameters {\n  vector[p] beta_;\n  real&lt;lower=0&gt; k;\n}\nmodel {\n  k ~ lognormal(mu_k, sigma_k);    // specifies the prior\n  beta_ ~ normal(0, tau);\n  y_obs ~ weibull(k, exp(X_obs * beta_));  // add the observed (non-censored) data\n  target += weibull_lccdf(y_cens | k, exp(X_cens * beta_)); // add censored. \n}\n'\n\nWe set up the data and prior lists that will be supplied to stan:\n\ndata &lt;- list(p = dim(X_obs)[2], N_obs = length(y_obs), N_cens = length(y_cens), \n             y_obs = y_obs, y_cens = y_cens, X_obs = X_obs, X_cens = X_cens)\n\nprior &lt;- list(tau = tau, mu_k = mu_k, sigma_k = sigma_k)\n\nLoad rstan and set some options\n\n#install.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', \n#                                    getOption(\"repos\")))\nsuppressMessages(library(rstan))\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nSample from the posterior distribution using HMC in stan\n\nnDraws = 5000\nfit = stan(model_code = weibull_survivalreg, data = c(data, prior), iter = nDraws)\n\nTrying to compile a simple C file\n\n\nRunning /usr/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‘gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0’\ngcc -I\"/usr/share/R/include\" -DNDEBUG   -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/Rcpp/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/unsupported\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/BH/include\" -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/src/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppParallel/include/\"  -I\"/home/mv/R/x86_64-pc-linux-gnu-library/4.4/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1       -fpic  -g -O2 -ffile-prefix-map=/build/r-base-6tgf7J/r-base-4.4.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2  -c foo.c -o foo.o\nIn file included from /home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/Eigen/Core:19,\n                 from /home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/Eigen/Dense:1,\n                 from /home/mv/R/x86_64-pc-linux-gnu-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/home/mv/R/x86_64-pc-linux-gnu-library/4.4/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [/usr/lib/R/etc/Makeconf:195: foo.o] Error 1\n\n\n\ns &lt;- summary(fit, pars = c(\"beta_\", \"k\"), probs = c(0.025, 0.975))\ns$summary  # results from all the different runs (chains) merged.\n\n                 mean      se_mean          sd        2.5%        97.5%\nbeta_[1]  6.315968116 0.0046902464 0.323590361  5.70229551  6.955465622\nbeta_[2] -0.002650617 0.0000756713 0.005230241 -0.01302516  0.007427617\nbeta_[3]  0.236187145 0.0011784779 0.096395210  0.05605168  0.431397191\nbeta_[4] -0.260188684 0.0007972355 0.068450894 -0.39338361 -0.128970865\nk         1.451174408 0.0008789717 0.074989979  1.30886288  1.605034378\n            n_eff      Rhat\nbeta_[1] 4759.922 1.0004160\nbeta_[2] 4777.284 1.0003663\nbeta_[3] 6690.646 1.0003806\nbeta_[4] 7371.994 0.9999980\nk        7278.749 0.9999244\n\n\nPlotting the marginal posterior of \\(k\\)\n\n# Plot histogram from stan draws\npostsamples &lt;- extract(fit, pars = c(\"beta_\",\"k\"))\nhist(postsamples$k, 50, freq = FALSE, col = colors[5], \n     xlab = expression(k), ylab = \"posterior density\", \n     main = expression(k))\n\n\n\n\n\n\n\n\nPlotting the marginal posteriors for each \\(\\beta\\) coefficient\n\nvarNames = c(\"intercept\", \"age\", \"sex\", \"ph.ecog\")\npar(mfrow = c(2,2))\nfor (j in 1:p){\n  hist(postsamples$beta_[,j], 50, col = colors[6], \n       xlab = expression(beta), ylab = \"density\", main = varNames[j])\n}"
  },
  {
    "objectID": "exercises/ch10solutions.html",
    "href": "exercises/ch10solutions.html",
    "title": "Chapter 10 - Markov Chain Monte Carlo: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution.\n\nExercise 10.1\nThis exercise extends the analysis of the Weibull model for survival times of lung cancer patients in Exercise to the regression setting. The survival times are here modelled as independent Weibull distributed with a scale parameter \\(\\lambda\\) that is a function of covariates, i.e. using a Weibull regression model. The response variable time is denoted by \\(y\\) and is modelled as a function of the three covariates age, sex and ph.ecog (ECOG performance score). The model for patient \\(i\\) is: \\[\ny_i \\vert \\mathbf{x}_i, \\boldsymbol{\\beta}, k \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Weibull}\\big(\\lambda_i = \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}),k\\big).\n\\] where \\(\\boldsymbol{\\beta}\\) is the vector with regression coefficients. Note that by the properties of the Weibull distribution, the conditional mean in this model is \\(\\mathbb{E}(y \\vert \\mathbf{x}_i) = \\lambda_i\\Gamma(1+1/k)\\), so the regression coefficients do not quite have the usual interpretation of the effect on the conditional mean. The three covariates are placed in a \\(n\\times p\\) matrix \\(\\mathbf{X}\\) with the first column being one for all observations to model the intercept. Use a multivariate normal prior for \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0},\\tau^2\\mathbf{I}_p)\\) with the non-informative choice \\(\\tau = 100\\), and the prior \\(\\ k \\sim \\mathrm{logNormal}(0,2^2)\\). Remove the patients with missing values in the selected covariates.\nSample from the posterior distribution \\(p(\\boldsymbol{\\beta}, k \\vert \\mathbf{y}, \\mathbf{X})\\) using HMC in stan. Plot the marginal posterior for \\(k\\) and the marginal posteriors of each regression coefficient.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(rstan)\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nThe code below defines the iid Weibull survival model with censored data in stan. The code here extends this example in the Stan User Guide to the case with different censoring points for each patient. Note the target += construction where the censored data points are added to the target (the log posterior) after the initial uncensored (observed) data are included in the log posterior with the y_obs ~ weibull(k, lambda) statement. The weibull_lccdf function in stan is a convenience function that computes the survival probability \\(\\mathrm{Pr}(X &gt;= x) = 1 - F(x)\\), where \\(F()\\) is the cdf of the Weibull distribution. There are _lccdf versions of all distribution in stan.\n\nweibull_survivalmodel &lt;- '\ndata {\n\n  // Data\n  int&lt;lower=0&gt; N_obs;\n  int&lt;lower=0&gt; N_cens;\n  array[N_obs] real y_obs;\n  array[N_cens] real y_cens;\n  \n  // Model setting\n  real&lt;lower=0&gt; k;\n  \n  // Prior hyperparameters theta ~ Gamma(alpha, beta)\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; beta;\n}\nparameters {\n  real lambda;\n}\nmodel {\n  lambda ~ gamma(alpha, beta); // specifies the prior\n  y_obs ~ weibull(k, lambda);  // add the observed (non-censored) data\n  target += weibull_lccdf(y_cens | k, lambda); // add censored. lccdf is 1-cdf\n}\n'\n\nWe set up the data and prior lists that will be supplied to stan:\n\nlibrary(survival) # loads the lung cancer data as `lung`\nk = 3/2\ny_obs &lt;- lung %&gt;% filter(status == 2) %&gt;% pull(time)\ny_cens &lt;- lung %&gt;% filter(status == 1) %&gt;% pull(time)\n\ndata &lt;- list(N_obs = length(y_obs), N_cens = length(y_cens), \n             y_obs = y_obs, y_cens = y_cens, k = k)\n\nalpha_prior &lt;- 3     # shape parameter\nbeta_prior &lt;- 1/50   # rate parameter\nprior &lt;- list(alpha = alpha_prior, beta = beta_prior)\n\nLoad rstan and set some options\n\n#install.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', \n#                                    getOption(\"repos\")))\nsuppressMessages(library(rstan))\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nSample from the posterior distribution using HMC in stan\n\nnDraws = 5000\nfit = stan(model_code = weibull_survivalmodel, data = c(data, prior), iter = nDraws)\n\n\n\n\n\n\nExercise 10.2\nExercise plotted the posterior for the parameter \\(\\lambda\\) in the Weibull model \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] for different fixed values of \\(k\\). This exercise asks you to implement the same model in to sample from the posterior distribution of \\(\\lambda\\) for a given \\(k=3/2\\). The describes how to implement censoring in the model. The example in the User Guide has the same censoring point for all patients, which is not the case in the dataset. So you need to generalize that to a vector of censoring points, one for each patient.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(rstan)\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet data and set up prior hyperparameters\n\nlibrary(survival) # loads the lung cancer data as `lung`\nlung &lt;- lung %&gt;% select(c(\"time\", \"status\", \"age\", \"sex\", \"ph.ecog\")) %&gt;% drop_na()\ny = lung$time\nX = cbind(1, lung$age, lung$sex == 2, lung$ph.ecog) # sex = 1 is female\np = dim(X)[2]\ncensored = (lung$status == 1)\ny_obs = y[-censored]\ny_cens = y[censored]\nX_obs = X[-censored,]\nX_cens = X[censored,]\nmu &lt;- rep(0,p)  # beta ~ N(mu, tau^2*I)\ntau &lt;- 100    \nmu_k &lt;- 0       # k ~ LogNormal(mu_k, sigma_k^2)\nsigma_k &lt;- 2\n\nSet up the stan model for the Weibull regression.\n\nweibull_survivalreg &lt;- '\ndata {\n\n  // Data\n  int&lt;lower=0&gt; N_obs;\n  int&lt;lower=0&gt; N_cens;\n  int&lt;lower=1&gt; p;\n  array[N_obs] real y_obs;\n  array[N_cens] real y_cens;\n  matrix[N_obs,p] X_obs;\n  matrix[N_cens,p] X_cens;\n  \n  // Prior hyperparameters k ~ LogNormal(mu_k, sigma_k) and beta_ ~ N(0, tau^2*I)\n  real mu_k;\n  real&lt;lower=0&gt; sigma_k;\n  real&lt;lower = 0&gt; tau;\n}\nparameters {\n  vector[p] beta_;\n  real&lt;lower=0&gt; k;\n}\nmodel {\n  k ~ lognormal(mu_k, sigma_k);    // specifies the prior\n  beta_ ~ normal(0, tau);\n  y_obs ~ weibull(k, exp(X_obs * beta_));  // add the observed (non-censored) data\n  target += weibull_lccdf(y_cens | k, exp(X_cens * beta_)); // add censored. \n}\n'\n\nWe set up the data and prior lists that will be supplied to stan:\n\ndata &lt;- list(p = dim(X_obs)[2], N_obs = length(y_obs), N_cens = length(y_cens), \n             y_obs = y_obs, y_cens = y_cens, X_obs = X_obs, X_cens = X_cens)\n\nprior &lt;- list(tau = tau, mu_k = mu_k, sigma_k = sigma_k)\n\nLoad rstan and set some options\n\n#install.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', \n#                                    getOption(\"repos\")))\nsuppressMessages(library(rstan))\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nSample from the posterior distribution using HMC in stan\n\nnDraws = 5000\nfit = stan(model_code = weibull_survivalreg, data = c(data, prior), iter = nDraws)\n\n\ns &lt;- summary(fit, pars = c(\"beta_\", \"k\"), probs = c(0.025, 0.975))\ns$summary  # results from all the different runs (chains) merged.\n\n                 mean      se_mean          sd        2.5%        97.5%\nbeta_[1]  6.326162394 4.869551e-03 0.323775649  5.70127222  6.968225881\nbeta_[2] -0.002834145 7.845797e-05 0.005254269 -0.01319023  0.007348398\nbeta_[3]  0.234288159 1.080355e-03 0.095184244  0.05184712  0.425373285\nbeta_[4] -0.259055417 8.814885e-04 0.069571466 -0.39388588 -0.124469559\nk         1.449843839 8.886235e-04 0.074705666  1.30512775  1.599808428\n            n_eff     Rhat\nbeta_[1] 4420.898 1.001538\nbeta_[2] 4484.877 1.001563\nbeta_[3] 7762.421 1.000146\nbeta_[4] 6229.154 1.000016\nk        7067.592 1.000660\n\n\nPlotting the marginal posterior of \\(k\\)\n\n# Plot histogram from stan draws\npostsamples &lt;- extract(fit, pars = c(\"beta_\",\"k\"))\nhist(postsamples$k, 50, freq = FALSE, col = colors[5], \n     xlab = expression(k), ylab = \"posterior density\", \n     main = expression(k))\n\n\n\n\n\n\n\n\nPlotting the marginal posteriors for each \\(\\beta\\) coefficient\n\nvarNames = c(\"intercept\", \"age\", \"sex\", \"ph.ecog\")\npar(mfrow = c(2,2))\nfor (j in 1:p){\n  hist(postsamples$beta_[,j], 50, col = colors[6], \n       xlab = expression(beta), ylab = \"density\", main = varNames[j])\n}"
  },
  {
    "objectID": "exercises/ch2/gamma_post.html",
    "href": "exercises/ch2/gamma_post.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.4\nLet \\(X_1,\\ldots,X_n\\) be an iid sample from a distribution with density function \\[\np(x) \\propto \\theta^2 x \\exp (-x\\theta)\\quad \\text{ for } x&gt;0 \\text{ and } \\theta&gt;0.\n\\] Find the conjugate prior for this distribution and derive the posterior distribution from an iid sample \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood function from a sample \\(x_1,\\ldots,x_n\\) is\n\\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\prod_{i=1}^n\\theta^2 x_i \\exp (-x_i\\theta) \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\n\\]\nThis likelihood resembles a Gamma distribution, so a good guess for a conjugate prior would be the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) distribution; to see that this is indeed a reasonable guess, note that the particular form of the Gamma density (a power of \\(\\theta\\) times an exponential in \\(\\theta\\)) makes it closed under multiplication. The posterior distribution is then\n\\[\n\\begin{align}\np(\\theta|x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n      & \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\\theta^{\\alpha-1}\\exp(-\\theta\\beta) \\\\\n      & \\propto \\theta^{\\alpha + 2n - 1}\\exp\\Big(-\\theta (\\beta+\\sum_{i=1}^n x_i) \\Big)\n\\end{align}\n\\] and the posterior is therefore \\(\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + 2n,\\beta + \\sum_{i=1}^n x_i)\\). Since the posterior belongs to the same (Gamma) family as the prior, the Gamma prior is indeed conjugate to this likelihood."
  },
  {
    "objectID": "exercises/ch1/plagiarism.html",
    "href": "exercises/ch1/plagiarism.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "A university uses an automatic tool to detect plagiarism in student essays. The tool has a sensitivity of 0.95 (probability of flagging plagarism when the essay is plagiarized) and a specificity of 0.90 (probability of not flagging plagarism when the essay is not plagiarized). Assume that 1% of all students actually plagiarize. If a student is flagged by the tool, what is the probability that the student actually has plagiarized?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(\\mathrm{A}\\) be the event that a student has plagiarized and \\(\\mathrm{B}\\) the event that the student is flagged by the tool. We want to compute \\(\\mathrm{Pr}(A|B)\\) given that \\(\\mathrm{Pr}(B|A)=0.95\\), \\(\\mathrm{Pr}(B^c|A^c)=0.90\\) and \\(\\mathrm{Pr}(A)=0.01\\). Using Bayes’ theorem we get \\[\\begin{align*}\n    \\mathrm{Pr}(A|B) =\\frac{\\mathrm{Pr}(B|A)\\mathrm{Pr}(A)}{\\mathrm{Pr}(B|A)\\mathrm{Pr}(A)+\\mathrm{Pr}(B|A^c)\\mathrm{Pr}(A^c)} \\approx 0.0876.\n  \\end{align*}\\] Hence, even if the student is flagged by the tool there is still only an 8.76% probability that the student has actually plagiarized."
  },
  {
    "objectID": "exercises/ch3solutions.html",
    "href": "exercises/ch3solutions.html",
    "title": "Chapter 3 - Multi-parameter models: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution.\n\nExercise 3.1\nA dataset contains observations on measurements of pesticide for \\(n=24\\) pike fish in a lake in southern Italy. The sample mean pesticide level is \\(\\bar{x} = 12.5\\) with a sample standard deviation of \\(s=3.1\\). Assume that the pesticide levels \\(X_1,\\ldots,X_n|\\theta,\\sigma^2 \\sim \\mathrm{N}(\\theta,\\sigma^2)\\) are independent and identically distributed normal random variables with unknown mean \\(\\theta\\) and unknown variance \\(\\sigma^2\\).\nCompute the joint posterior distribution for \\(\\theta\\) and \\(\\sigma^2\\) using a conjugate prior distribution. Use a prior mean for \\(\\theta\\) of \\(8\\) and an (imaginary) prior sample size of \\(5\\). Use \\(\\sigma_0^2 = 1^2\\) and \\(\\nu_0 = 3\\) degrees of freedom in your prior for \\(\\sigma^2\\). Plot the marginal posterior distribution of \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe posterior distribution is given by \\[\n\\begin{aligned}\n\\theta | \\sigma^2 , \\boldsymbol{x} &\\sim N\\Big(\\mu_n,\\frac{\\sigma^2}{\\kappa_n}\\Big) \\\\\n\\sigma^2 | \\boldsymbol{x} &\\sim \\mathrm{Inv-}\\chi^2(\\nu_n,\\sigma_n^2)\n\\end{aligned}\n\\] where \\[\n\\begin{aligned}\n\\mu_n &= w \\bar x + (1-w)\\mu_0 \\\\\nw &= \\frac{n}{\\kappa_0+n} \\\\\n\\kappa_n &= \\kappa_0 + n \\\\\n\\nu_n &= \\nu_0 + n  \\\\\n\\nu_n\\sigma_n^2 &= \\nu_0\\sigma_0^2 + (n-1)s^2 + \\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar x -\\mu_0)^2\n\\end{aligned}\n\\] We have\n\n# data \nn = 24\nxBar = 12.5\ns2 = 3.1^2\n\n# prior\nmu0 = 8\nkappa0 = 5\nnu0 = 3\nsigma02 = 1^2\n\n# posterior\nw = n/(kappa0 + n)\nmun = w*xBar + (1-w)*mu0\nkappan = kappa0 + n\nnun = nu0 + n\nsigman2 = (nu0*sigma02 + (n-1)*s2  + \n             (kappa0*n/(kappa0 + n))*(xBar - mu0)^2 )/nun\n\nSo the joint posterior is \\[\n\\begin{aligned}\n\\theta | \\sigma^2 , \\boldsymbol{x} &\\sim N\\Big(11.724,\\frac{\\sigma^2}{29}\\Big) \\\\\n\\sigma^2 | \\boldsymbol{x} &\\sim \\mathrm{Inv-}\\chi^2(27,11.401)\n\\end{aligned}\n\\] The marginal posterior for \\(\\theta\\) is \\[\n\\theta \\vert \\boldsymbol{x} \\sim t\\Big(\\mu_n,\\frac{\\sigma_n^2}{\\kappa_n},\\nu_n\\Big) = t\\Big(27,\\frac{11.401}{29},27\\Big)\n\\] Plotting the marginal posterior of \\(\\theta\\):\n\nthetaGrid = seq(7, 18, length = 1000)\nsigma2Grid = seq(0.001, 30, length = 1000)\ndtdist &lt;- function(x, mu, sigma2, nu){\n  return((1/sqrt(sigma2))*dt(x = (x - mu)/sqrt(sigma2), df = nu))\n}\nplot(thetaGrid, dtdist(thetaGrid, mun, sigman2/kappan, nun), type = \"l\", \n     xlab = expression(theta), ylab = \"posterior density\", \n     col = \"indianred\", main = expression(theta))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.3\nLet \\(X_1,\\ldots,X_n \\mid \\theta,\\sigma^2 \\overset{\\mathrm{iid}}{\\sim} \\mathrm{N}(\\theta,\\sigma^2)\\), where \\(\\theta\\) is assumed known. Show that the \\(\\mathrm{Inv-}\\chi^2\\) distribution is a conjugate prior for \\(\\sigma^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe normal density function for a single observation \\(x_i\\) is \\[\np(x_i \\vert \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}(x_i-\\theta)^2\\Big) \\propto \\frac{1}{(\\sigma^2)^{1/2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}(x_i-\\theta)^2\\Big)\n\\] The likelihood for the iid normal model with known mean \\(\\theta\\) is therefore the product of \\(n\\) such density functions: \\[\n\\begin{aligned}\np(x_1,\\ldots,x_n \\vert \\theta, \\sigma^2) = \\prod_{i=1}^n p(x_i \\vert \\theta, \\sigma^2) &\\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\theta)^2\\Big) \\\\\n&\\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{n s^2}{2\\sigma^2}\\Big)\n\\end{aligned}\n\\] where we have defined \\(s^2 = \\frac{\\sum_{i=1}^n (x_i-\\theta)^2}{n}\\) as the sample standard deviation (dividing by \\(n\\) instead of \\(n-1\\) since the mean \\(\\theta\\) is assumed known).\nThe density of the \\(\\sigma^2 \\sim \\mathrm{Scaled-Inv-}\\chi^2(\\nu_0,\\sigma_0^2)\\) prior is of the form \\[\np(\\sigma^2) \\propto \\frac{1}{(\\sigma^2)^{1+\\nu_0/2}}\\exp\\Big( -\\frac{\\nu_0\\sigma_0^2}{2\\sigma^2} \\Big)\n\\] The posterior distribution from using the \\(\\sigma^2 \\sim \\mathrm{Scaled-Inv-}\\chi^2(\\nu_0,\\sigma_0^2)\\) prior is given by Bayes’ theorem as (to avoid cluttering the notation, we do not write out the conditioning on \\(\\theta\\) since it is known) \\[\n\\begin{aligned}\np(\\sigma^2 \\vert x_1,\\ldots,x_n) &\\propto\np(x_1,\\ldots,x_n \\vert \\sigma^2)p(\\sigma^2) \\\\\n& \\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{n s^2}{2\\sigma^2}\\Big)\n\\frac{1}{(\\sigma^2)^{1+\\nu_0/2}}\\exp\\Big( -\\frac{\\nu_0\\sigma_0^2}{2\\sigma^2} \\Big) \\\\\n&\\propto \\frac{1}{(\\sigma^2)^{1+(\\nu_0+n)/2}}\\exp\\Big( -\\frac{1}{2\\sigma^2}\\big( \\nu_0 \\sigma_0^2 + n s^2 \\big)  \\Big) \\\\\n&=  \\frac{1}{(\\sigma^2)^{1+(\\nu_0+n)/2}}\\exp\\Big( -\\frac{\\nu_0+n}{2\\sigma^2}\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n}  \\Big)\n\\end{aligned}\n\\] which is proportional to a \\(\\sigma^2 \\sim \\chi^2(\\nu_0+n,\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n})\\) density. Note how the location parameter in the posterior \\[\n\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n} = \\frac{\\nu_0}{\\nu_0+n} \\sigma_0^2 + \\frac{n}{\\nu_0+n}s^2\n\\] is a weighted average of prior location \\(\\sigma_0^2\\) and the data estimate \\(s^2\\), with more weight placed on the strongest information source (prior with \\(\\nu_0\\) imaginary sample data points versus the data with \\(n\\) actual data points).\n\n\n\n\n\n\nExercise 3.6\nA Swedish poll in 2024 asked \\(2311\\) persons the question: The table below gives the poll results across the eight parties in parliament and the nineth option . \nLet \\(\\boldsymbol{y} = (y_1,\\ldots,y_9)\\) denote the number of votes for each of the nine parties, where \\(y_c\\) is the number of votes for party \\(c\\). Model the data as iid from a multinomial distribution \\[\\begin{equation*}\n    \\boldsymbol{y} \\mid \\boldsymbol{\\theta} \\sim \\mathrm{Multinomial}(n,\\boldsymbol{\\theta}),\n\\end{equation*}\\] where \\(n=2311\\) is the total number of respondents and \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_9)\\) is the vector of voting proportions for each party, where \\(\\theta_c\\) is the proportion of votes for party \\(c\\).\nThe previous election in 2022 resulted in the following voting percentages: \nUse Dirichlet prior \\(\\boldsymbol{\\theta} \\sim \\mathrm{Dirichlet}(\\boldsymbol{\\alpha})\\) with \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_9)\\). Set the priorhyperparameters based on the election results from 2022, but assuming that the prior information is equivalent to only \\(400\\) respondents today.\na) What is the posterior distribution for the voting shares?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSet up data for the current poll, and the results from the election in 2022:\n\ny = c(410, 88, 83, 81, 721, 196, 238, 434, 60)\nelection2022 = c(19.10, 4.61, 6.71, 5.34, 30.33, 6.75, 5.08, 20.54, 1.54)\nproportionElection2022 = election2022/100\nalphaPrior = 400*proportionElection2022 # not integers, but that's OK\n\nCompute the posterior hyperparameters by adding up the data count \\(y_c\\) in each category with its corresponding prior count \\(\\alpha_c\\):\n\nalphaPost = alphaPrior + y\n\nSo the (joint) posterior distribution for the Swedish voting shares are \\[\n\\boldsymbol{\\theta} \\vert \\boldsymbol{y} \\sim \\mathrm{Dirichlet}\\Big(486.4, 106.44, 109.84, 102.36, 842.32, 223, 258.32, 516.16, 66.16\\Big)\n\\]\n\n\n\nb) What is the marginal posterior distribution for the voting share of the S-party? Plot the distribution without resorting to simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom the properties of the Dirichlet distribution box in the Bayesian Learning book, we know that if \\((\\theta_1,\\ldots,\\theta_C) \\sim \\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_C)\\), then marginal distribution for the probability/proportion in category \\(c\\) is\n\\[\n\\theta_c \\sim \\mathrm{Beta}(\\alpha_c, \\alpha_+ -\\alpha_c).\n\\] We can apply this result to get the marginal posterior for \\(\\theta_5\\) (the proportion of S-voters), we just need to remember that the parameters in the posterior are \\(\\alpha_c + y_c\\) for \\(c=1,\\ldots,C\\), or alphaPost in the code. The marginal posterior for the proportion of S-party voters is \\[\n\\theta_5 \\vert \\boldsymbol{y} \\sim \\mathrm{Beta}\\Big(842.32, 1868.68 \\Big),\n\\] which we can plot:\n\nthetaGrid = seq(0, 1, length = 1000)\nplot(thetaGrid, dbeta(thetaGrid, alphaPost[5], sum(alphaPost) - alphaPost[5]), \n     xlab = \"proportion of S-voters\", ylab = \"posterior density\", type = \"l\", \n     lwd = 3, col = \"indianred\")\n\n\n\n\n\n\n\n\n\n\n\nc) A party needs at least 4% of the votes to get into parliament. Use simulation to compute the posterior probability that both L and KD parties do not make it to parliament.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nDefine a Dirichlet random number generator using the algorithm in Chapter 3.\n\nrdirichlet &lt;- function(nIter, param){\n  nCat &lt;- length(param)\n  thetaDraws &lt;- matrix(0,nIter,nCat) #Storage\n  for (j in 1:nCat){\n    thetaDraws[,j] &lt;- rgamma(nIter, param[j], 1)\n  }\n  for (i in 1:nIter){\n    thetaDraws[i,] = thetaDraws[i,]/sum(thetaDraws[i,])\n  }\n  return(thetaDraws)\n}\n\nUse the simulator to simulate \\(10000\\) draws from the posterior distribution. For each draw check if both the L and KD party each have less than 4% of the votes.\n\nnSim = 10000\npostDraws = rdirichlet(nSim, alphaPost)\nbothPartyInParliament = rep(NA, nSim)\nfor (i in 1:nSim){\n  bothPartyInParliament[i] = all(postDraws[i,c(2,4)] &lt; 0.04)\n}\n\nThe posterior probability that both parties are below \\(4\\%\\) is 0.4318.\n\n\n\n\n\n\nExercise 3.7\nThe monthly income (in thousands Swedish Krona) of ten randomly selected persons are: 14, 25, 45, 25, 30, 33, 19, 50, 34 and 67. The log-normal distribution (see Box~ and ) is a commonly used model for income distributions. Let \\(Y_1,\\ldots,Y_n \\vert \\theta,\\sigma^{2}\\overset{\\mathrm{iid}}{\\sim}\\mathrm{LN}(\\theta,\\sigma^{2})\\), where \\(\\theta=\\log(33)\\) is assumed to be known but \\(\\sigma^{2}\\) is unknown with non-informative prior \\(p(\\sigma^{2})\\propto1/\\sigma^{2}\\). The posterior for \\(\\sigma^{2}\\) given \\(\\theta\\) is the \\(\\mathrm{Inv-}\\chi^{2}(n,\\tau^{2})\\) distribution, where \\[\\begin{equation*}\n    \\tau^{2}=\\frac{\\sum_{i=1}^{n}(\\log y_{i}-\\theta)^{2}}{n}.\n  \\end{equation*}\\]\na) Simulate \\(10,000\\) draws from the posterior of \\(\\sigma^{2}\\), assuming \\(\\theta=\\log(33)\\) is known. Plot a histogram of the posterior draws of the \\(\\sigma\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny = c(14, 25, 45, 25, 30, 33, 19, 50, 34, 67)\nlogy = log(y)\nn = length(y)\ntheta = log(33)\ntau2 = sum((logy - theta)^2)/n\n\n# Defining a function that simulates nSim draws\n# from the scaled inverse Chi-square distribution\nrScaledInvChi2 &lt;- function(nSim, df, scale){\n  return((df*scale)/rchisq(nSim, df=df))\n}\nnSim = 10000\nsigma2Draws = rScaledInvChi2(nSim, n, tau2)\nhist(sqrt(sigma2Draws), 100, col = \"steelblue\", main = \"\", freq = FALSE,\n     xlab = expression(sigma), ylab = \"posterior density\")\n\n\n\n\n\n\n\n\n\n\n\nb) A commonly used measure of income inequality is the Gini coefficient, \\(0\\leq G\\leq1\\), where \\(G=0\\) is complete income equality, and \\(G=1\\) means complete income inequality. It can be shown that \\(G=2\\Phi\\left(\\sigma/\\sqrt{2}\\right)-1\\) when incomes follow a \\(\\mathrm{LN}(\\theta,\\sigma^{2})\\) distribution, where \\(\\Phi(z)\\) is the cumulative distribution function (CDF) for the standard normal distribution. Use the posterior draws from a) to compute the posterior distribution of the Gini coefficient \\(G\\). Plot a histogram of the posterior draws of \\(G\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngini = rep(0, nSim)\nfor (i in 1:nSim){\n  gini[i] = 2*pnorm(sqrt(sigma2Draws[i])/sqrt(2)) - 1\n} \nhist(gini, 100, col = \"cornsilk\", main = \"\", freq = FALSE,\n     xlab = \"Gini coefficient, G\", ylab = \"posterior density\")\n\n\n\n\n\n\n\n\n\n\n\nc) Compute a \\(95\\)% Highest Posterior Density (HPD) interval for \\(G\\). To compute the HPD interval you will need an estimate of the posterior density for \\(G\\); a common approach is to use a kernel density estimator, for example the density function in R.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first estimate the posterior density from the posterior draws using a kernel density estimator, and plotting it on top of the histogram:\n\nkde = density(gini)\nginiGrid = kde$x\npostDens = kde$y\nhist(gini, 100, col = \"cornsilk\", main = \"\", freq = FALSE,\n     xlab = \"Gini coefficient, G\", ylab = \"posterior density\")\nlines(giniGrid, postDens, lwd = 3, col = \"indianred\")\n\n\n\n\n\n\n\n\nNow that we have the posterior density, and we can see that it is unimodal, we can use the same code as in the Solution to Exercise @q:iid_poisson_ambulance_b.\n\nbinWidth = giniGrid[2]- giniGrid[1]\n# first, sort the density values from highest to lowest\npostDensOrdered = sort(postDens, decreasing = TRUE)  \n# reorder the thetaValues so that they still match the density values\nginiOrdered = giniGrid[order(postDens, decreasing = TRUE)] \ncumsumPostDens = cumsum(binWidth*postDensOrdered) # posterior cdf \ninHPD = which(cumsumPostDens &lt; 0.95) # find highest pdf vals up to 95% quota.\nhpd = c(min(giniOrdered[inHPD]), max(giniOrdered[inHPD]))\nmessage(paste0(\"The 95% HPD interval for the Gini coefficient is (\", hpd[1], \",\", hpd[2],\")\"))\n\nThe 95% HPD interval for the Gini coefficient is (0.159044415119536,0.392529484142363)"
  },
  {
    "objectID": "exercises/ch1/favorite_team.html",
    "href": "exercises/ch1/favorite_team.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Think about an event that you are uncertain about, for example the event that you favorite sports team wins their next game. Try to elicit your subjective probability for the event by considering a betting situation where you win $100 if the event occurs. Start with a price of $1 and ask yourself if you would be willing to take the bet. Then gradually increase the price of the bet until you are indifferent between taking the bet or not.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMy favorite sports team is the Italian soccer team Napoli. They are playing Juventus at home in Naples next week. I would be willing to pay around $75, but not more,for a bet giving me $100 if Napoli wins. Hence, my subjective probability is around 0.75."
  },
  {
    "objectID": "exercises/ch1solutions.html",
    "href": "exercises/ch1solutions.html",
    "title": "Chapter 1 - The Bayesics: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution."
  },
  {
    "objectID": "exercises/ch1/plagiarism.html#exercise-1.1",
    "href": "exercises/ch1/plagiarism.html#exercise-1.1",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "A university uses an automatic tool to detect plagiarism in student essays. The tool has a sensitivity of 0.95 (probability of flagging plagarism when the essay is plagiarized) and a specificity of 0.90 (probability of not flagging plagarism when the essay is not plagiarized). Assume that 1% of all students actually plagiarize. If a student is flagged by the tool, what is the probability that the student actually has plagiarized?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(\\mathrm{A}\\) be the event that a student has plagiarized and \\(\\mathrm{B}\\) the event that the student is flagged by the tool. We want to compute \\(\\mathrm{Pr}(A|B)\\) given that \\(\\mathrm{Pr}(B|A)=0.95\\), \\(\\mathrm{Pr}(B^c|A^c)=0.90\\) and \\(\\mathrm{Pr}(A)=0.01\\). Using Bayes’ theorem we get \\[\\begin{align*}\n    \\mathrm{Pr}(A|B) =\\frac{\\mathrm{Pr}(B|A)\\mathrm{Pr}(A)}{\\mathrm{Pr}(B|A)\\mathrm{Pr}(A)+\\mathrm{Pr}(B|A^c)\\mathrm{Pr}(A^c)} \\approx 0.0876.\n  \\end{align*}\\] Hence, even if the student is flagged by the tool there is still only an 8.76% probability that the student has actually plagiarized."
  },
  {
    "objectID": "exercises/ch1/plagiarism.html#exercise-1.2",
    "href": "exercises/ch1/plagiarism.html#exercise-1.2",
    "title": "Bayesian Learning Book",
    "section": "Exercise 1.2",
    "text": "Exercise 1.2\nThink about an event that you are uncertain about, for example the event that you favorite sports team wins their next game. Try to elicit your subjective probability for the event by considering a betting situation where you win $100 if the event occurs. Start with a price of $1 and ask yourself if you would be willing to take the bet. Then gradually increase the price of the bet until you are indifferent between taking the bet or not."
  },
  {
    "objectID": "exercises/ch1/plagiarism.html#exercise-1.3",
    "href": "exercises/ch1/plagiarism.html#exercise-1.3",
    "title": "Bayesian Learning Book",
    "section": "Exercise 1.3",
    "text": "Exercise 1.3\nThink about a political party that you care about. Elicit a histogram to represent your prior distribution for the party’s support in percent, \\(0\\leq \\theta \\leq 100\\), in the next national election, by asking yourself questions about the probability of certain intervals. For example, what is the probability that the party’s support is below 10%? Between 10% and 20%? And so on. Make sure that the final histogram integrates to one over the full support, and plot it."
  },
  {
    "objectID": "exercises/ch1/plagiarism.html#exercise-1.4",
    "href": "exercises/ch1/plagiarism.html#exercise-1.4",
    "title": "Bayesian Learning Book",
    "section": "Exercise 1.4",
    "text": "Exercise 1.4\nReproduce the first row of Figure \\(\\ref{fig:berndiscr}\\) by writing your own code in your favorite programming language."
  },
  {
    "objectID": "exercises/ch1solutions.html#exercise-1.1",
    "href": "exercises/ch1solutions.html#exercise-1.1",
    "title": "Chapter 1 - The Bayesics: Exercise solutions",
    "section": "Exercise 1.1",
    "text": "Exercise 1.1\nA university uses an automatic tool to detect plagiarism in student essays. The tool has a sensitivity of 0.95 (probability of flagging plagarism when the essay is plagiarized) and a specificity of 0.90 (probability of not flagging plagarism when the essay is not plagiarized). Assume that 1% of all students actually plagiarize. If a student is flagged by the tool, what is the probability that the student actually has plagiarized?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(\\mathrm{A}\\) be the event that a student has plagiarized and \\(\\mathrm{B}\\) the event that the student is flagged by the tool. We want to compute \\(\\mathrm{Pr}(A|B)\\) given that \\(\\mathrm{Pr}(B|A)=0.95\\), \\(\\mathrm{Pr}(B^c|A^c)=0.90\\) and \\(\\mathrm{Pr}(A)=0.01\\). Using Bayes’ theorem we get \\[\\begin{align*}\n    \\mathrm{Pr}(A|B) =\\frac{\\mathrm{Pr}(B|A)\\mathrm{Pr}(A)}{\\mathrm{Pr}(B|A)\\mathrm{Pr}(A)+\\mathrm{Pr}(B|A^c)\\mathrm{Pr}(A^c)} \\approx 0.0876.\n  \\end{align*}\\] Hence, even if the student is flagged by the tool there is still only an 8.76% probability that the student has actually plagiarized."
  },
  {
    "objectID": "exercises/ch1solutions.html#exercise-1.2",
    "href": "exercises/ch1solutions.html#exercise-1.2",
    "title": "Chapter 1 - The Bayesics: Exercise solutions",
    "section": "Exercise 1.2",
    "text": "Exercise 1.2\nThink about an event that you are uncertain about, for example the event that you favorite sports team wins their next game. Try to elicit your subjective probability for the event by considering a betting situation where you win $100 if the event occurs. Start with a price of $1 and ask yourself if you would be willing to take the bet. Then gradually increase the price of the bet until you are indifferent between taking the bet or not.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMy favorite sports team is the Italian soccer team Napoli. They are playing Juventus at home in Naples next week. I would be willing to pay around $75, but not more,for a bet giving me $100 if Napoli wins. Hence, my subjective probability is around 0.75."
  },
  {
    "objectID": "exercises/ch1solutions.html#exercise-1.3",
    "href": "exercises/ch1solutions.html#exercise-1.3",
    "title": "Chapter 1 - The Bayesics: Exercise solutions",
    "section": "Exercise 1.3",
    "text": "Exercise 1.3\nThink about a political party that you care about. Elicit a histogram to represent your prior distribution for the party’s support in percent, \\(0\\leq \\theta \\leq 100\\), in the next national election, by asking yourself questions about the probability of certain intervals. For example, what is the probability that the party’s support is below 10%? Between 10% and 20%? And so on. Make sure that the final histogram integrates to one over the full support, and plot it.\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch1solutions.html#exercise-1.4",
    "href": "exercises/ch1solutions.html#exercise-1.4",
    "title": "Chapter 1 - The Bayesics: Exercise solutions",
    "section": "Exercise 1.4",
    "text": "Exercise 1.4\nReproduce the first row of Figure by writing your own code in your favorite programming language.\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch1/voting_share_histogram.html",
    "href": "exercises/ch1/voting_share_histogram.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Think about a political party that you care about. Elicit a histogram to represent your prior distribution for the party’s support in percent, \\(0\\leq \\theta \\leq 100\\), in the next national election, by asking yourself questions about the probability of certain intervals. For example, what is the probability that the party’s support is below 10%? Between 10% and 20%? And so on. Make sure that the final histogram integrates to one over the full support, and plot it.\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch1/voting_share_histogram.html#exercise-1.3",
    "href": "exercises/ch1/voting_share_histogram.html#exercise-1.3",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Think about a political party that you care about. Elicit a histogram to represent your prior distribution for the party’s support in percent, \\(0\\leq \\theta \\leq 100\\), in the next national election, by asking yourself questions about the probability of certain intervals. For example, what is the probability that the party’s support is below 10%? Between 10% and 20%? And so on. Make sure that the final histogram integrates to one over the full support, and plot it.\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch1/favorite_team.html#exercise-1.2",
    "href": "exercises/ch1/favorite_team.html#exercise-1.2",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Think about an event that you are uncertain about, for example the event that you favorite sports team wins their next game. Try to elicit your subjective probability for the event by considering a betting situation where you win $100 if the event occurs. Start with a price of $1 and ask yourself if you would be willing to take the bet. Then gradually increase the price of the bet until you are indifferent between taking the bet or not.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMy favorite sports team is the Italian soccer team Napoli. They are playing Juventus at home in Naples next week. I would be willing to pay around $75, but not more,for a bet giving me $100 if Napoli wins. Hence, my subjective probability is around 0.75."
  },
  {
    "objectID": "exercises/ch1/discrete_bayes.html",
    "href": "exercises/ch1/discrete_bayes.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Reproduce the first row of Figure \\(\\ref{fig:berndiscr}\\) by writing your own code in your favorite programming language.\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch1/discrete_bayes.html#exercise-1.4",
    "href": "exercises/ch1/discrete_bayes.html#exercise-1.4",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Reproduce the first row of Figure \\(\\ref{fig:berndiscr}\\) by writing your own code in your favorite programming language.\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch2/iid_gamma2.html",
    "href": "exercises/ch2/iid_gamma2.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.4\nLet \\(X_1,\\ldots,X_n\\) be an iid sample from a distribution with density function \\[\np(x) \\propto \\theta^2 x \\exp (-x\\theta)\\quad \\text{ for } x&gt;0 \\text{ and } \\theta&gt;0.\n\\] Find the conjugate prior for this distribution and derive the posterior distribution from an iid sample \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood function from a sample \\(x_1,\\ldots,x_n\\) is\n\\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\prod_{i=1}^n\\theta^2 x_i \\exp (-x_i\\theta) \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\n\\]\nThis likelihood resembles a Gamma distribution, so a good guess for a conjugate prior would be the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) distribution; to see that this is indeed a reasonable guess, note that the particular form of the Gamma density (a power of \\(\\theta\\) times an exponential in \\(\\theta\\)) makes it closed under multiplication. The posterior distribution is then\n\\[\n\\begin{align}\np(\\theta|x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n      & \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\\theta^{\\alpha-1}\\exp(-\\theta\\beta) \\\\\n      & \\propto \\theta^{\\alpha + 2n - 1}\\exp\\Big(-\\theta (\\beta+\\sum_{i=1}^n x_i) \\Big)\n\\end{align}\n\\] and the posterior is therefore \\(\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + 2n,\\beta + \\sum_{i=1}^n x_i)\\). Since the posterior belongs to the same (Gamma) family as the prior, the Gamma prior is indeed conjugate to this likelihood."
  },
  {
    "objectID": "exercises/ch2/iid_exp_survival_all.html",
    "href": "exercises/ch2/iid_exp_survival_all.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.8\nExercise modelled the survival times of uncensored lung cancer patients with an iid exponential model. In this exercise we will extend that analysis to include also the censored patients, using the same prior as in Exercise . Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values. Finally, assess the fit of the exponential model by plotting a histogram of \\(\\texttt{time}\\) and overlay the pdf of the exponential model with the parameter \\(\\theta\\) estimated with the posterior mode.\n\\(\\textit{Hint}\\): The posterior is no longer tractable due to contributions of the censored patients to the likelihood. For the censored patients we only know that they lived the number of days recorded in the dataset. The likelihood contribution \\(p(x_c \\vert \\theta)\\) for the \\(c\\)th censored patient with recorded time \\(x_c\\) is therefore \\(p(X \\geq x_c \\vert \\theta) = e^{-\\theta x_c}\\), which follows from the distribution function of the exponential distribution \\(p(X \\leq x \\vert \\theta) = 1 - e^{-\\theta x}\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nThe likelihood for all data, censored and uncensored, is \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\theta) & = \\prod_{i=1}^n p(x_i \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} p(x_c \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} \\left(1 - F(x_c \\vert \\theta)\\right)\n\\end{align}\n\\] where \\(\\mathcal{U}\\) and \\(\\mathcal{C}\\) are the sets of observation indicies for the uncensored and censored data, respectively. The likelihood for the uncensored data (the first product) is the same as before \\[\n\\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) = \\prod_{u \\in \\mathcal{U}} \\theta e^{-\\theta x_u} = \\theta^{n_u} e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u},\n\\] where \\(n_u\\) is the number of uncensored observations. The likelihood contribution for each observation in the censored set (the second product) is the survival function \\[\n\\mathrm{Pr}(X \\geq x_c) = 1 - F(x_c \\vert \\theta),\n\\] where \\(F(x_c \\vert \\theta) = 1 - e^{-x_c \\theta}\\) is the cumulative distribution function of the exponential distribution evaluated at \\(x_c\\).\nSo the likelihood function is \\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\theta^n e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u} \\times e^{-\\theta \\sum_{u \\in \\mathcal{U}} x_c} = \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i}.\n\\] where one should note that \\(\\theta\\) is raised to the number of uncensored observations, \\(n_u\\) while the sum in the exponential term includes both uncensored and censored observations.\nBy Bayes’ theorem, the posterior distribution is again a Gamma distribution \\[\n\\begin{align}\np(\\theta \\vert x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n& \\propto \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i} \\times \\theta^{\\alpha-1}e^{-\\beta\\theta} \\\\\n& = \\theta^{\\alpha + n_u - 1} e^{ -\\theta(\\beta + \\sum_{i = 1}^n x_i)},\n\\end{align}\n\\] which we recognize as proportional to the following Gamma distribution \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n_u,\\beta + \\sum\\nolimits_{i=1}^n x_i).\n\\]\nThe code below plots both:\n\nthe posterior from the previous exercise (a) with only the uncensored data and\nthe posterior from with all data.\n\nThe posterior with all data is more informative and concentrates on smaller \\(\\theta\\) values. Since smaller \\(\\theta\\) values correspond to longer expected survival times, this is makes sense since the censored patients were still alive at the end of the study.\n\n# Summarize the data needed for the posterior, grouped by `status`:\ndata_summary &lt;- lung %&gt;% group_by(status) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters - only uncensored data\nalpha_post_u &lt;- alpha_prior + data_summary$n[2] # second row is uncensored data (status = 2)  \nbeta_post_u &lt;- beta_prior + data_summary$sum_x[2] # sum over uncensored observations\n\n# Compute posterior hyperparameters - all data\nalpha_post_all &lt;- alpha_prior + data_summary$n[2] # note: this is still n_u \nbeta_post_all &lt;- beta_prior + sum(data_summary$sum_x) # sum over all observations   \n\n\n# Plot the prior and the two posterior densities \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nposterior_density_u &lt;- dgamma(thetaGrid, shape = alpha_post_u, rate = beta_post_u)\nposterior_density_all &lt;- dgamma(thetaGrid, shape = alpha_post_all, rate = beta_post_all)\n\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  posterior_uncensored = posterior_density_u,\n  posterior_all = posterior_density_all\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior_uncensored\", \"posterior_all\"), \n    values = c(colors[2], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.2\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThe code below plots the histogram and the pdf of the exponential model with the parameter \\(\\theta\\) set equal to the posterior mode. It is clear that the exponential model with its monotonically decreasing density is not fitting the data well.\n\npostMode = df$thetaGrid[which.max(df$posterior_all)]\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dexp, args = list(rate = postMode), lwd = 1, \n                aes(color = \"Exponential fit\"),\n  ) +\n  labs(title = \"Exercise 2.2c - Exponential model fit to lung cancer survival\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[5]) +\n  scale_color_manual(\"\", values = colors[3]) +\n  theme_minimal()"
  },
  {
    "objectID": "exercises/ch2/iid_exp_conj.html",
    "href": "exercises/ch2/iid_exp_conj.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.1\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\) be iid exponentially distributed data. Show that the Gamma distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood from an iid sample from \\(\\mathrm{Expon}(\\theta)\\) is \\[\np(x_1,\\ldots,x_n \\vert \\theta)= \\prod_{i=1}^n p(x_i \\vert \\theta) =\n  \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\n\\] The density of the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) prior is \\[\np(\\theta) =  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\theta^{\\alpha-1}e^{-\\beta\\theta}\n             \\propto \\theta^{\\alpha-1}e^{-\\beta\\theta}\n\\]\nBy Bayes’ theorem, the posterior distribution is \\[\n\\begin{align}\n  p(\\theta \\vert x_1,\\ldots,x_n) &\\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta)   \\\\\n& \\propto \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\\theta^{\\alpha-1}e^{-\\beta\\theta}  \\\\\n& =  \\theta^{\\alpha + n - 1} e^{ -\\theta(\\beta + \\sum_{i=1}^n x_i)},\n\\end{align}\n\\] which can be recognized as proportional to the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha +n,\\beta + \\sum\\nolimits_{i=1}^n x_i)\\) distribution. Since the prior and posterior belongs to the same (Gamma) distributional family, the Gamma prior is indeed conjugate to the exponential likelihood."
  },
  {
    "objectID": "exercises/ch2/iid_exp_survival_uncens.html",
    "href": "exercises/ch2/iid_exp_survival_uncens.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.2\nThe dataset in the R package contains data on 228 patients with advanced lung cancer. We will here analyze the survival time of the patient in days (). The variable is a binary variable with if the survival time of the patient is censored (patient was still alive at the end of the study) and if the survival time was uncensored (patient was dead before the end of the study).\nIn this exercise we will only analyze the uncensored patients; Exercise \\(\\ref{ex:survival_all}\\) below asks you to analyze all patients. Assume that the survival times \\(X_1,\\ldots,X_n\\) of the uncensored patients are iid \\(\\mathrm{Expon}(\\theta)\\) distributed. Use the conjugate prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha=3,\\beta=300)\\), which can be shown to imply that the expected survival time \\(\\mathbb{E}(X \\vert \\theta) = 1/\\theta\\) for this population is around \\(200\\) days. Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom Exercise 2.1, we know that the posterior distribution is \\[\\theta \\sim \\mathrm{Gamma}(\\alpha + n_u, \\beta + \\sum\\nolimits_{u \\in \\mathcal{U}} x_u),\\] where \\(n_u\\) is the number of uncensored observations and \\(\\mathcal{U}\\) is the set of observation indices for the uncensored data.\nThe following code plots the prior, likelihood (normalized) and posterior over a grid of values for \\(\\theta\\). Note that the data is so much stronger than the prior that the posterior is virtually identical to the likelihood, which is why the normalized likelihood is not visible in the plot.\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\n\n# Summarize the data needed for the posterior, filter out censored data\ndata_summary &lt;- lung %&gt;% filter(status == 2) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters\nalpha_post &lt;- alpha_prior + data_summary$n  \nbeta_post &lt;- beta_prior + data_summary$sum_x   \n\n\n# Plot the prior and posterior densities, and the (normalized) likelihood \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nlikelihood_density &lt;- dgamma(thetaGrid, shape = data_summary$n, rate = data_summary$sum_x)\nposterior_density &lt;- dgamma(thetaGrid, shape = alpha_post, rate = beta_post)\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  likelihood = likelihood_density,\n  posterior = posterior_density\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"likelihood\", \"posterior\"), \n    values = c(colors[2], colors[1], colors[3])) +\n  labs(title = \"Survival lung cancer - uncensored patients\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()"
  },
  {
    "objectID": "exercises/ch2/iid_uniform.html",
    "href": "exercises/ch2/iid_uniform.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.7\na) Let \\(X_1,\\dots,X_n |\\theta \\sim \\mathrm{Uniform}(\\theta-1/2,\\theta+1/2)\\) for \\(-\\infty &lt; \\theta &lt;\\infty\\). The estimator \\(\\hat\\theta= \\bar X\\) is unbiased for \\(\\theta\\). Calculate for the sampling variance of \\(\\hat\\theta\\).\n\\(\\textit{Note}\\): A more efficient estimator is the mid-range \\(\\hat\\theta = (X_{\\min} + X_{\\max})/2\\), but we use the sample mean here for simplicity.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe variance of a sample mean is always of the form \\[\n\\mathbb{V}(\\bar X) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of each observation \\(X_i \\sim \\mathrm{Uniform}(\\theta-1/2,\\theta+1/2)\\). The variance of \\(\\mathrm{Uniform}(a,b)\\) variable is \\(\\frac{(b-a)^2}{2}\\), so here we have \\(\\sigma^2 = \\frac{1}{12}\\) and the sampling variance of the estimator is \\[\n\\mathbb{V}(\\bar X) = \\frac{1}{12n}\n\\]\n\n\n\nb) Derive the posterior distribution for \\(\\theta\\) assuming a uniform prior distribution over the whole real line.\n\\(\\textit{Hint}\\): once you have observed some data, some values for \\(\\theta\\) are no longer possible.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe density for each observation in the sample is \\[\np(x \\vert \\theta) = I\\Big(\\theta- \\frac{1}{2} \\leq x \\leq \\theta + \\frac{1}{2}\\Big).\n\\] where \\(I()\\) is an indicator function that returns the value one when the condition inside the parenthesis is true, and zero otherwise. The likelihood based a single observation is therefore zero whenever \\(\\theta \\geq x-\\frac{1}{2}\\) or when \\(\\theta \\leq x + \\frac{1}{2}\\). So, to highlight that we care about \\(p(x\\vert\\theta)\\) as function of \\(\\theta\\), we can write \\[\np(x \\vert \\theta) = I\\Big(x - \\frac{1}{2} \\leq \\theta \\leq x + \\frac{1}{2}\\Big).\n\\]\nThe likelihood from an iid sample of \\(n\\) observation can therefore be written \\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\prod_{i=1}^n p(x_i \\vert \\theta) = \\prod_{i=1}^n I\\Big(x_i - \\frac{1}{2} \\leq \\theta \\leq x_i + \\frac{1}{2}\\Big)\n\\] The plots below illustrate how certain \\(\\theta\\) values makes the single data point \\(x_1 = 2.1\\) impossible.\n\n\nCode\nxi = 2.1\npar(mfrow = c(3,1))\n\n# Adding the uniform density for an impossible theta\ntheta = 1.2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 1.2 is too low - distribution misses the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"orange\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"orange\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"orange\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"orange\")\n\n# Adding the uniform density for an impossible theta\ntheta = 2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 2 is OK - distribution captures the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"cornflowerblue\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"cornflowerblue\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"cornflowerblue\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"cornflowerblue\")\n\n# Adding the uniform density for an impossible theta\ntheta = 3.2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 3.2 is too high - distribution misses the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"green\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"green\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"green\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"green\")\n\n\n\n\n\n\n\n\n\nThe likelihood is non-zero only for the \\(\\theta\\) values where \\(x_i - \\frac{1}{2} \\leq \\theta \\leq x_i + \\frac{1}{2}\\) for all data observations \\(i=1,2,\\ldots,n\\). This means that the likelihood is non-zero only when \\(x_\\max - \\frac{1}{2} \\leq \\theta \\leq x_\\min + \\frac{1}{2}\\), where \\(x_\\min\\) and \\(x_\\max\\) are the minimum and maximum of the sample. With a uniform prior on \\(\\theta\\), the posterior is proportional to the likelihood, so \\[\np(\\theta \\vert x_1,\\ldots,x_n) \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\propto I\\Big( x_\\max - \\frac{1}{2} \\leq \\theta \\leq x_\\min + \\frac{1}{2} \\Big)\n\\] So the posterior distribution is \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Uniform}\\Big(x_\\max - \\frac{1}{2} , \\leq x_\\min + \\frac{1}{2} \\Big)\n\\]\n\n\n\nc) Assume that you have observed three data observations: \\(x_1 = 1.1, x_2 = 2.05, x_3 = 1.21\\). What would a frequentist conclude about \\(\\theta\\)? What would a Bayesian conclude? Discuss.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe frequentist with \\(\\bar X\\) as the estimator of \\(\\theta\\) obtains the estimate \\(\\bar x \\approx 1.453\\). The sampling standard deviation is \\(\\mathbb{S}(\\bar X) = \\sqrt{\\frac{1}{12n}} = \\sqrt{\\frac{1}{3\\cdot 12}} \\approx 0.1666\\), which is the variability of the sample mean estimator on average over all possible datasets of size \\(n=3\\). The variability is rather large since we only have three observations, so we can easily obtain a sample with three extreme (all small or all large) observations. Here is a plot of the simulated sampling distribution for \\(\\bar X\\) from a sample with three observations:\n\ntheta = 1  # any value is ok, it will be the center of the sampling distr.\nnRep = 50000\nn = 3\nxbar = rep(0, nRep)\nfor (i in 1:nRep){\n  x_rep = runif(n, min = theta - 0.5, max = theta + 0.5)\n  xbar[i] = mean(x_rep)\n}\nhist(xbar, 50, freq = FALSE, \n     main = \"sampling distribution of the sample mean\", \n     xlab = \"sample mean\", ylab = \"density\", col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n(As a side-note: note how fast the central limit theorem is here, the sampling distribution is already close to normal as \\(n=3\\))\nFor the given dataset we have \\(x_\\min = 1.1\\) and \\(x_\\max = 2.05\\) and the posterior is therefore \\[\n\\theta \\vert x_1,x_2,x_3 \\sim \\mathrm{Uniform}\\big(1.55 , 1.60 \\big)\n\\] Since we were lucky to obtain a range of the data close to \\(1\\) (the range is the difference between the maximum and minimum observations: \\(2.05-1.1 = 0.95\\)), the Bayesian gets a tight posterior which is uniform between \\(1.55\\) and \\(1.60\\). The posterior is plotted here:\n\n\nCode\nx = c(1.10, 2.05, 1.21)\nplot(x = NA, y = NA, xlim = c(1,2), ylim = c(-1,1), \n     xlab = expression(theta), ylab = \"posterior density\")\npost_low = max(x) -0.5\npost_high = min(x) + 0.5\nlines(c(post_low, post_high), c(1, 1), col = \"orange\")\nlines(c(post_low, post_low), c(0, 1), col = \"orange\")\nlines(c(post_high, post_high), c(1, 0), col = \"orange\")\nabline(h=0, lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nThe difference between the frequentist and Bayesian solutions is that the Bayesian solution conditions on the observed data, while the frequentist inferences are unconditional on the data, measuring the variability of the estimator over all possible datasets. We got lucky with a wide range in the actually observed data, so the Bayesian can provide a tight posterior for \\(\\theta\\) with little uncertainty."
  },
  {
    "objectID": "exercises/ch2/iid_geo_conj.html",
    "href": "exercises/ch2/iid_geo_conj.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.3\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Geom}(\\theta)\\) be iid from a geometric distribution with parameter \\(0&lt;\\theta&lt;1\\). Show that the Beta distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe geometric distribution has probability mass function \\[\np(x) = (1-\\theta)^{x}\\theta, \\quad \\text{ for }x=0,1,2,...\n\\] The likelihood from a sample of \\(n\\) observations is therefore \\[\np(x_{1},\\ldots,x_{n}\\vert\\theta) = \\prod_{i=1}^n p(x_i \\vert \\theta)= (1-\\theta)^{\\sum_{i=1}^n x_i}\\theta^{n}\n\\] The posterior distribution when using a \\(\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\) prior is then by Bayes’ theorem \\[\np(\\theta\\vert x_{1},\\ldots,x_n) \\propto\n(1-\\theta)^{\\sum_{i=1}^n x_i} \\theta^{n} \\cdot \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n=\\theta^{\\alpha + n-1}(1-\\theta)^{\\beta + \\sum_{i=1}^n x_i-1}\n\\] which is proportional to the \\(\\mathrm{Beta}(\\alpha+n,\\beta+\\sum_{i=1}^n x_i)\\) distribution. Since the posterior is in the same Beta family as the prior, the prior \\(\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\) is a conjugate prior to the geometric model."
  },
  {
    "objectID": "exercises/ch2/template.html",
    "href": "exercises/ch2/template.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 3.1\nLet \\(X_1,\\ldots,X_n \\mid \\theta,\\sigma^2 \\overset{\\mathrm{iid}}{\\sim} \\mathrm{N}(\\theta,\\sigma^2)\\), where \\(\\theta\\) is assumed known. Show that the \\(\\mathrm{Inv-}\\chi^2\\) distribution is a conjugate prior for \\(\\sigma^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTBD"
  },
  {
    "objectID": "exercises/ch2/truncated_normal.html",
    "href": "exercises/ch2/truncated_normal.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.5\na) Let \\(x_{1},\\ldots,x_{10}\\) be a sample with mean \\(\\bar{x}=1.873\\). Assume the model \\(X_1,\\ldots,X_n \\overset{\\mathrm{iid}}{\\sim} N(\\theta,1)\\). Use the prior \\(\\theta \\sim N(0,5)\\). Note that the second parameter of the normal distribution is a variance, not a standard deviation. Compute the posterior distribution of \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\(n=10,\\bar{x}=1.873,\\sigma^{2}=1,\\mu_{0}=0,\\tau_{0}^{2}=5\\). The posterior is normal with \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{1}}{\\frac{10}{1}+\\frac{1}{5}}=\\frac{50}{51}\\approx0.98039 \\\\\n      \\mu_{n}   &= \\frac{50}{51}\\cdot1.873+\\frac{1}{51}\\cdot0=1.8363 \\\\\n      \\tau_{n}^{2}  &= \\left(\\frac{10}{1}+\\frac{1}{5}\\right)^{-1}=\\frac{5}{51}.\n    \\end{align*}\\]\n\n\n\nb) You now get hold of a second sample \\(Y_1,\\ldots,Y_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta ,2)\\), where \\(\\theta\\) is the same quantity as in (a) but the measurements have a larger variance. The sample mean in this second sample is \\(\\bar{y}=0.582\\). Compute the posterior distribution of \\(\\theta\\) using both samples (the \\(x\\)’s and the \\(y\\)’s) under the assumption that the two samples are independent.\n\\(\\textit{Hint}\\): batch learning.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe easiest way to do this is use the posterior from the first sample as a prior for the second sample. That is, for this second sample we use the prior \\[\\begin{equation*}\n      \\theta\\sim N\\left(1.836,\\frac{5}{51}\\right),\n    \\end{equation*}\\] which gives the posterior \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{2}}{\\frac{10}{2}+\\frac{1}{5/51}}=\\frac{25}{76} \\\\\n\\mu_{n} &= \\frac{25}{76}\\cdot0.582+\\left(1-\\frac{25}{76}\\right)\\cdot1.836=1.4237\\\\\n\\tau_{n}^{2}    &= \\left(\\frac{10}{2}+\\frac{5}{51}\\right)^{-1}=\\frac{5}{76}.\n    \\end{align*}\\]\n\n\n\nc) You finally obtain a third sample \\(Z_{1},\\ldots,Z_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta,3)\\), with mean \\(\\bar{z}=1.221\\). Unfortunately, the measuring device for this latter sample was defective and any measurement above \\(3\\) was recorded as exactly \\(3\\). There were two such measurements. Give an expression for the unnormalized posterior distribution (likelihood times prior) for \\(\\theta\\) based on all three samples (\\(x,y\\) and \\(z\\)). Plot this unnormalized posterior over a grid of \\(\\theta\\) values.\n\\(\\textit{Hint}\\): the posterior distribution is not normal anymore when the measurements are truncated at \\(3\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet us do as before, using the posterior from the first two samples (obtained in problem b) above) as the prior. The prior is therefore \\(\\theta\\sim N\\left(1.4237,\\frac{5}{76}\\right)\\).\nLet us first use these eight observations to update the posterior, and then add the information from the two measurements that where truncated at \\(3\\). The mean of the eight measurements which were correctly recorded is \\(1.221\\cdot10-3\\cdot2=0.77625\\). The eight correctly recorded observations gives the following updating of the \\(N\\left(1.4237,\\frac{5}{76}\\right)\\) prior: \\[\\begin{align*}\n      w &=\\frac{\\frac{8}{3}}{\\frac{8}{3}+\\frac{1}{5/76}}=\\frac{10}{67} \\\\\n      \\mu_{n}   &=\\frac{10}{67}\\cdot0.77625+\\left(1-\\frac{10}{67}\\right)\\cdot1.4237=1.3271 \\\\\n      \\tau_{n}^{2} &=\\left(\\frac{8}{3}+\\frac{1}{5/76}\\right)^{-1}=\\frac{15}{268}=0.05597.\n    \\end{align*}\\] Note that most of the weight is now given to the prior, i.e. the posterior after the updates in (a) and (b). This is reasonable since the \\(8\\) new observations have relatively large variance, \\(\\sigma^{2}=3\\).\nWe are now ready for the final piece of information: the two truncated observations. We do not know their exact values, but we know that they were equal to or larger than \\(3\\). This is important information which we cannot ignore. The likelihood of these two observations (let’s call them \\(z_{1}\\) and \\(z_{2}\\)) is \\[\\begin{align*}\n      p(z_{1},z_{2}\\vert\\theta) &=\\mathrm{Pr}(z_{1}\\geq3\\vert\\theta)\\mathrm{Pr}(z_{2}\\geq3\\vert\\theta) \\\\\n      &=\\Big(1-\\mathrm{Pr}(z_{1}\\leq 3\\vert\\theta)\\Big) \\Big(1-\\mathrm{Pr}(z_{2}\\leq3\\vert\\theta)\\Big) \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]  \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\n    \\end{align*}\\] where \\(\\Phi(\\cdot)\\) is the CDF of the \\(\\mathrm{N}(0,1)\\) distribution. The posterior based on all \\(30\\) data points is now obtained by multiplying this likelihood with the prior at this stage, that is the posterior based on the first \\(28\\) correctly recorded data: \\(N(1.3271,0.05597)\\). The posterior is therefore proportional to \\[\\begin{equation*}\n      \\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\\exp\\left[-\\frac{1}{2\\cdot0.05597}\\left(\\mu-1.3271\\right)^{2}\\right].\n    \\end{equation*}\\]\nThe code below plots the prior (which is the posterior based on the \\(28\\) correctly recorded observations), the likelihood from the two truncated observations and the posterior based on all \\(30\\) data points. Note the form of the likelihood function from the two truncated observations: the probability of observing them increases monotonically with \\(\\theta\\) since all we know about these observations is that they are larger or equal to \\(3\\).\n\ntheta_grid = seq(0.5, 2.5, length = 1000)\nbin_width = theta_grid[2] - theta_grid[1]\nlike = (1 - pnorm(3, theta_grid, sqrt(3)))^2 \nprior = dnorm(theta_grid, 1.3271, sqrt(0.05597))\npost = like * prior # unnormalized\npost = post / sum(post * bin_width)\nlike = like / sum(like * bin_width)\nplot(theta_grid, prior, type = \"l\", col = \"orange\", lwd = 3, xlab = expression(theta), ylab = \"density\")\nlines(theta_grid, like, col = \"steelblue\", lwd = 3)\nlines(theta_grid, post, col = \"indianred\", lwd = 3)\nlegend(\"topleft\", legend = c(\"Prior\", \"Likelihood\", \"Posterior\"),\n       col = c(\"orange\", \"steelblue\", \"indianred\"), lwd = 3, bty = \"n\")"
  },
  {
    "objectID": "exercises/ch10/survival_weibullreg_stan.html",
    "href": "exercises/ch10/survival_weibullreg_stan.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 10.2\nExercise plotted the posterior for the parameter \\(\\lambda\\) in the Weibull model \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] for different fixed values of \\(k\\). This exercise asks you to implement the same model in to sample from the posterior distribution of \\(\\lambda\\) for a given \\(k=3/2\\). The describes how to implement censoring in the model. The example in the User Guide has the same censoring point for all patients, which is not the case in the dataset. So you need to generalize that to a vector of censoring points, one for each patient.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(rstan)\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet data and set up prior hyperparameters\n\nlibrary(survival) # loads the lung cancer data as `lung`\nlung &lt;- lung %&gt;% select(c(\"time\", \"status\", \"age\", \"sex\", \"ph.ecog\")) %&gt;% drop_na()\ny = lung$time\nX = cbind(1, lung$age, lung$sex == 2, lung$ph.ecog) # sex = 1 is female\np = dim(X)[2]\ncensored = (lung$status == 1)\ny_obs = y[-censored]\ny_cens = y[censored]\nX_obs = X[-censored,]\nX_cens = X[censored,]\nmu &lt;- rep(0,p)  # beta ~ N(mu, tau^2*I)\ntau &lt;- 100    \nmu_k &lt;- 0       # k ~ LogNormal(mu_k, sigma_k^2)\nsigma_k &lt;- 2\n\nSet up the stan model for the Weibull regression.\n\nweibull_survivalreg &lt;- '\ndata {\n\n  // Data\n  int&lt;lower=0&gt; N_obs;\n  int&lt;lower=0&gt; N_cens;\n  int&lt;lower=1&gt; p;\n  array[N_obs] real y_obs;\n  array[N_cens] real y_cens;\n  matrix[N_obs,p] X_obs;\n  matrix[N_cens,p] X_cens;\n  \n  // Prior hyperparameters k ~ LogNormal(mu_k, sigma_k) and beta_ ~ N(0, tau^2*I)\n  real mu_k;\n  real&lt;lower=0&gt; sigma_k;\n  real&lt;lower = 0&gt; tau;\n}\nparameters {\n  vector[p] beta_;\n  real&lt;lower=0&gt; k;\n}\nmodel {\n  k ~ lognormal(mu_k, sigma_k);    // specifies the prior\n  beta_ ~ normal(0, tau);\n  y_obs ~ weibull(k, exp(X_obs * beta_));  // add the observed (non-censored) data\n  target += weibull_lccdf(y_cens | k, exp(X_cens * beta_)); // add censored. \n}\n'\n\nWe set up the data and prior lists that will be supplied to stan:\n\ndata &lt;- list(p = dim(X_obs)[2], N_obs = length(y_obs), N_cens = length(y_cens), \n             y_obs = y_obs, y_cens = y_cens, X_obs = X_obs, X_cens = X_cens)\n\nprior &lt;- list(tau = tau, mu_k = mu_k, sigma_k = sigma_k)\n\nLoad rstan and set some options\n\n#install.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', \n#                                    getOption(\"repos\")))\nsuppressMessages(library(rstan))\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nSample from the posterior distribution using HMC in stan\n\nnDraws = 5000\nfit = stan(model_code = weibull_survivalreg, data = c(data, prior), iter = nDraws)\n\n\ns &lt;- summary(fit, pars = c(\"beta_\", \"k\"), probs = c(0.025, 0.975))\ns$summary  # results from all the different runs (chains) merged.\n\n                 mean      se_mean          sd        2.5%        97.5%\nbeta_[1]  6.329758116 5.153144e-03 0.326453913  5.68926444  6.984666563\nbeta_[2] -0.002885742 8.361374e-05 0.005278519 -0.01349017  0.007382942\nbeta_[3]  0.236152063 1.097192e-03 0.094758759  0.05419602  0.425781024\nbeta_[4] -0.260043308 8.411990e-04 0.069228094 -0.39386009 -0.126329485\nk         1.450195141 8.547994e-04 0.075036484  1.30723375  1.600197785\n            n_eff      Rhat\nbeta_[1] 4013.277 1.0009799\nbeta_[2] 3985.373 1.0009686\nbeta_[3] 7458.884 0.9998745\nbeta_[4] 6772.784 1.0003502\nk        7705.778 1.0001429\n\n\nPlotting the marginal posterior of \\(k\\)\n\n# Plot histogram from stan draws\npostsamples &lt;- extract(fit, pars = c(\"beta_\",\"k\"))\nhist(postsamples$k, 50, freq = FALSE, col = colors[5], \n     xlab = expression(k), ylab = \"posterior density\", \n     main = expression(k))\n\n\n\n\n\n\n\n\nPlotting the marginal posteriors for each \\(\\beta\\) coefficient\n\nvarNames = c(\"intercept\", \"age\", \"sex\", \"ph.ecog\")\npar(mfrow = c(2,2))\nfor (j in 1:p){\n  hist(postsamples$beta_[,j], 50, col = colors[6], \n       xlab = expression(beta), ylab = \"density\", main = varNames[j])\n}"
  },
  {
    "objectID": "exercises/ch10/survival_weibull_stan.html",
    "href": "exercises/ch10/survival_weibull_stan.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 10.1\nThis exercise extends the analysis of the Weibull model for survival times of lung cancer patients in Exercise to the regression setting. The survival times are here modelled as independent Weibull distributed with a scale parameter \\(\\lambda\\) that is a function of covariates, i.e. using a Weibull regression model. The response variable time is denoted by \\(y\\) and is modelled as a function of the three covariates age, sex and ph.ecog (ECOG performance score). The model for patient \\(i\\) is: \\[\ny_i \\vert \\mathbf{x}_i, \\boldsymbol{\\beta}, k \\overset{\\mathrm{ind}}{\\sim} \\mathrm{Weibull}\\big(\\lambda_i = \\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\beta}),k\\big).\n\\] where \\(\\boldsymbol{\\beta}\\) is the vector with regression coefficients. Note that by the properties of the Weibull distribution, the conditional mean in this model is \\(\\mathbb{E}(y \\vert \\mathbf{x}_i) = \\lambda_i\\Gamma(1+1/k)\\), so the regression coefficients do not quite have the usual interpretation of the effect on the conditional mean. The three covariates are placed in a \\(n\\times p\\) matrix \\(\\mathbf{X}\\) with the first column being one for all observations to model the intercept. Use a multivariate normal prior for \\(\\boldsymbol{\\beta} \\sim N(\\mathbf{0},\\tau^2\\mathbf{I}_p)\\) with the non-informative choice \\(\\tau = 100\\), and the prior \\(\\ k \\sim \\mathrm{logNormal}(0,2^2)\\). Remove the patients with missing values in the selected covariates.\nSample from the posterior distribution \\(p(\\boldsymbol{\\beta}, k \\vert \\mathbf{y}, \\mathbf{X})\\) using HMC in stan. Plot the marginal posterior for \\(k\\) and the marginal posteriors of each regression coefficient.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(rstan)\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nThe code below defines the iid Weibull survival model with censored data in stan. The code here extends this example in the Stan User Guide to the case with different censoring points for each patient. Note the target += construction where the censored data points are added to the target (the log posterior) after the initial uncensored (observed) data are included in the log posterior with the y_obs ~ weibull(k, lambda) statement. The weibull_lccdf function in stan is a convenience function that computes the survival probability \\(\\mathrm{Pr}(X &gt;= x) = 1 - F(x)\\), where \\(F()\\) is the cdf of the Weibull distribution. There are _lccdf versions of all distribution in stan.\n\nweibull_survivalmodel &lt;- '\ndata {\n\n  // Data\n  int&lt;lower=0&gt; N_obs;\n  int&lt;lower=0&gt; N_cens;\n  array[N_obs] real y_obs;\n  array[N_cens] real y_cens;\n  \n  // Model setting\n  real&lt;lower=0&gt; k;\n  \n  // Prior hyperparameters theta ~ Gamma(alpha, beta)\n  real&lt;lower=0&gt; alpha;\n  real&lt;lower=0&gt; beta;\n}\nparameters {\n  real lambda;\n}\nmodel {\n  lambda ~ gamma(alpha, beta); // specifies the prior\n  y_obs ~ weibull(k, lambda);  // add the observed (non-censored) data\n  target += weibull_lccdf(y_cens | k, lambda); // add censored. lccdf is 1-cdf\n}\n'\n\nWe set up the data and prior lists that will be supplied to stan:\n\nlibrary(survival) # loads the lung cancer data as `lung`\nk = 3/2\ny_obs &lt;- lung %&gt;% filter(status == 2) %&gt;% pull(time)\ny_cens &lt;- lung %&gt;% filter(status == 1) %&gt;% pull(time)\n\ndata &lt;- list(N_obs = length(y_obs), N_cens = length(y_cens), \n             y_obs = y_obs, y_cens = y_cens, k = k)\n\nalpha_prior &lt;- 3     # shape parameter\nbeta_prior &lt;- 1/50   # rate parameter\nprior &lt;- list(alpha = alpha_prior, beta = beta_prior)\n\nLoad rstan and set some options\n\n#install.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', \n#                                    getOption(\"repos\")))\nsuppressMessages(library(rstan))\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nSample from the posterior distribution using HMC in stan\n\nnDraws = 5000\nfit = stan(model_code = weibull_survivalmodel, data = c(data, prior), iter = nDraws)"
  },
  {
    "objectID": "exercises/ch2/iid_weibull_survival_all.html",
    "href": "exercises/ch2/iid_weibull_survival_all.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.10\nExercise modelled the survival times of uncensored lung cancer patients with an iid exponential model. Here we extend that analysis to the more general Weibull distribution: \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] The value of \\(k\\) determines how the failure rate changes with time:\nPlot the posterior distribution of \\(\\lambda\\) conditional on \\(k=1\\), \\(k=3/2\\) and \\(k=2\\). For all \\(k\\), use the prior \\(\\lambda \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) with \\(\\alpha=3\\) and \\(\\beta=1/50\\) (which is a similar prior for \\(\\theta=1/\\lambda\\) as in Exercise 2.2). Plot the time variable as a histogram and overlay the fitted model for the three different \\(k\\)-values; use the posterior mode for \\(\\theta\\) in each model when plotting the fitted model density.\n\\(\\textit{Hint}\\): the posterior distribution for \\(k\\neq 1\\) is intractable, so use numerical evaluation of the posterior over a grid of \\(\\lambda\\)-values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood can be computed with separate treatment of the uncensored and censored observations: \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\lambda, k) & = \\prod_{i=1}^n p(x_i \\vert \\lambda, k) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\lambda, k) \\prod_{c \\in \\mathcal{C}} \\Big(1 - F(x_c \\vert \\lambda, k)\\Big)\n\\end{align}\n\\] where \\(p(x \\vert \\lambda, k)\\) is the pdf of a Weibull variable \\[\np(x \\vert \\lambda, k) = \\frac{k}{\\lambda}\\Big( \\frac{x}{\\lambda} \\Big)^{k-1}e^{-(x/\\lambda)^k}\\quad\\text{ for }x&gt;0\n\\] which is implemented in R as dweibull. The cdf of the Weibull distribution is of rather simple form \\[\nF(x \\vert \\lambda, k) = 1 - e^{-(x/\\lambda)^k}\n\\] and is implemented in R as pweibull.\nThe code below plots the prior and posterior distribution for \\(\\lambda\\) for the three different \\(k\\)-values. We could have inserted the mathematical expressions for the pdf and cdf and simplified the final likelihood expression; we will instead use the dweibull and pweibull functions without simplifications since it gives a more general template that can be used for any distribution, not just the Weibull model. For numerical stability we usually compute the posterior distribution on the log scale \\[\n\\log p(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j)\n\\] for a grid of equally spaced \\(\\lambda\\)-values: \\(\\lambda^{(1)}\\ldots,\\lambda^{(J)}\\). The \\(\\propto\\) sign now means that there is a missing additive constant \\(\\log p(x_1,\\ldots,x_n)\\) which does not depend on the unknown parameter \\(\\lambda\\). When we have computed \\(\\log p(\\lambda \\vert x_1,\\ldots,x_n)\\) over a grid of \\(\\lambda\\) values we compute the posterior on the original scale by \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j) \\Big)\n\\] and then divide all numbers with the normalizing constant to make sure that the posterior integrates to one. This is done numerically by approximating the integral by a Riemann rectangle sum \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) =\n\\frac{\\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(j)}) + \\log p(\\lambda^{(j)}) \\Big)}\n{\\sum_{h=1}^J \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(h)}) + \\log p(\\lambda^{(h)}) \\Big) \\Delta}\n\\] where \\(\\Delta\\) is the spacing between the grid points of \\(\\lambda\\)-values: \\(\\lambda^{(1)}, \\ldots, \\lambda^{(J)}\\).\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet up prior hyperparameters\n\nalpha_prior &lt;- 3     # shape parameter\nbeta_prior &lt;- 1/50   # rate parameter\n\nSet up function that computes the likelihood for any \\(\\lambda\\) value:\n\n# Make a function that computes the likelihood\nweibull_loglike &lt;- function(lambda, x, censored, k){\n  loglik_uncensored = sum(dweibull(x[-censored], shape = k, scale = lambda, \n                                   log = TRUE))\n  loglik_censored = sum(pweibull(x[censored], shape = k, scale = lambda, \n                                 lower.tail = FALSE, log.p = TRUE))\n  return(loglik_uncensored + loglik_censored)\n}\n\nSet up a function that computes the posterior density over a grid of \\(\\lambda\\):\n\nweibull_posterior &lt;- function(lambdaGrid, x, censored, k, alpha_prior, beta_prior){\n  Delta = lambdaGrid[2] - lambdaGrid[1] # Grid step size\n  logPrior &lt;- dgamma(lambdaGrid, shape = alpha_prior, rate = beta_prior, log = TRUE)\n  logLike &lt;- sapply(lambdaGrid, weibull_loglike, x, censored, k)\n  logPost &lt;- logLike + logPrior\n  logPost &lt;- logPost - max(logPost) # subtract constant to avoid overflow\n  post &lt;- exp(logPost)/(sum(exp(logPost))*Delta) # original scale and normalize\n  logLike &lt;- logLike - max(logLike)\n  likeNorm &lt;- exp(logLike)/(sum(exp(logLike))*Delta) # normalized likelihood\n  return(list(post = post, prior = exp(logPrior), likeNorm = likeNorm))\n}\n\n\n# Plot the prior and posterior densities\n\nlambdaGrid &lt;- seq(200, 800, length.out = 1000)\n# Compute to get the prior\npostRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k = 1, \n                             alpha_prior, beta_prior)\ndf &lt;- data.frame(\n  lambdaGrid = lambdaGrid, \n  prior = postRes$prior\n)\n\n# Compute for all selected k values\npostModes = c()\nfor (k in c(1, 3/2, 2)){\n  postRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k, alpha_prior, beta_prior)\n  df[str_glue(\"posterior k={k}\")] &lt;- postRes$post\n  postModes = c(postModes, lambdaGrid[which.max(postRes$post)])\n}\n\ndf_long &lt;- df %&gt;% pivot_longer(-lambdaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = lambdaGrid, y = density, color = density_type) +\n  geom_line() +\n  xlim(250,600) +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior k=1\", \"posterior k=1.5\", \"posterior k=2\"), \n    values = c(colors[2], colors[1], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.3\", x = expression(lambda), y = \"Density\", color = \"\") + \n  theme_minimal()\n\nWarning: Removed 1668 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe fit of the three Weibull models are plotted below. The best fit seems to be for \\(k=3/2\\), but it is still not very good. In a later exercise you will be asked to freely estimate both \\(\\lambda\\) and \\(k\\), and even later to fit a Weibull regression model with covariates.\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dweibull, args = list(shape = 1, scale = postModes[1]), lwd = 1, \n                aes(color = \"Weibull fit k = 1\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 3/2, scale = postModes[2]), lwd = 1, \n                aes(color = \"Weibull fit k = 3/2\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 2, scale = postModes[3]), lwd = 1, \n                aes(color = \"Weibull fit k = 2\"),\n  ) +\n  labs(title = \"Weibull model fits\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[6]) +\n  scale_color_manual(\"\", values = c(colors[1], colors[3], colors[4])) +\n  theme_minimal()"
  },
  {
    "objectID": "exercises/ch2/normal_in_expfamily.html",
    "href": "exercises/ch2/normal_in_expfamily.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.10\nShow that the \\(N(\\mu,1)\\) distribution belongs to the exponential family.\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch2/iid_normal_derive.html",
    "href": "exercises/ch2/iid_normal_derive.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.6\nDerive the posterior distribution for the normal model with a normal prior.\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch2/iid_uniform_conj.html",
    "href": "exercises/ch2/iid_uniform_conj.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.11\nLet \\[\nX_1,\\ldots,X_n \\overset{iid}{\\sim } \\mathrm{Uniform}(0,\\theta).\n\\] Show that the Pareto prior \\(\\theta \\sim \\mathrm{Pareto}(\\alpha, \\beta)\\), is conjugate to the Uniform model by deriving the posterior distribution. See Box~ for a definition of the Pareto distribution and properties; note the support of the Pareto distribution.\n\\(\\textit{Hint}\\): Do not forget to include an indicator function when you write up the likelihood function since the \\(\\mathrm{Uniform}(0,\\theta)\\) distribution is zero for outcomes larger than \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe pdf of the \\(\\mathrm{Uniform}(0,\\theta)\\) distribution is \\[\\begin{equation*}\n  p(x) = \\begin{cases}\n          \\frac{1}{\\theta}  & \\text{if } x \\leq  \\theta \\\\\n          0                 & \\text{otherwise }\n          \\end{cases}\n\\end{equation*}\\] which can be written with an indicator function as \\[\np(x) = \\frac{1}{\\theta} I(x_i \\leq \\theta).\n\\] The likelihood function is therefore of the form \\[\\begin{equation*}\n   p(x_1,\\ldots,x_n\\vert\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} I(x_i \\leq \\theta) = \\frac{1}{\\theta^n} \\prod_{i=1}^n I(x_i \\leq \\theta).\n\\end{equation*}\\] The factor \\(\\prod_{i=1}^n I(x_i \\leq \\theta)\\) is only non-zero if all \\(x_1,\\ldots,x_n\\) are smaller or equal to \\(\\theta\\), i.e. if \\(x_{\\mathrm{max}} := \\mathrm{max}(x_1, \\dots,x_n)\\) is smaller or equal to \\(\\theta\\). We can therefore write the likelihood as \\[\\begin{equation*}\n   p(x_1,\\ldots,x_n\\vert\\theta) = \\frac{1}{\\theta^n}  I(x_{\\mathrm{max}} \\leq \\theta).\n\\end{equation*}\\]\nThe Pareto prior can also be written with an indicator function \\[\\begin{align*}\n    p(\\theta) = \\frac{\\alpha \\beta^\\alpha }{\\theta^{\\alpha+1}}\\cdot I(\\beta \\leq \\theta),\n\\end{align*}\\] to explicitly include that \\(p(\\theta)=0\\) if \\(\\theta &lt; \\beta\\).\nBy Bayes’ theorem, the posterior is then \\[\\begin{align*}\n    p(\\theta \\vert x_1, \\dots,x_n) &\\propto p(x_1, \\dots,x_n \\vert \\theta)p(\\theta) \\\\\n                       &= \\frac{1}{\\theta^n} I(x_{\\mathrm{max}} \\leq \\theta)\n                            \\frac{\\alpha \\beta^\\alpha }{\\theta^{\\alpha+1}}\\cdot I(\\beta \\leq \\theta) \\\\\n                       &=  \\frac{\\alpha \\beta^\\alpha }{\\theta^{n+\\alpha+1}} \\cdot  \n                            I\\big(\\tilde \\beta \\leq \\theta \\big)        \n\\end{align*}\\] where \\(\\tilde \\beta = \\mathrm{max}(x_{\\mathrm{max}},\\beta)\\). This is proportional to a \\(\\mathrm{Pareto}\\big(\\alpha + n, \\tilde \\beta \\big)\\) density. Since the prior and posterior are both in the Pareto family, the Pareto prior is conjugate to the \\(\\mathrm{Uniform}(0,\\theta)\\) model."
  },
  {
    "objectID": "exercises/ch2/censored_normal.html",
    "href": "exercises/ch2/censored_normal.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.5\na) Let \\(x_{1},\\ldots,x_{10}\\) be a sample with mean \\(\\bar{x}=1.873\\). Assume the model \\(X_1,\\ldots,X_n \\overset{\\mathrm{iid}}{\\sim} N(\\theta,1)\\). Use the prior \\(\\theta \\sim N(0,5)\\). Note that the second parameter of the normal distribution is a variance, not a standard deviation. Compute the posterior distribution of \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\(n=10,\\bar{x}=1.873,\\sigma^{2}=1,\\mu_{0}=0,\\tau_{0}^{2}=5\\). The posterior is normal with \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{1}}{\\frac{10}{1}+\\frac{1}{5}}=\\frac{50}{51}\\approx0.98039 \\\\\n      \\mu_{n}   &= \\frac{50}{51}\\cdot1.873+\\frac{1}{51}\\cdot0=1.8363 \\\\\n      \\tau_{n}^{2}  &= \\left(\\frac{10}{1}+\\frac{1}{5}\\right)^{-1}=\\frac{5}{51}.\n    \\end{align*}\\]\n\n\n\nb) You now get hold of a second sample \\(Y_1,\\ldots,Y_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta ,2)\\), where \\(\\theta\\) is the same quantity as in (a) but the measurements have a larger variance. The sample mean in this second sample is \\(\\bar{y}=0.582\\). Compute the posterior distribution of \\(\\theta\\) using both samples (the \\(x\\)’s and the \\(y\\)’s) under the assumption that the two samples are independent.\n\\(\\textit{Hint}\\): batch learning.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe easiest way to do this is use the posterior from the first sample as a prior for the second sample. That is, for this second sample we use the prior \\[\\begin{equation*}\n      \\theta\\sim N\\left(1.836,\\frac{5}{51}\\right),\n    \\end{equation*}\\] which gives the posterior \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{2}}{\\frac{10}{2}+\\frac{1}{5/51}}=\\frac{25}{76} \\\\\n\\mu_{n} &= \\frac{25}{76}\\cdot0.582+\\left(1-\\frac{25}{76}\\right)\\cdot1.836=1.4237\\\\\n\\tau_{n}^{2}    &= \\left(\\frac{10}{2}+\\frac{1}{5/51}\\right)^{-1}=\\frac{5}{76}.\n    \\end{align*}\\]\n\n\n\nc) You finally obtain a third sample \\(Z_{1},\\ldots,Z_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta,3)\\), with mean \\(\\bar{z}=1.221\\). Unfortunately, the measuring device for this latter sample was defective and any measurement above \\(3\\) was recorded as exactly \\(3\\). There were two such measurements. Give an expression for the unnormalized posterior distribution (likelihood times prior) for \\(\\theta\\) based on all three samples (\\(x,y\\) and \\(z\\)). Plot this unnormalized posterior over a grid of \\(\\theta\\) values.\n\\(\\textit{Hint}\\): the posterior distribution is not normal anymore when the measurements are censored at \\(3\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet us do as before, using the posterior from the first two samples (obtained in problem b) above) as the prior. The prior is therefore \\(\\theta\\sim N\\left(1.4237,\\frac{5}{76}\\right)\\).\nLet us first use these eight observations to update the posterior, and then afterwards add the information from the two measurements that where censored at \\(3\\). The mean of the eight measurements which were correctly recorded is \\(\\frac{1.221\\cdot10-3\\cdot 2}{8} = 0.77625\\). The eight correctly recorded observations gives the following updating of the \\(N\\left(1.4237,\\frac{5}{76}\\right)\\) prior: \\[\\begin{align*}\n      w &=\\frac{\\frac{8}{3}}{\\frac{8}{3}+\\frac{1}{5/76}}=\\frac{10}{67} \\\\\n      \\mu_{n}   &=\\frac{10}{67}\\cdot0.77625+\\left(1-\\frac{10}{67}\\right)\\cdot1.4237=1.3271 \\\\\n      \\tau_{n}^{2} &=\\left(\\frac{8}{3}+\\frac{1}{5/76}\\right)^{-1}=\\frac{15}{268}=0.05597.\n    \\end{align*}\\] Note that most of the weight is now given to the prior, i.e. the posterior after the updates in (a) and (b). This is reasonable since the \\(8\\) new observations have a relatively large variance, \\(\\sigma^{2}=3\\), and the prior at this step is based on \\(20\\) previous observations.\nWe are now ready for the final piece of information: the two censored observations. We do not know their exact values, but we know that they were equal to or larger than \\(3\\). This is important information which we cannot ignore. The likelihood from these two observations (let’s call them \\(z_{1}\\) and \\(z_{2}\\)) is \\[\\begin{align*}\n      p(z_{1},z_{2}\\vert\\theta) &=\\mathrm{Pr}(z_{1}\\geq3\\vert\\theta)\\mathrm{Pr}(z_{2}\\geq3\\vert\\theta) \\\\\n      &=\\Big(1-\\mathrm{Pr}(z_{1}\\leq 3\\vert\\theta)\\Big) \\Big(1-\\mathrm{Pr}(z_{2}\\leq3\\vert\\theta)\\Big) \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]  \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\n    \\end{align*}\\] where \\(\\Phi(\\cdot)\\) is the CDF of the \\(\\mathrm{N}(0,1)\\) distribution. The posterior based on all \\(30\\) data points is now obtained by multiplying this likelihood with the prior at this stage, that is the posterior based on the first \\(28\\) correctly recorded data: \\(N(1.3271,0.05597)\\). The posterior is therefore proportional to \\[\\begin{equation*}\n      \\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\\exp\\left[-\\frac{1}{2\\cdot0.05597}\\left(\\mu-1.3271\\right)^{2}\\right].\n    \\end{equation*}\\]\nThe code below plots the prior (which is the posterior based on the \\(28\\) correctly recorded observations), the (normalized) likelihood from the two censored observations and the posterior based on all \\(30\\) data points. Note the form of the likelihood function from the two censored observations: the probability of observing them increases monotonically with \\(\\theta\\) since all we know about these observations is that they are larger or equal to \\(3\\).\n\ntheta_grid = seq(0.5, 2.5, length = 1000)\nbin_width = theta_grid[2] - theta_grid[1]\nlike = (1 - pnorm(3, theta_grid, sqrt(3)))^2 \nprior = dnorm(theta_grid, 1.3271, sqrt(0.05597))\npost = like * prior # unnormalized\npost = post / sum(post * bin_width)\nlike = like / sum(like * bin_width)\nplot(theta_grid, prior, type = \"l\", col = \"orange\", lwd = 3, xlab = expression(theta), ylab = \"density\")\nlines(theta_grid, like, col = \"steelblue\", lwd = 3)\nlines(theta_grid, post, col = \"indianred\", lwd = 3)\nlegend(\"topleft\", legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),\n       col = c(\"orange\", \"steelblue\", \"indianred\"), lwd = 3, bty = \"n\")"
  },
  {
    "objectID": "exercises/ch2/iid_exp_conj.html#exercise-2.1",
    "href": "exercises/ch2/iid_exp_conj.html#exercise-2.1",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Let \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\) be iid exponentially distributed data. Show that the Gamma distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood from an iid sample from \\(\\mathrm{Expon}(\\theta)\\) is \\[\np(x_1,\\ldots,x_n \\vert \\theta)= \\prod_{i=1}^n p(x_i \\vert \\theta) =\n  \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\n\\] The density of the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) prior is \\[\np(\\theta) =  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\theta^{\\alpha-1}e^{-\\beta\\theta}\n             \\propto \\theta^{\\alpha-1}e^{-\\beta\\theta}\n\\]\nBy Bayes’ theorem, the posterior distribution is \\[\n\\begin{align}\n  p(\\theta \\vert x_1,\\ldots,x_n) &\\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta)   \\\\\n& \\propto \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\\theta^{\\alpha-1}e^{-\\beta\\theta}  \\\\\n& =  \\theta^{\\alpha + n - 1} e^{ -\\theta(\\beta + \\sum_{i=1}^n x_i)},\n\\end{align}\n\\] which can be recognized as proportional to the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha +n,\\beta + \\sum\\nolimits_{i=1}^n x_i)\\) distribution. Since the prior and posterior belongs to the same (Gamma) distributional family, the Gamma prior is indeed conjugate to the exponential likelihood."
  },
  {
    "objectID": "exercises/ch2solutions.html#exercise-2.1",
    "href": "exercises/ch2solutions.html#exercise-2.1",
    "title": "Chapter 2 - Single-parameter models: Exercise solutions",
    "section": "Exercise 2.1",
    "text": "Exercise 2.1\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\) be iid exponentially distributed data. Show that the Gamma distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood from an iid sample from \\(\\mathrm{Expon}(\\theta)\\) is \\[\np(x_1,\\ldots,x_n \\vert \\theta)= \\prod_{i=1}^n p(x_i \\vert \\theta) =\n  \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\n\\] The density of the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) prior is \\[\np(\\theta) =  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\theta^{\\alpha-1}e^{-\\beta\\theta}\n             \\propto \\theta^{\\alpha-1}e^{-\\beta\\theta}\n\\]\nBy Bayes’ theorem, the posterior distribution is \\[\n\\begin{align}\n  p(\\theta \\vert x_1,\\ldots,x_n) &\\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta)   \\\\\n& \\propto \\theta^n e^{-\\theta\\sum_{i=1}^n x_i}\\theta^{\\alpha-1}e^{-\\beta\\theta}  \\\\\n& =  \\theta^{\\alpha + n - 1} e^{ -\\theta(\\beta + \\sum_{i=1}^n x_i)},\n\\end{align}\n\\] which can be recognized as proportional to the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha +n,\\beta + \\sum\\nolimits_{i=1}^n x_i)\\) distribution. Since the prior and posterior belongs to the same (Gamma) distributional family, the Gamma prior is indeed conjugate to the exponential likelihood.\n\n\n\n\n\nExercise 2.2\nThe dataset \\(\\texttt{lung}\\) in the R package \\(\\texttt{survival}\\) contains data on 228 patients with advanced lung cancer. We will here analyze the survival time of the patient in days (\\(\\texttt{time}\\)). The variable \\(\\texttt{status}\\) is a binary variable with \\(\\texttt{status}=1\\) if the survival time of the patient is censored (patient was still alive at the end of the study) and \\(\\texttt{status}=2\\) if the survival time was uncensored (patient was dead before the end of the study).\nIn this exercise we will only analyze the uncensored patients; Exercise below asks you to analyze all patients. Assume that the survival times \\(X_1,\\ldots,X_n\\) of the uncensored patients are iid \\(\\mathrm{Expon}(\\theta)\\) distributed. Use the conjugate prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha=3,\\beta=300)\\), which can be shown to imply that the expected survival time \\(\\mathbb{E}(X \\vert \\theta) = 1/\\theta\\) for this population is around \\(200\\) days. Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom Exercise 2.1, we know that the posterior distribution is \\[\\theta \\sim \\mathrm{Gamma}(\\alpha + n_u, \\beta + \\sum\\nolimits_{u \\in \\mathcal{U}} x_u),\\] where \\(n_u\\) is the number of uncensored observations and \\(\\mathcal{U}\\) is the set of observation indices for the uncensored data.\nThe following code plots the prior, likelihood (normalized) and posterior over a grid of values for \\(\\theta\\). Note that the data is so much stronger than the prior that the posterior is virtually identical to the likelihood, which is why the normalized likelihood is not visible in the plot.\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\n\n# Summarize the data needed for the posterior, filter out censored data\ndata_summary &lt;- lung %&gt;% filter(status == 2) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters\nalpha_post &lt;- alpha_prior + data_summary$n  \nbeta_post &lt;- beta_prior + data_summary$sum_x   \n\n\n# Plot the prior and posterior densities, and the (normalized) likelihood \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nlikelihood_density &lt;- dgamma(thetaGrid, shape = data_summary$n, rate = data_summary$sum_x)\nposterior_density &lt;- dgamma(thetaGrid, shape = alpha_post, rate = beta_post)\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  likelihood = likelihood_density,\n  posterior = posterior_density\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"likelihood\", \"posterior\"), \n    values = c(colors[2], colors[1], colors[3])) +\n  labs(title = \"Survival lung cancer - uncensored patients\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.3\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Geom}(\\theta)\\) be iid from a geometric distribution with parameter \\(0&lt;\\theta&lt;1\\). Show that the Beta distribution is the conjugate prior for this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe geometric distribution has probability mass function \\[\np(x) = (1-\\theta)^{x}\\theta, \\quad \\text{ for }x=0,1,2,...\n\\] The likelihood from a sample of \\(n\\) observations is therefore \\[\np(x_{1},\\ldots,x_{n}\\vert\\theta) = \\prod_{i=1}^n p(x_i \\vert \\theta)= (1-\\theta)^{\\sum_{i=1}^n x_i}\\theta^{n}\n\\] The posterior distribution when using a \\(\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\) prior is then by Bayes’ theorem \\[\np(\\theta\\vert x_{1},\\ldots,x_n) \\propto\n(1-\\theta)^{\\sum_{i=1}^n x_i} \\theta^{n} \\cdot \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n=\\theta^{\\alpha + n-1}(1-\\theta)^{\\beta + \\sum_{i=1}^n x_i-1}\n\\] which is proportional to the \\(\\mathrm{Beta}(\\alpha+n,\\beta+\\sum_{i=1}^n x_i)\\) distribution. Since the posterior is in the same Beta family as the prior, the prior \\(\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)\\) is a conjugate prior to the geometric model.\n\n\n\n\n\n\nExercise 2.4\nLet \\(X_1,\\ldots,X_n\\) be an iid sample from a distribution with density function \\[\np(x) \\propto \\theta^2 x \\exp (-x\\theta)\\quad \\text{ for } x&gt;0 \\text{ and } \\theta&gt;0.\n\\] Find the conjugate prior for this distribution and derive the posterior distribution from an iid sample \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood function from a sample \\(x_1,\\ldots,x_n\\) is\n\\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\prod_{i=1}^n\\theta^2 x_i \\exp (-x_i\\theta) \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\n\\]\nThis likelihood resembles a Gamma distribution, so a good guess for a conjugate prior would be the \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) distribution; to see that this is indeed a reasonable guess, note that the particular form of the Gamma density (a power of \\(\\theta\\) times an exponential in \\(\\theta\\)) makes it closed under multiplication. The posterior distribution is then\n\\[\n\\begin{align}\np(\\theta|x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n      & \\propto \\theta^{2n}\\exp\\Big(-\\theta \\sum_{i=1}^n x_i \\Big)\\theta^{\\alpha-1}\\exp(-\\theta\\beta) \\\\\n      & \\propto \\theta^{\\alpha + 2n - 1}\\exp\\Big(-\\theta (\\beta+\\sum_{i=1}^n x_i) \\Big)\n\\end{align}\n\\] and the posterior is therefore \\(\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + 2n,\\beta + \\sum_{i=1}^n x_i)\\). Since the posterior belongs to the same (Gamma) family as the prior, the Gamma prior is indeed conjugate to this likelihood.\n\n\n\n\n\n\nExercise 2.5\na) Let \\(x_{1},\\ldots,x_{10}\\) be a sample with mean \\(\\bar{x}=1.873\\). Assume the model \\(X_1,\\ldots,X_n \\overset{\\mathrm{iid}}{\\sim} N(\\theta,1)\\). Use the prior \\(\\theta \\sim N(0,5)\\). Note that the second parameter of the normal distribution is a variance, not a standard deviation. Compute the posterior distribution of \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\(n=10,\\bar{x}=1.873,\\sigma^{2}=1,\\mu_{0}=0,\\tau_{0}^{2}=5\\). The posterior is normal with \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{1}}{\\frac{10}{1}+\\frac{1}{5}}=\\frac{50}{51}\\approx0.98039 \\\\\n      \\mu_{n}   &= \\frac{50}{51}\\cdot1.873+\\frac{1}{51}\\cdot0=1.8363 \\\\\n      \\tau_{n}^{2}  &= \\left(\\frac{10}{1}+\\frac{1}{5}\\right)^{-1}=\\frac{5}{51}.\n    \\end{align*}\\]\n\n\n\nb) You now get hold of a second sample \\(Y_1,\\ldots,Y_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta ,2)\\), where \\(\\theta\\) is the same quantity as in (a) but the measurements have a larger variance. The sample mean in this second sample is \\(\\bar{y}=0.582\\). Compute the posterior distribution of \\(\\theta\\) using both samples (the \\(x\\)’s and the \\(y\\)’s) under the assumption that the two samples are independent.\n\\(\\textit{Hint}\\): batch learning.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe easiest way to do this is use the posterior from the first sample as a prior for the second sample. That is, for this second sample we use the prior \\[\\begin{equation*}\n      \\theta\\sim N\\left(1.836,\\frac{5}{51}\\right),\n    \\end{equation*}\\] which gives the posterior \\[\\begin{align*}\n      w &= \\frac{\\frac{10}{2}}{\\frac{10}{2}+\\frac{1}{5/51}}=\\frac{25}{76} \\\\\n\\mu_{n} &= \\frac{25}{76}\\cdot0.582+\\left(1-\\frac{25}{76}\\right)\\cdot1.836=1.4237\\\\\n\\tau_{n}^{2}    &= \\left(\\frac{10}{2}+\\frac{1}{5/51}\\right)^{-1}=\\frac{5}{76}.\n    \\end{align*}\\]\n\n\n\nc) You finally obtain a third sample \\(Z_{1},\\ldots,Z_{10} | \\theta \\overset{\\mathrm{iid}}{\\sim} N(\\theta,3)\\), with mean \\(\\bar{z}=1.221\\). Unfortunately, the measuring device for this latter sample was defective and any measurement above \\(3\\) was recorded as exactly \\(3\\). There were two such measurements. Give an expression for the unnormalized posterior distribution (likelihood times prior) for \\(\\theta\\) based on all three samples (\\(x,y\\) and \\(z\\)). Plot this unnormalized posterior over a grid of \\(\\theta\\) values.\n\\(\\textit{Hint}\\): the posterior distribution is not normal anymore when the measurements are censored at \\(3\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet us do as before, using the posterior from the first two samples (obtained in problem b) above) as the prior. The prior is therefore \\(\\theta\\sim N\\left(1.4237,\\frac{5}{76}\\right)\\).\nLet us first use these eight observations to update the posterior, and then afterwards add the information from the two measurements that where censored at \\(3\\). The mean of the eight measurements which were correctly recorded is \\(\\frac{1.221\\cdot10-3\\cdot 2}{8} = 0.77625\\). The eight correctly recorded observations gives the following updating of the \\(N\\left(1.4237,\\frac{5}{76}\\right)\\) prior: \\[\\begin{align*}\n      w &=\\frac{\\frac{8}{3}}{\\frac{8}{3}+\\frac{1}{5/76}}=\\frac{10}{67} \\\\\n      \\mu_{n}   &=\\frac{10}{67}\\cdot0.77625+\\left(1-\\frac{10}{67}\\right)\\cdot1.4237=1.3271 \\\\\n      \\tau_{n}^{2} &=\\left(\\frac{8}{3}+\\frac{1}{5/76}\\right)^{-1}=\\frac{15}{268}=0.05597.\n    \\end{align*}\\] Note that most of the weight is now given to the prior, i.e. the posterior after the updates in (a) and (b). This is reasonable since the \\(8\\) new observations have a relatively large variance, \\(\\sigma^{2}=3\\), and the prior at this step is based on \\(20\\) previous observations.\nWe are now ready for the final piece of information: the two censored observations. We do not know their exact values, but we know that they were equal to or larger than \\(3\\). This is important information which we cannot ignore. The likelihood from these two observations (let’s call them \\(z_{1}\\) and \\(z_{2}\\)) is \\[\\begin{align*}\n      p(z_{1},z_{2}\\vert\\theta) &=\\mathrm{Pr}(z_{1}\\geq3\\vert\\theta)\\mathrm{Pr}(z_{2}\\geq3\\vert\\theta) \\\\\n      &=\\Big(1-\\mathrm{Pr}(z_{1}\\leq 3\\vert\\theta)\\Big) \\Big(1-\\mathrm{Pr}(z_{2}\\leq3\\vert\\theta)\\Big) \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]\\left[1-\\Phi\\left(\\frac{3-\\theta}{\\sqrt{3}}\\right)\\right]  \\\\\n      &=\\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\n    \\end{align*}\\] where \\(\\Phi(\\cdot)\\) is the CDF of the \\(\\mathrm{N}(0,1)\\) distribution. The posterior based on all \\(30\\) data points is now obtained by multiplying this likelihood with the prior at this stage, that is the posterior based on the first \\(28\\) correctly recorded data: \\(N(1.3271,0.05597)\\). The posterior is therefore proportional to \\[\\begin{equation*}\n      \\left[1-\\Phi\\left(\\frac{3-\\mu}{\\sqrt{3}}\\right)\\right]^{2}\\exp\\left[-\\frac{1}{2\\cdot0.05597}\\left(\\mu-1.3271\\right)^{2}\\right].\n    \\end{equation*}\\]\nThe code below plots the prior (which is the posterior based on the \\(28\\) correctly recorded observations), the (normalized) likelihood from the two censored observations and the posterior based on all \\(30\\) data points. Note the form of the likelihood function from the two censored observations: the probability of observing them increases monotonically with \\(\\theta\\) since all we know about these observations is that they are larger or equal to \\(3\\).\n\ntheta_grid = seq(0.5, 2.5, length = 1000)\nbin_width = theta_grid[2] - theta_grid[1]\nlike = (1 - pnorm(3, theta_grid, sqrt(3)))^2 \nprior = dnorm(theta_grid, 1.3271, sqrt(0.05597))\npost = like * prior # unnormalized\npost = post / sum(post * bin_width)\nlike = like / sum(like * bin_width)\nplot(theta_grid, prior, type = \"l\", col = \"orange\", lwd = 3, xlab = expression(theta), ylab = \"density\")\nlines(theta_grid, like, col = \"steelblue\", lwd = 3)\nlines(theta_grid, post, col = \"indianred\", lwd = 3)\nlegend(\"topleft\", legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),\n       col = c(\"orange\", \"steelblue\", \"indianred\"), lwd = 3, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.6\nDerive the posterior distribution for the normal model with a normal prior.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.7\na) Let \\(X_1,\\dots,X_n |\\theta \\sim \\mathrm{Uniform}(\\theta-1/2,\\theta+1/2)\\) for \\(-\\infty &lt; \\theta &lt;\\infty\\). The estimator \\(\\hat\\theta= \\bar X\\) is unbiased for \\(\\theta\\). Calculate for the sampling variance of \\(\\hat\\theta\\).\n\\(\\textit{Note}\\): A more efficient estimator is the mid-range \\(\\hat\\theta = (X_{\\min} + X_{\\max})/2\\), but we use the sample mean here for simplicity.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe variance of a sample mean is always of the form \\[\n\\mathbb{V}(\\bar X) = \\frac{\\sigma^2}{n},\n\\] where \\(\\sigma^2\\) is the variance of each observation \\(X_i \\sim \\mathrm{Uniform}(\\theta-1/2,\\theta+1/2)\\). The variance of \\(\\mathrm{Uniform}(a,b)\\) variable is \\(\\frac{(b-a)^2}{2}\\), so here we have \\(\\sigma^2 = \\frac{1}{12}\\) and the sampling variance of the estimator is \\[\n\\mathbb{V}(\\bar X) = \\frac{1}{12n}\n\\]\n\n\n\nb) Derive the posterior distribution for \\(\\theta\\) assuming a uniform prior distribution over the whole real line.\n\\(\\textit{Hint}\\): once you have observed some data, some values for \\(\\theta\\) are no longer possible.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe density for each observation in the sample is \\[\np(x \\vert \\theta) = I\\Big(\\theta- \\frac{1}{2} \\leq x \\leq \\theta + \\frac{1}{2}\\Big).\n\\] The likelihood based a single observation is therefore zero whenever \\(\\theta \\geq x-\\frac{1}{2}\\) or when \\(\\theta \\leq x + \\frac{1}{2}\\). So, to highlight that we care about \\(p(x\\vert\\theta)\\) as function of \\(\\theta\\), we can write \\[\np(x \\vert \\theta) = I\\Big(x - \\frac{1}{2} \\leq \\theta \\leq x + \\frac{1}{2}\\Big).\n\\]\nThe likelihood from an iid sample of \\(n\\) observation can therefore be written \\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\prod_{i=1}^n p(x_i \\vert \\theta) = \\prod_{i=1}^n I\\Big(x_i - \\frac{1}{2} \\leq \\theta \\leq x_i + \\frac{1}{2}\\Big)\n\\] The plots below illustrate how certain \\(\\theta\\) values makes the single data point \\(x_1 = 2.1\\) impossible.\n\n\nCode\nxi = 2.1\npar(mfrow = c(3,1))\n\n# Adding the uniform density for an impossible theta\ntheta = 1.2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 1.2 is too low - distribution misses the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"orange\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"orange\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"orange\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"orange\")\n\n# Adding the uniform density for an impossible theta\ntheta = 2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 2 is OK - distribution captures the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"cornflowerblue\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"cornflowerblue\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"cornflowerblue\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"cornflowerblue\")\n\n# Adding the uniform density for an impossible theta\ntheta = 3.2\nplot(xi, 0, pch = 19, col = \"indianred\", xlim = c(0,4), ylim = c(-0.1,1.1), \n     main = \"theta = 3.2 is too high - distribution misses the data x = 2.1\")\nabline(h = 0, lty = \"dashed\")\npoints(theta, 0, pch = 3, col = \"green\", lwd = 3)\nlines(c(theta - 0.5, theta + 0.5), c(1,1), col = \"green\")\nlines(c(theta - 0.5, theta - 0.5), c(0,1), col = \"green\")\nlines(c(theta + 0.5, theta + 0.5), c(0,1), col = \"green\")\n\n\n\n\n\n\n\n\n\nThe likelihood is non-zero only for the \\(\\theta\\) values where \\(x_i - \\frac{1}{2} \\leq \\theta \\leq x_i + \\frac{1}{2}\\) for all data observations \\(i=1,2,\\ldots,n\\). This means that the likelihood is non-zero only when \\(x_\\max - \\frac{1}{2} \\leq \\theta \\leq x_\\min + \\frac{1}{2}\\), where \\(x_\\min\\) and \\(x_\\max\\) are the minimum and maximum of the sample. With a uniform prior on \\(\\theta\\), the posterior is proportional to the likelihood, so \\[\np(\\theta \\vert x_1,\\ldots,x_n) \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\propto I\\Big( x_\\max - \\frac{1}{2} \\leq \\theta \\leq x_\\min + \\frac{1}{2} \\Big)\n\\] So the posterior distribution is \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Uniform}\\Big(x_\\max - \\frac{1}{2} , \\leq x_\\min + \\frac{1}{2} \\Big)\n\\]\n\n\n\nc) Assume that you have observed three data observations: \\(x_1 = 1.1, x_2 = 2.05, x_3 = 1.21\\). What would a frequentist conclude about \\(\\theta\\)? What would a Bayesian conclude? Discuss.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe frequentist with \\(\\bar X\\) as the estimator of \\(\\theta\\) obtains the estimate \\(\\bar x \\approx 1.453\\). The sampling standard deviation is \\(\\mathbb{S}(\\bar X) = \\sqrt{\\frac{1}{12n}} = \\sqrt{\\frac{1}{3\\cdot 12}} \\approx 0.1666\\), which is the variability of the sample mean estimator on average over all possible datasets of size \\(n=3\\). The variability is rather large since we only have three observations, so we can easily obtain a sample with three extreme (all small or all large) observations. Here is a plot of the simulated sampling distribution for \\(\\bar X\\) from a sample with three observations:\n\ntheta = 1  # any value is ok, it will be the center of the sampling distr.\nnRep = 50000\nn = 3\nxbar = rep(0, nRep)\nfor (i in 1:nRep){\n  x_rep = runif(n, min = theta - 0.5, max = theta + 0.5)\n  xbar[i] = mean(x_rep)\n}\nhist(xbar, 50, freq = FALSE, \n     main = \"sampling distribution of the sample mean\", \n     xlab = \"sample mean\", ylab = \"density\", col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n(As a side-note: note how fast the central limit theorem is here, the sampling distribution is already close to normal as \\(n=3\\))\nFor the given dataset we have \\(x_\\min = 1.1\\) and \\(x_\\max = 2.05\\) and the posterior is therefore \\[\n\\theta \\vert x_1,x_2,x_3 \\sim \\mathrm{Uniform}\\big(1.55 , 1.60 \\big)\n\\] Since we were lucky to obtain a range of the data close to \\(1\\) (the range is the difference between the maximum and minimum observations: \\(2.05-1.1 = 0.95\\)), the Bayesian gets a tight posterior which is uniform between \\(1.55\\) and \\(1.60\\). The posterior is plotted here:\n\n\nCode\nx = c(1.10, 2.05, 1.21)\nplot(x = NA, y = NA, xlim = c(1,2), ylim = c(-1,1), \n     xlab = expression(theta), ylab = \"posterior density\")\npost_low = max(x) -0.5\npost_high = min(x) + 0.5\nlines(c(post_low, post_high), c(1, 1), col = \"orange\")\nlines(c(post_low, post_low), c(0, 1), col = \"orange\")\nlines(c(post_high, post_high), c(1, 0), col = \"orange\")\nabline(h=0, lty = \"dashed\")\n\n\n\n\n\n\n\n\n\nThe difference between the frequentist and Bayesian solutions is that the Bayesian solution conditions on the observed data, while the frequentist inferences are unconditional on the data, measuring the variability of the estimator over all possible datasets. We got lucky with a wide range in the actually observed data, so the Bayesian can provide a tight posterior for \\(\\theta\\) with little uncertainty.\n\n\n\n\n\n\nExercise 2.8\nExercise modelled the survival times of uncensored lung cancer patients with an iid exponential model. In this exercise we will extend that analysis to include also the censored patients, using the same prior as in Exercise . Plot the prior and posterior densities for \\(\\theta\\) over a suitable grid of \\(\\theta\\)-values. Finally, assess the fit of the exponential model by plotting a histogram of \\(\\texttt{time}\\) and overlay the pdf of the exponential model with the parameter \\(\\theta\\) estimated with the posterior mode.\n\\(\\textit{Hint}\\): The posterior is no longer tractable due to contributions of the censored patients to the likelihood. For the censored patients we only know that they lived the number of days recorded in the dataset. The likelihood contribution \\(p(x_c \\vert \\theta)\\) for the \\(c\\)th censored patient with recorded time \\(x_c\\) is therefore \\(p(X \\geq x_c \\vert \\theta) = e^{-\\theta x_c}\\), which follows from the distribution function of the exponential distribution \\(p(X \\leq x \\vert \\theta) = 1 - e^{-\\theta x}\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nThe likelihood for all data, censored and uncensored, is \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\theta) & = \\prod_{i=1}^n p(x_i \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} p(x_c \\vert \\theta) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) \\prod_{c \\in \\mathcal{C}} \\left(1 - F(x_c \\vert \\theta)\\right)\n\\end{align}\n\\] where \\(\\mathcal{U}\\) and \\(\\mathcal{C}\\) are the sets of observation indicies for the uncensored and censored data, respectively. The likelihood for the uncensored data (the first product) is the same as before \\[\n\\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\theta) = \\prod_{u \\in \\mathcal{U}} \\theta e^{-\\theta x_u} = \\theta^{n_u} e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u},\n\\] where \\(n_u\\) is the number of uncensored observations. The likelihood contribution for each observation in the censored set (the second product) is the survival function \\[\n\\mathrm{Pr}(X \\geq x_c) = 1 - F(x_c \\vert \\theta),\n\\] where \\(F(x_c \\vert \\theta) = 1 - e^{-x_c \\theta}\\) is the cumulative distribution function of the exponential distribution evaluated at \\(x_c\\).\nSo the likelihood function is \\[\np(x_1,\\ldots,x_n \\vert \\theta) = \\theta^n e^{-\\theta\\sum_{u \\in \\mathcal{U}} x_u} \\times e^{-\\theta \\sum_{u \\in \\mathcal{U}} x_c} = \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i}.\n\\] where one should note that \\(\\theta\\) is raised to the number of uncensored observations, \\(n_u\\) while the sum in the exponential term includes both uncensored and censored observations.\nBy Bayes’ theorem, the posterior distribution is again a Gamma distribution \\[\n\\begin{align}\np(\\theta \\vert x_1,\\ldots,x_n) & \\propto p(x_1,\\ldots,x_n \\vert \\theta)p(\\theta) \\\\\n& \\propto \\theta^{n_u} e^{-\\theta\\sum_{i = 1}^n x_i} \\times \\theta^{\\alpha-1}e^{-\\beta\\theta} \\\\\n& = \\theta^{\\alpha + n_u - 1} e^{ -\\theta(\\beta + \\sum_{i = 1}^n x_i)},\n\\end{align}\n\\] which we recognize as proportional to the following Gamma distribution \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n_u,\\beta + \\sum\\nolimits_{i=1}^n x_i).\n\\]\nThe code below plots both:\n\nthe posterior from the previous exercise (a) with only the uncensored data and\nthe posterior from with all data.\n\nThe posterior with all data is more informative and concentrates on smaller \\(\\theta\\) values. Since smaller \\(\\theta\\) values correspond to longer expected survival times, this is makes sense since the censored patients were still alive at the end of the study.\n\n# Summarize the data needed for the posterior, grouped by `status`:\ndata_summary &lt;- lung %&gt;% group_by(status) %&gt;% summarize(n = n(), sum_x = sum(time))\n\n\n# Set up prior hyperparameters\nalpha_prior &lt;- 3   # shape parameter\nbeta_prior &lt;- 300  # rate parameter\n\n# Compute posterior hyperparameters - only uncensored data\nalpha_post_u &lt;- alpha_prior + data_summary$n[2] # second row is uncensored data (status = 2)  \nbeta_post_u &lt;- beta_prior + data_summary$sum_x[2] # sum over uncensored observations\n\n# Compute posterior hyperparameters - all data\nalpha_post_all &lt;- alpha_prior + data_summary$n[2] # note: this is still n_u \nbeta_post_all &lt;- beta_prior + sum(data_summary$sum_x) # sum over all observations   \n\n\n# Plot the prior and the two posterior densities \nthetaGrid &lt;- seq(0, 0.03, length.out = 1000)\nprior_density &lt;- dgamma(thetaGrid, shape = alpha_prior, rate = beta_prior)\nposterior_density_u &lt;- dgamma(thetaGrid, shape = alpha_post_u, rate = beta_post_u)\nposterior_density_all &lt;- dgamma(thetaGrid, shape = alpha_post_all, rate = beta_post_all)\n\n\ndf &lt;- data.frame(\n  thetaGrid = thetaGrid, \n  prior = prior_density, \n  posterior_uncensored = posterior_density_u,\n  posterior_all = posterior_density_all\n)\n\ndf_long &lt;- df %&gt;% pivot_longer(-thetaGrid, names_to = \"density_type\", values_to = \"density\")\n\nggplot(df_long) +\n  aes(x = thetaGrid, y = density, color = density_type) +\n  geom_line() +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior_uncensored\", \"posterior_all\"), \n    values = c(colors[2], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.2\", x = expression(theta), y = \"Density\", color = \"\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThe code below plots the histogram and the pdf of the exponential model with the parameter \\(\\theta\\) set equal to the posterior mode. It is clear that the exponential model with its monotonically decreasing density is not fitting the data well.\n\npostMode = df$thetaGrid[which.max(df$posterior_all)]\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dexp, args = list(rate = postMode), lwd = 1, \n                aes(color = \"Exponential fit\"),\n  ) +\n  labs(title = \"Exercise 2.2c - Exponential model fit to lung cancer survival\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[5]) +\n  scale_color_manual(\"\", values = colors[3]) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.9\nShow that the \\(N(\\mu,1)\\) distribution belongs to the exponential family.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.10\nExercise modelled the survival times of uncensored lung cancer patients with an iid exponential model. Here we extend that analysis to the more general Weibull distribution: \\[\nX_1,\\ldots,X_n \\vert \\lambda, k \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Weibull}(\\lambda,k).\n\\] The value of \\(k\\) determines how the failure rate changes with time:\nPlot the posterior distribution of \\(\\lambda\\) conditional on \\(k=1\\), \\(k=3/2\\) and \\(k=2\\). For all \\(k\\), use the prior \\(\\lambda \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) with \\(\\alpha=3\\) and \\(\\beta=1/50\\) (which is a similar prior for \\(\\theta=1/\\lambda\\) as in Exercise 2.2). Plot the time variable as a histogram and overlay the fitted model for the three different \\(k\\)-values; use the posterior mode for \\(\\theta\\) in each model when plotting the fitted model density.\n\\(\\textit{Hint}\\): the posterior distribution for \\(k\\neq 1\\) is intractable, so use numerical evaluation of the posterior over a grid of \\(\\lambda\\)-values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood can be computed with separate treatment of the uncensored and censored observations: \\[\n\\begin{align}\np(x_1,\\ldots,x_n \\vert \\lambda, k) & = \\prod_{i=1}^n p(x_i \\vert \\lambda, k) \\\\\n& = \\prod_{u \\in \\mathcal{U}} p(x_u \\vert \\lambda, k) \\prod_{c \\in \\mathcal{C}} \\Big(1 - F(x_c \\vert \\lambda, k)\\Big)\n\\end{align}\n\\] where \\(p(x \\vert \\lambda, k)\\) is the pdf of a Weibull variable \\[\np(x \\vert \\lambda, k) = \\frac{k}{\\lambda}\\Big( \\frac{x}{\\lambda} \\Big)^{k-1}e^{-(x/\\lambda)^k}\\quad\\text{ for }x&gt;0\n\\] which is implemented in R as dweibull. The cdf of the Weibull distribution is of rather simple form \\[\nF(x \\vert \\lambda, k) = 1 - e^{-(x/\\lambda)^k}\n\\] and is implemented in R as pweibull.\nThe code below plots the prior and posterior distribution for \\(\\lambda\\) for the three different \\(k\\)-values. We could have inserted the mathematical expressions for the pdf and cdf and simplified the final likelihood expression; we will instead use the dweibull and pweibull functions without simplifications since it gives a more general template that can be used for any distribution, not just the Weibull model. For numerical stability we usually compute the posterior distribution on the log scale \\[\n\\log p(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j)\n\\] for a grid of equally spaced \\(\\lambda\\)-values: \\(\\lambda^{(1)}\\ldots,\\lambda^{(J)}\\). The \\(\\propto\\) sign now means that there is a missing additive constant \\(\\log p(x_1,\\ldots,x_n)\\) which does not depend on the unknown parameter \\(\\lambda\\). When we have computed \\(\\log p(\\lambda \\vert x_1,\\ldots,x_n)\\) over a grid of \\(\\lambda\\) values we compute the posterior on the original scale by \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) \\propto \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda_j) + \\log p(\\lambda_j) \\Big)\n\\] and then divide all numbers with the normalizing constant to make sure that the posterior integrates to one. This is done numerically by approximating the integral by a Riemann rectangle sum \\[\np(\\lambda^{(j)} \\vert x_1,\\ldots,x_n) =\n\\frac{\\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(j)}) + \\log p(\\lambda^{(j)}) \\Big)}\n{\\sum_{h=1}^J \\exp\\Big( \\log p(x_1,\\ldots,x_n \\vert \\lambda^{(h)}) + \\log p(\\lambda^{(h)}) \\Big) \\Delta}\n\\] where \\(\\Delta\\) is the spacing between the grid points of \\(\\lambda\\)-values: \\(\\lambda^{(1)}, \\ldots, \\lambda^{(J)}\\).\n\nlibrary(tidyverse) # loads data manipulation and visualization packages\nlibrary(survival) # loads the lung cancer data as `lung`\ncolors = c(\"#6C8EBF\", \"#c0a34d\", \"#780000\",\"#007878\",\"#B5C6DF\",\"#EADAAA\",\"#AE6666\")\n\nSet up prior hyperparameters\n\nalpha_prior &lt;- 3     # shape parameter\nbeta_prior &lt;- 1/50   # rate parameter\n\nSet up function that computes the likelihood for any \\(\\lambda\\) value:\n\n# Make a function that computes the likelihood\nweibull_loglike &lt;- function(lambda, x, censored, k){\n  loglik_uncensored = sum(dweibull(x[-censored], shape = k, scale = lambda, \n                                   log = TRUE))\n  loglik_censored = sum(pweibull(x[censored], shape = k, scale = lambda, \n                                 lower.tail = FALSE, log.p = TRUE))\n  return(loglik_uncensored + loglik_censored)\n}\n\nSet up a function that computes the posterior density over a grid of \\(\\lambda\\):\n\nweibull_posterior &lt;- function(lambdaGrid, x, censored, k, alpha_prior, beta_prior){\n  Delta = lambdaGrid[2] - lambdaGrid[1] # Grid step size\n  logPrior &lt;- dgamma(lambdaGrid, shape = alpha_prior, rate = beta_prior, log = TRUE)\n  logLike &lt;- sapply(lambdaGrid, weibull_loglike, x, censored, k)\n  logPost &lt;- logLike + logPrior\n  logPost &lt;- logPost - max(logPost) # subtract constant to avoid overflow\n  post &lt;- exp(logPost)/(sum(exp(logPost))*Delta) # original scale and normalize\n  logLike &lt;- logLike - max(logLike)\n  likeNorm &lt;- exp(logLike)/(sum(exp(logLike))*Delta) # normalized likelihood\n  return(list(post = post, prior = exp(logPrior), likeNorm = likeNorm))\n}\n\n\n# Plot the prior and posterior densities\n\nlambdaGrid &lt;- seq(200, 800, length.out = 1000)\n# Compute to get the prior\npostRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k = 1, \n                             alpha_prior, beta_prior)\ndf &lt;- data.frame(\n  lambdaGrid = lambdaGrid, \n  prior = postRes$prior\n)\n\n# Compute for all selected k values\npostModes = c()\nfor (k in c(1, 3/2, 2)){\n  postRes &lt;- weibull_posterior(lambdaGrid, lung$time, lung$status == 1, k, alpha_prior, beta_prior)\n  df[str_glue(\"posterior k={k}\")] &lt;- postRes$post\n  postModes = c(postModes, lambdaGrid[which.max(postRes$post)])\n}\n\ndf_long &lt;- df %&gt;% pivot_longer(-lambdaGrid, names_to = \"density_type\", values_to = \"density\")\n\n# Plot using ggplot2\nggplot(df_long) +\n  aes(x = lambdaGrid, y = density, color = density_type) +\n  geom_line() +\n  xlim(250,600) +\n  scale_colour_manual(\n    breaks = c(\"prior\", \"posterior k=1\", \"posterior k=1.5\", \"posterior k=2\"), \n    values = c(colors[2], colors[1], colors[3], colors[4])) +\n  labs(title = \"Exercise 2.3\", x = expression(lambda), y = \"Density\", color = \"\") + \n  theme_minimal()\n\nWarning: Removed 1668 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThe fit of the three Weibull models are plotted below. The best fit seems to be for \\(k=3/2\\), but it is still not very good. In a later exercise you will be asked to freely estimate both \\(\\lambda\\) and \\(k\\), and even later to fit a Weibull regression model with covariates.\n\nggplot(lung, aes(time)) +\n  geom_histogram(aes(y = after_stat(density), fill = \"Data\"), bins = 30) +\n  stat_function(fun = dweibull, args = list(shape = 1, scale = postModes[1]), lwd = 1, \n                aes(color = \"Weibull fit k = 1\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 3/2, scale = postModes[2]), lwd = 1, \n                aes(color = \"Weibull fit k = 3/2\"),\n  ) +\n  stat_function(fun = dweibull, args = list(shape = 2, scale = postModes[3]), lwd = 1, \n                aes(color = \"Weibull fit k = 2\"),\n  ) +\n  labs(title = \"Weibull model fits\", x = \"days\", y = \"Density\") + \n  scale_fill_manual(\"\", values = colors[6]) +\n  scale_color_manual(\"\", values = c(colors[1], colors[3], colors[4])) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.11\nLet \\[\nX_1,\\ldots,X_n \\overset{iid}{\\sim } \\mathrm{Uniform}(0,\\theta).\n\\] Show that the Pareto prior \\(\\theta \\sim \\mathrm{Pareto}(\\alpha, \\beta)\\), is conjugate to the Uniform model by deriving the posterior distribution. See Box~ for a definition of the Pareto distribution and properties; note the support of the Pareto distribution.\n\\(\\textit{Hint}\\): Do not forget to include an indicator function when you write up the likelihood function since the \\(\\mathrm{Uniform}(0,\\theta)\\) distribution is zero for outcomes larger than \\(\\theta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe pdf of the \\(\\mathrm{Uniform}(0,\\theta)\\) distribution is \\[\\begin{equation*}\n  p(x) = \\begin{cases}\n          \\frac{1}{\\theta}  & \\text{if } x \\leq  \\theta \\\\\n          0                 & \\text{otherwise }\n          \\end{cases}\n\\end{equation*}\\] which can be written with an indicator function as \\[\np(x) = \\frac{1}{\\theta} I(x_i \\leq \\theta).\n\\] The likelihood function is therefore of the form \\[\\begin{equation*}\n   p(x_1,\\ldots,x_n\\vert\\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} I(x_i \\leq \\theta) = \\frac{1}{\\theta^n} \\prod_{i=1}^n I(x_i \\leq \\theta).\n\\end{equation*}\\] The factor \\(\\prod_{i=1}^n I(x_i \\leq \\theta)\\) is only non-zero if all \\(x_1,\\ldots,x_n\\) are smaller or equal to \\(\\theta\\), i.e. if \\(x_{\\mathrm{max}} := \\mathrm{max}(x_1, \\dots,x_n)\\) is smaller or equal to \\(\\theta\\). We can therefore write the likelihood as \\[\\begin{equation*}\n   p(x_1,\\ldots,x_n\\vert\\theta) = \\frac{1}{\\theta^n}  I(x_{\\mathrm{max}} \\leq \\theta).\n\\end{equation*}\\]\nThe Pareto prior can also be written with an indicator function \\[\\begin{align*}\n    p(\\theta) = \\frac{\\alpha \\beta^\\alpha }{\\theta^{\\alpha+1}}\\cdot I(\\beta \\leq \\theta),\n\\end{align*}\\] to explicitly include that \\(p(\\theta)=0\\) if \\(\\theta &lt; \\beta\\).\nBy Bayes’ theorem, the posterior is then \\[\\begin{align*}\n    p(\\theta \\vert x_1, \\dots,x_n) &\\propto p(x_1, \\dots,x_n \\vert \\theta)p(\\theta) \\\\\n                       &= \\frac{1}{\\theta^n} I(x_{\\mathrm{max}} \\leq \\theta)\n                            \\frac{\\alpha \\beta^\\alpha }{\\theta^{\\alpha+1}}\\cdot I(\\beta \\leq \\theta) \\\\\n                       &=  \\frac{\\alpha \\beta^\\alpha }{\\theta^{n+\\alpha+1}} \\cdot  \n                            I\\big(\\tilde \\beta \\leq \\theta \\big)        \n\\end{align*}\\] where \\(\\tilde \\beta = \\mathrm{max}(x_{\\mathrm{max}},\\beta)\\). This is proportional to a \\(\\mathrm{Pareto}\\big(\\alpha + n, \\tilde \\beta \\big)\\) density. Since the prior and posterior are both in the Pareto family, the Pareto prior is conjugate to the \\(\\mathrm{Uniform}(0,\\theta)\\) model."
  },
  {
    "objectID": "exercises/ch2/iid_normal_mean_known.html",
    "href": "exercises/ch2/iid_normal_mean_known.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.8\nLet \\(X_1,\\ldots,X_n \\mid \\theta,\\sigma^2 \\overset{\\mathrm{iid}}{\\sim} \\mathrm{N}(\\theta,\\sigma^2)\\), where \\(\\theta\\) is assumed known. Show that the \\(\\mathrm{Inv-}\\chi^2\\) distribution is a conjugate prior for \\(\\sigma^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe normal density function for a single observation \\(x_i\\) is \\[\np(x_i \\vert \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}(x_i-\\theta)^2\\Big) \\propto \\frac{1}{(\\sigma^2)^{1/2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}(x_i-\\theta)^2\\Big)\n\\] The likelihood for the iid normal model with known mean \\(\\theta\\) is therefore the product of \\(n\\) such density functions: \\[\n\\begin{aligned}\np(x_1,\\ldots,x_n \\vert \\theta, \\sigma^2) = \\prod_{i=1}^n p(x_i \\vert \\theta, \\sigma^2) &\\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\theta)^2\\Big) \\\\\n&\\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{n s^2}{2\\sigma^2}\\Big)\n\\end{aligned}\n\\] where we have defined \\(s^2 = \\frac{\\sum_{i=1}^n (x_i-\\theta)^2}{n}\\) as the sample standard deviation (dividing by \\(n\\) instead of \\(n-1\\) since the mean \\(\\theta\\) is assumed known).\nThe density of the \\(\\sigma^2 \\sim \\mathrm{Scaled-Inv-}\\chi^2(\\nu_0,\\sigma_0^2)\\) prior is of the form \\[\np(\\sigma^2) \\propto \\frac{1}{(\\sigma^2)^{1+\\nu_0/2}}\\exp\\Big( -\\frac{\\nu_0\\sigma_0^2}{2\\sigma^2} \\Big)\n\\] The posterior distribution from using the \\(\\sigma^2 \\sim \\mathrm{Scaled-Inv-}\\chi^2(\\nu_0,\\sigma_0^2)\\) prior is given by Bayes’ theorem as (to avoid cluttering the notation, we do not write out the conditioning on \\(\\theta\\) since it is known) \\[\n\\begin{aligned}\np(\\sigma^2 \\vert x_1,\\ldots,x_n) &\\propto\np(x_1,\\ldots,x_n \\vert \\sigma^2)p(\\sigma^2) \\\\\n& \\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{n s^2}{2\\sigma^2}\\Big)\n\\frac{1}{(\\sigma^2)^{1+\\nu_0/2}}\\exp\\Big( -\\frac{\\nu_0\\sigma_0^2}{2\\sigma^2} \\Big) \\\\\n&\\propto \\frac{1}{(\\sigma^2)^{1+(\\nu_0+n)/2}}\\exp\\Big( -\\frac{1}{2\\sigma^2}\\big( \\nu_0 \\sigma_0^2 + n s^2 \\big)  \\Big) \\\\\n&=  \\frac{1}{(\\sigma^2)^{1+(\\nu_0+n)/2}}\\exp\\Big( -\\frac{\\nu_0+n}{2\\sigma^2}\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n}  \\Big)\n\\end{aligned}\n\\] which is proportional to a \\(\\sigma^2 \\sim \\chi^2(\\nu_0+n,\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n})\\) density. Note how the location parameter in the posterior \\[\n\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n} = \\frac{\\nu_0}{\\nu_0+n} \\sigma_0^2 + \\frac{n}{\\nu_0+n}s^2\n\\] is a weighted average of prior location \\(\\sigma_0^2\\) and the data estimate \\(s^2\\), with more weight placed on the strongest information source (prior with \\(\\nu_0\\) imaginary sample data points versus the data with \\(n\\) actual data points)."
  },
  {
    "objectID": "exercises/ch3/iid_normal_mean_known.html",
    "href": "exercises/ch3/iid_normal_mean_known.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 3.3\nLet \\(X_1,\\ldots,X_n \\mid \\theta,\\sigma^2 \\overset{\\mathrm{iid}}{\\sim} \\mathrm{N}(\\theta,\\sigma^2)\\), where \\(\\theta\\) is assumed known. Show that the \\(\\mathrm{Inv-}\\chi^2\\) distribution is a conjugate prior for \\(\\sigma^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe normal density function for a single observation \\(x_i\\) is \\[\np(x_i \\vert \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}(x_i-\\theta)^2\\Big) \\propto \\frac{1}{(\\sigma^2)^{1/2}}\\exp\\Big(-\\frac{1}{2\\sigma^2}(x_i-\\theta)^2\\Big)\n\\] The likelihood for the iid normal model with known mean \\(\\theta\\) is therefore the product of \\(n\\) such density functions: \\[\n\\begin{aligned}\np(x_1,\\ldots,x_n \\vert \\theta, \\sigma^2) = \\prod_{i=1}^n p(x_i \\vert \\theta, \\sigma^2) &\\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\theta)^2\\Big) \\\\\n&\\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{n s^2}{2\\sigma^2}\\Big)\n\\end{aligned}\n\\] where we have defined \\(s^2 = \\frac{\\sum_{i=1}^n (x_i-\\theta)^2}{n}\\) as the sample standard deviation (dividing by \\(n\\) instead of \\(n-1\\) since the mean \\(\\theta\\) is assumed known).\nThe density of the \\(\\sigma^2 \\sim \\mathrm{Scaled-Inv-}\\chi^2(\\nu_0,\\sigma_0^2)\\) prior is of the form \\[\np(\\sigma^2) \\propto \\frac{1}{(\\sigma^2)^{1+\\nu_0/2}}\\exp\\Big( -\\frac{\\nu_0\\sigma_0^2}{2\\sigma^2} \\Big)\n\\] The posterior distribution from using the \\(\\sigma^2 \\sim \\mathrm{Scaled-Inv-}\\chi^2(\\nu_0,\\sigma_0^2)\\) prior is given by Bayes’ theorem as (to avoid cluttering the notation, we do not write out the conditioning on \\(\\theta\\) since it is known) \\[\n\\begin{aligned}\np(\\sigma^2 \\vert x_1,\\ldots,x_n) &\\propto\np(x_1,\\ldots,x_n \\vert \\sigma^2)p(\\sigma^2) \\\\\n& \\propto  \\frac{1}{(\\sigma^2)^{n/2}}\\exp\\Big( -\\frac{n s^2}{2\\sigma^2}\\Big)\n\\frac{1}{(\\sigma^2)^{1+\\nu_0/2}}\\exp\\Big( -\\frac{\\nu_0\\sigma_0^2}{2\\sigma^2} \\Big) \\\\\n&\\propto \\frac{1}{(\\sigma^2)^{1+(\\nu_0+n)/2}}\\exp\\Big( -\\frac{1}{2\\sigma^2}\\big( \\nu_0 \\sigma_0^2 + n s^2 \\big)  \\Big) \\\\\n&=  \\frac{1}{(\\sigma^2)^{1+(\\nu_0+n)/2}}\\exp\\Big( -\\frac{\\nu_0+n}{2\\sigma^2}\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n}  \\Big)\n\\end{aligned}\n\\] which is proportional to a \\(\\sigma^2 \\sim \\chi^2(\\nu_0+n,\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n})\\) density. Note how the location parameter in the posterior \\[\n\\frac{\\nu_0 \\sigma_0^2 + n s^2 }{\\nu_0+n} = \\frac{\\nu_0}{\\nu_0+n} \\sigma_0^2 + \\frac{n}{\\nu_0+n}s^2\n\\] is a weighted average of prior location \\(\\sigma_0^2\\) and the data estimate \\(s^2\\), with more weight placed on the strongest information source (prior with \\(\\nu_0\\) imaginary sample data points versus the data with \\(n\\) actual data points)."
  },
  {
    "objectID": "exercises/ch3/multinomial_dirichlet_voting.html",
    "href": "exercises/ch3/multinomial_dirichlet_voting.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 3.6\nA Swedish poll in 2024 asked \\(2311\\) persons the question: The table below gives the poll results across the eight parties in parliament and the nineth option . \nLet \\(\\boldsymbol{y} = (y_1,\\ldots,y_9)\\) denote the number of votes for each of the nine parties, where \\(y_c\\) is the number of votes for party \\(c\\). Model the data as iid from a multinomial distribution \\[\\begin{equation*}\n    \\boldsymbol{y} \\mid \\boldsymbol{\\theta} \\sim \\mathrm{Multinomial}(n,\\boldsymbol{\\theta}),\n\\end{equation*}\\] where \\(n=2311\\) is the total number of respondents and \\(\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_9)\\) is the vector of voting proportions for each party, where \\(\\theta_c\\) is the proportion of votes for party \\(c\\).\nThe previous election in 2022 resulted in the following voting percentages: \nUse Dirichlet prior \\(\\boldsymbol{\\theta} \\sim \\mathrm{Dirichlet}(\\boldsymbol{\\alpha})\\) with \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_9)\\). Set the priorhyperparameters based on the election results from 2022, but assuming that the prior information is equivalent to only \\(400\\) respondents today.\na) What is the posterior distribution for the voting shares?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSet up data for the current poll, and the results from the election in 2022:\n\ny = c(410, 88, 83, 81, 721, 196, 238, 434, 60)\nelection2022 = c(19.10, 4.61, 6.71, 5.34, 30.33, 6.75, 5.08, 20.54, 1.54)\nproportionElection2022 = election2022/100\nalphaPrior = 400*proportionElection2022 # not integers, but that's OK\n\nCompute the posterior hyperparameters by adding up the data count \\(y_c\\) in each category with its corresponding prior count \\(\\alpha_c\\):\n\nalphaPost = alphaPrior + y\n\nSo the (joint) posterior distribution for the Swedish voting shares are \\[\n\\boldsymbol{\\theta} \\vert \\boldsymbol{y} \\sim \\mathrm{Dirichlet}\\Big(486.4, 106.44, 109.84, 102.36, 842.32, 223, 258.32, 516.16, 66.16\\Big)\n\\]\n\n\n\nb) What is the marginal posterior distribution for the voting share of the S-party? Plot the distribution without resorting to simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom the properties of the Dirichlet distribution box in the Bayesian Learning book, we know that if \\((\\theta_1,\\ldots,\\theta_C) \\sim \\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_C)\\), then marginal distribution for the probability/proportion in category \\(c\\) is\n\\[\n\\theta_c \\sim \\mathrm{Beta}(\\alpha_c, \\alpha_+ -\\alpha_c).\n\\] We can apply this result to get the marginal posterior for \\(\\theta_5\\) (the proportion of S-voters), we just need to remember that the parameters in the posterior are \\(\\alpha_c + y_c\\) for \\(c=1,\\ldots,C\\), or alphaPost in the code. The marginal posterior for the proportion of S-party voters is \\[\n\\theta_5 \\vert \\boldsymbol{y} \\sim \\mathrm{Beta}\\Big(842.32, 1868.68 \\Big),\n\\] which we can plot:\n\nthetaGrid = seq(0, 1, length = 1000)\nplot(thetaGrid, dbeta(thetaGrid, alphaPost[5], sum(alphaPost) - alphaPost[5]), \n     xlab = \"proportion of S-voters\", ylab = \"posterior density\", type = \"l\", \n     lwd = 3, col = \"indianred\")\n\n\n\n\n\n\n\n\n\n\n\nc) A party needs at least 4% of the votes to get into parliament. Use simulation to compute the posterior probability that both L and KD parties do not make it to parliament.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nDefine a Dirichlet random number generator using the algorithm in Chapter 3.\n\nrdirichlet &lt;- function(nIter, param){\n  nCat &lt;- length(param)\n  thetaDraws &lt;- matrix(0,nIter,nCat) #Storage\n  for (j in 1:nCat){\n    thetaDraws[,j] &lt;- rgamma(nIter, param[j], 1)\n  }\n  for (i in 1:nIter){\n    thetaDraws[i,] = thetaDraws[i,]/sum(thetaDraws[i,])\n  }\n  return(thetaDraws)\n}\n\nUse the simulator to simulate \\(10000\\) draws from the posterior distribution. For each draw check if both the L and KD party each have less than 4% of the votes.\n\nnSim = 10000\npostDraws = rdirichlet(nSim, alphaPost)\nbothPartyInParliament = rep(NA, nSim)\nfor (i in 1:nSim){\n  bothPartyInParliament[i] = all(postDraws[i,c(2,4)] &lt; 0.04)\n}\n\nThe posterior probability that both parties are below \\(4\\%\\) is 0.4353."
  },
  {
    "objectID": "exercises/ch3/iid_normal_pesticide.html",
    "href": "exercises/ch3/iid_normal_pesticide.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 3.1\nA dataset contains observations on measurements of pesticide for \\(n=24\\) pike fish in a lake in southern Italy. The sample mean pesticide level is \\(\\bar{x} = 12.5\\) with a sample standard deviation of \\(s=3.1\\). Assume that the pesticide levels \\(X_1,\\ldots,X_n|\\theta,\\sigma^2 \\sim \\mathrm{N}(\\theta,\\sigma^2)\\) are independent and identically distributed normal random variables with unknown mean \\(\\theta\\) and unknown variance \\(\\sigma^2\\).\nCompute the joint posterior distribution for \\(\\theta\\) and \\(\\sigma^2\\) using a conjugate prior distribution. Use a prior mean for \\(\\theta\\) of \\(8\\) and an (imaginary) prior sample size of \\(5\\). Use \\(\\sigma_0^2 = 1^2\\) and \\(\\nu_0 = 3\\) degrees of freedom in your prior for \\(\\sigma^2\\). Plot both the marginal posterior distributions for \\(\\theta\\) and \\(\\sigma^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe posterior distribution is given by \\[\n\\begin{aligned}\n\\theta | \\sigma^2 , \\boldsymbol{x} &\\sim N\\Big(\\mu_n,\\frac{\\sigma^2}{\\kappa_n}\\Big) \\\\\n\\sigma^2 | \\boldsymbol{x} &\\sim \\mathrm{Inv-}\\chi^2(\\nu_n,\\sigma_n^2)\n\\end{aligned}\n\\] where \\[\n\\begin{aligned}\n\\mu_n &= w \\bar x + (1-w)\\mu_0 \\\\\nw &= \\frac{n}{\\kappa_0+n} \\\\\n\\kappa_n &= \\kappa_0 + n \\\\\n\\nu_n &= \\nu_0 + n  \\\\\n\\nu_n\\sigma_n^2 &= \\nu_0\\sigma_0^2 + (n-1)s^2 + \\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar x -\\mu_0)^2\n\\end{aligned}\n\\] We have\n\n# data \nn = 24\nxBar = 12.5\ns2 = 3.1^2\n\n# prior\nmu0 = 8\nkappa0 = 5\nnu0 = 3\nsigma02 = 1^2\n\n# posterior\nw = n/(kappa0 + n)\nmun = w*xBar + (1-w)*mu0\nkappan = kappa0 + n\nnun = nu0 + n\nsigman2 = (nu0*sigma02 + (n-1)*s2  + \n             (kappa0*n/(kappa0 + n))*(xBar - mu0)^2 )/nun\n\nSo the joint posterior is \\[\n\\begin{aligned}\n\\theta | \\sigma^2 , \\boldsymbol{x} &\\sim N\\Big(11.724,\\frac{\\sigma^2}{29}\\Big) \\\\\n\\sigma^2 | \\boldsymbol{x} &\\sim \\mathrm{Inv-}\\chi^2(27,11.401)\n\\end{aligned}\n\\] The marginal posterior for \\(\\theta\\) is \\[\n\\theta \\vert \\boldsymbol{x} \\sim t\\Big(\\mu_n,\\frac{\\sigma_n^2}{\\kappa_n},\\nu_n\\Big) = t\\Big(27,\\frac{11.401}{29},27\\Big)\n\\] Plotting the marginal posterior of \\(\\theta\\):\n\nthetaGrid = seq(7, 18, length = 1000)\nsigma2Grid = seq(0.001, 30, length = 1000)\ndtdist &lt;- function(x, mu, sigma2, nu){\n  return((1/sqrt(sigma2))*dt(x = (x - mu)/sqrt(sigma2), df = nu))\n}\nplot(thetaGrid, dtdist(thetaGrid, mun, sigman2/kappan, nun), type = \"l\", \n     xlab = expression(theta), ylab = \"posterior density\", \n     col = \"indianred\", main = expression(theta))"
  },
  {
    "objectID": "exercises/ch2/iid_poisson_ambulance.html",
    "href": "exercises/ch2/iid_poisson_ambulance.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 2.12\nThe number of patients per day in need of an ambulance to a small regional hospital is random following a Poisson distribution with unknown mean \\(\\theta\\). The hospital has collected data from the past \\(n=7\\) days, resulting in the following daily counts of ambulance trips: \\(8, 12,  6,  9,  9,  9,  5\\). Before collecting the data, the head of the ambulance drivers believed that the mean number of ambulance trips per day is about \\(12\\) and that a mean of less than \\(5\\) trips is very unlikely, “like winning a lottery with only one winning ticket per hundred tickets”.\na) Compute the posterior distribution for \\(\\theta\\) using a Gamma prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\) with parameters \\(\\alpha\\) and \\(\\beta\\) set to reflect the head’s beliefs. Provide a plot of the prior distribution, the normalized likelihood and the posterior distribution.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first need to determine the two hyperparameters \\(\\alpha\\) and \\(\\beta\\) of the \\(\\mathrm{Gamma}(\\alpha,\\beta)\\) prior. From properties of the gamma distribution we have \\[\n\\mathbb{E}(\\theta) = \\frac{\\alpha}{\\beta} = 12\n\\] to match the prior mean of the head. We must therefore have \\(\\alpha = 12\\beta\\), leaving the single hyperparameter \\(\\beta\\) to be determined from the second condition: \\[\n\\mathrm{Pr}(\\theta &lt; 5) = 0.01\n\\] Let \\(F(\\theta \\vert \\alpha,\\beta)\\) denote the cumulative distribution function (cdf) of the \\(\\mathrm{Gamma}(\\alpha,\\beta)\\) prior. Since \\(\\alpha = 12\\beta\\) we need to solve the following equation for \\(\\beta\\): \\[\nF(5 \\vert 12\\beta,\\beta) = 0.01\n\\] In R code, we would like to find the beta this pgamma(5, shape = 12*beta, rate = beta) = 0.01 This cannot be (easily) solved analytically. We can either find the \\(\\beta\\) that satisfies pgamma(5, shape = 12*beta, rate = beta) = 0.01 by trial and error (that is, try different values of beta until you get really close to pgamma(5, shape = 12*beta, rate = beta) = 0.01). A faster way is to use a numerical method (for example Newton’s method) that finds roots to equations \\(f(x) = 0\\) (a so called root-finding algorithm), since our problem can be formulated as finding the \\(\\beta\\) that solves the equation \\(F(5 \\vert 12\\beta,\\beta) - 0.01 = 0\\). The uniroot function in R does the job:\n\nf &lt;- function(x){\n  return(pgamma(5, shape = 12*x, rate = x) - 0.01)\n}\nresults = uniroot(f, c(0.001,100)) # second argument is a search interval\nbetaPrior = results$root \nalphaPrior = 12*betaPrior\n\nThe value of \\(\\beta\\) that satisfies the equation \\(F(5 \\vert 12\\beta,\\beta) = 0.01\\) is \\(\\beta = 0.8472998\\). We can check that this \\(\\beta\\) indeed gives the correct prior probability \\(\\mathrm{Pr}(\\theta &lt; 5) = 0.01\\):\n\npgamma(5, shape = alphaPrior, rate = betaPrior)\n\n[1] 0.01000101\n\n\nBingo!\nSince \\(\\alpha = 12\\beta\\), the complete prior is \\(\\theta \\sim \\mathrm{Gamma}(\\alpha = 10.168, 0.847)\\). Here is a plot:\n\nthetaGrid = seq(0, 30, length = 1000)\nplot(thetaGrid, dgamma(thetaGrid, shape = alphaPrior, rate = betaPrior), \n     type = \"l\", ylab = \"prior density\", xlab = expression(theta), \n     col = \"orange\", lwd = 2)\n\n\n\n\n\n\n\n\nSanity check: the mean of \\(12\\) seems right, and also that \\(\\mathrm{Pr}(\\theta &lt; 5) = 0.01\\).\nFrom Chapter 2 of the Bayesian Learning book, we know that the posterior distribution is \\[\n\\theta \\vert \\boldsymbol{x} \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n x_i, \\beta + n)\n\\] So, since \\(\\sum_{i=1}^7 x_i = 58\\) we have\n\nx = c(8,12,6,9,9,9,5)\nn = length(x)\nalphaPost = alphaPrior + sum(x)\nbetaPost = betaPrior + n\nnTheta = 1000\nthetaGrid = seq(0, 15, length = nTheta)\nbinWidth = thetaGrid[2] - thetaGrid[1] # used to normalize likelihood\npriorDens = dgamma(thetaGrid, alphaPrior, betaPrior)\npostDens = dgamma(thetaGrid, alphaPost, betaPost)\nlike = rep(0, nTheta)\nfor (i in 1:nTheta){\n  like[i] = prod(dpois(x, thetaGrid[i]))\n}\nlike_norm = like/sum(binWidth*like)\nplot(thetaGrid, dgamma(thetaGrid, alphaPost, betaPost), \n     xlab = expression(theta), ylab = \"density\", type = \"l\", \n     col = \"orange\", lwd = 3)\nlines(thetaGrid, priorDens, col = \"orange\", lwd = 3)\nlines(thetaGrid, like_norm, col = \"steelblue\", lwd = 3)\nlines(thetaGrid, postDens, col = \"indianred\", lwd = 3)\nlegend(\"topleft\", legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),\n       col = c(\"orange\", \"steelblue\", \"indianred\"), lwd = 3, bty = \"n\")\n\n\n\n\n\n\n\n\n\n\n\nb) Compare the posterior mean, median and mode, and summarize the posterior distribution by a 95% credible interval.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince the posterior is Gamma, we can look formulas for the mean and mode of a Gamma distribution. If \\(X\\sim \\mathrm{Gamma}(\\alpha, \\beta)\\), then \\[\n\\begin{aligned}\n\\mathbb{E}(X) &= \\frac{\\alpha}{\\beta} \\\\\n\\mathrm{Mode}(X) &= \\frac{\\alpha -1}{\\beta} \\text{ for } \\alpha \\geq 1, \\text{ zero otherwise}\n\\end{aligned}  \n\\] The median is not available in closed from, but R gladly computes it with the \\(qgamma\\) function. Here is posterior mean, median and mode in the ambulance data example:\n\npostMean = alphaPost/betaPost\npostMedian = qgamma(0.5, alphaPost, betaPost)\npostMode = (alphaPost - 1)/betaPost\nmessage(\"The posterior mean is \", round(postMean,3))\n\nThe posterior mean is 8.687\n\nmessage(\"The posterior median is \", round(postMedian,3))\n\nThe posterior median is 8.644\n\nmessage(\"The posterior mode is \", round(postMode,3))\n\nThe posterior mode is 8.559\n\n\nSince the posterior distribution is fairly symmetric, these three measures of posterior location are similar.\nFinally, a 95% credible interval can be computed by the following code (there are many ways of doing this). The code assumes for simplicity that the HPD interval is truly an interval, and not several disjoint intervals, as would happen when the posterior is multimodal. We know from the plot above that the posterior is unimodal, so the code is safe to use here.\n\n# first, sort the density values from highest to lowest\npostDensOrdered = sort(postDens, decreasing = TRUE)  \n# reorder the thetaValues so that they still match the density values\nthetaOrdered = thetaGrid[order(postDens, decreasing = TRUE)] \ncumsumPostDens = cumsum(binWidth*postDensOrdered) # posterior cdf \ninHPD = which(cumsumPostDens &lt; 0.95) # find highest pdf vals up to 95% quota.\nhpd = c(min(thetaOrdered[inHPD]), max(thetaOrdered[inHPD]))\nmessage(paste0(\"The 95% HPD interval for theta is (\", hpd[1], \",\", hpd[2],\")\"))\n\nThe 95% HPD interval for theta is (6.68168168168168,10.7657657657658)\n\n\nPlot the HPD as a blue line:\n\nplot(thetaGrid, postDens, type = \"l\", col = \"indianred\", lwd = 2, \n     xlab = expression(theta), ylab = \"posterior density\") \nlines(hpd, c(0,0), col = \"steelblue\", lwd = 3)"
  },
  {
    "objectID": "exercises/ch3/gini.html",
    "href": "exercises/ch3/gini.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 3.7\nThe monthly income (in thousands Swedish Krona) of ten randomly selected persons are: 14, 25, 45, 25, 30, 33, 19, 50, 34 and 67. The log-normal distribution (see Box~ and ) is a commonly used model for income distributions. Let \\(Y_1,\\ldots,Y_n \\vert \\theta,\\sigma^{2}\\overset{\\mathrm{iid}}{\\sim}\\mathrm{LN}(\\theta,\\sigma^{2})\\), where \\(\\theta=\\log(33)\\) is assumed to be known but \\(\\sigma^{2}\\) is unknown with non-informative prior \\(p(\\sigma^{2})\\propto1/\\sigma^{2}\\). The posterior for \\(\\sigma^{2}\\) given \\(\\theta\\) is the \\(\\mathrm{Inv-}\\chi^{2}(n,\\tau^{2})\\) distribution, where \\[\\begin{equation*}\n    \\tau^{2}=\\frac{\\sum_{i=1}^{n}(\\log y_{i}-\\theta)^{2}}{n}.\n  \\end{equation*}\\]\na) Simulate \\(10,000\\) draws from the posterior of \\(\\sigma^{2}\\), assuming \\(\\theta=\\log(33)\\) is known. Plot a histogram of the posterior draws of the \\(\\sigma\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny = c(14, 25, 45, 25, 30, 33, 19, 50, 34, 67)\nlogy = log(y)\nn = length(y)\ntheta = log(33)\ntau2 = sum((logy - theta)^2)/n\n\n# Defining a function that simulates nSim draws\n# from the scaled inverse Chi-square distribution\nrScaledInvChi2 &lt;- function(nSim, df, scale){\n  return((df*scale)/rchisq(nSim, df=df))\n}\nnSim = 10000\nsigma2Draws = rScaledInvChi2(nSim, n, tau2)\nhist(sqrt(sigma2Draws), 100, col = \"steelblue\", main = \"\", freq = FALSE,\n     xlab = expression(sigma), ylab = \"posterior density\")\n\n\n\n\n\n\n\n\n\n\n\nb) A commonly used measure of income inequality is the Gini coefficient, \\(0\\leq G\\leq1\\), where \\(G=0\\) is complete income equality, and \\(G=1\\) means complete income inequality. It can be shown that \\(G=2\\Phi\\left(\\sigma/\\sqrt{2}\\right)-1\\) when incomes follow a \\(\\mathrm{LN}(\\theta,\\sigma^{2})\\) distribution, where \\(\\Phi(z)\\) is the cumulative distribution function (CDF) for the standard normal distribution. Use the posterior draws from a) to compute the posterior distribution of the Gini coefficient \\(G\\). Plot a histogram of the posterior draws of \\(G\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngini = rep(0, nSim)\nfor (i in 1:nSim){\n  gini[i] = 2*pnorm(sqrt(sigma2Draws[i])/sqrt(2)) - 1\n} \nhist(gini, 100, col = \"cornsilk\", main = \"\", freq = FALSE,\n     xlab = \"Gini coefficient, G\", ylab = \"posterior density\")\n\n\n\n\n\n\n\n\n\n\n\nc) Compute a \\(95\\)% Highest Posterior Density (HPD) interval for \\(G\\). To compute the HPD interval you will need an estimate of the posterior density for \\(G\\); a common approach is to use a kernel density estimator, for example the density function in R.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first estimate the posterior density from the posterior draws using a kernel density estimator, and plotting it on top of the histogram:\n\nkde = density(gini)\nginiGrid = kde$x\npostDens = kde$y\nhist(gini, 100, col = \"cornsilk\", main = \"\", freq = FALSE,\n     xlab = \"Gini coefficient, G\", ylab = \"posterior density\")\nlines(giniGrid, postDens, lwd = 3, col = \"indianred\")\n\n\n\n\n\n\n\n\nNow that we have the posterior density, and we can see that it is unimodal, we can use the same code as in the Solution to Exercise @q:iid_poisson_ambulance_b.\n\nbinWidth = giniGrid[2]- giniGrid[1]\n# first, sort the density values from highest to lowest\npostDensOrdered = sort(postDens, decreasing = TRUE)  \n# reorder the thetaValues so that they still match the density values\nginiOrdered = giniGrid[order(postDens, decreasing = TRUE)] \ncumsumPostDens = cumsum(binWidth*postDensOrdered) # posterior cdf \ninHPD = which(cumsumPostDens &lt; 0.95) # find highest pdf vals up to 95% quota.\nhpd = c(min(giniOrdered[inHPD]), max(giniOrdered[inHPD]))\nmessage(paste0(\"The 95% HPD interval for the Gini coefficient is (\", hpd[1], \",\", hpd[2],\")\"))\n\nThe 95% HPD interval for the Gini coefficient is (0.1611254211642,0.390431555867963)"
  },
  {
    "objectID": "exercises/ch4solutions.html",
    "href": "exercises/ch4solutions.html",
    "title": "Chapter 4 - Priors: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution.\n\nExercise 4.3\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\).\na) Show that Jeffreys’ prior is \\(p(\\theta)\\propto1/\\theta\\). Is it proper?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood is \\[\np(x_1,\\ldots,x_n\\mid\\theta) = \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^{n}e^{-\\theta\\sum_{i=1}^{n}x_{i}}\n\\] with log likelihood \\[\n\\log p(x_1,\\ldots,x_n\\mid\\theta)=n\\log\\theta-\\theta\\sum_{i=1}^{n}x_{i}\n\\] and first derivative \\[\n\\frac{d\\log p(x_1,\\ldots,x_n\\mid\\theta)}{d\\theta}=\\frac{n}{\\theta}-\\sum_{i=1}^{n}x_{i}\n\\] and second derivative \\[\n\\frac{d^{2}\\log p(x_1,\\ldots,x_n\\mid\\theta)}{d\\theta^{2}}=-\\frac{n}{\\theta^{2}}\n\\] so the Fisher information is \\[\nI(\\theta)=E\\left(-\\frac{d^{2}\\log p(x_1,\\ldots,x_n\\mid\\theta)}{d\\theta^{2}}\\right)=\\frac{n}{\\theta^{2}}\n\\] and Jeffreys’ prior is therefore \\[\np(\\theta) = I(\\theta)^{1/2} = \\frac{\\sqrt{n}}{\\theta} \\propto \\frac{1}{\\theta}.\n\\] Jeffreys’ prior is not proper for the iid exponential model since the integral of the prior diverges. Since \\(1/\\theta &gt; 0\\) for all \\(\\theta\\), we can see that the integral diverges as follows: \\[\n\\int_{0}^{\\infty}\\frac{1}{\\theta}d\\theta \\, \\geq\n\\int_{1}^{\\infty}\\frac{1}{\\theta}d\\theta :=   \n\\lim_{a\\rightarrow \\infty} \\int_1^a \\frac{1}{\\theta}d\\theta\n= \\lim_{a\\rightarrow \\infty} [\\log(\\theta)]_1^a = \\lim_{a \\rightarrow \\infty}\\big(\\log(a) - 0 \\big) = \\infty.\n\\]\n\n\n\nb) Derive the posterior of \\(\\theta\\) for Jeffreys’ prior. Is it proper?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe posterior distribution is obtained in the usual way with Bayes’ theorem \\[\np(\\theta \\vert x_1,\\ldots,x_n) = \\theta^{n}e^{-\\theta\\sum_{i=1}^{n}x_{i}} \\cdot \\frac{1}{\\theta} \\propto \\theta^{n-1}e^{-\\theta\\sum_{i=1}^{n}x_{i}}\n\\] which is proportional to a \\(\\mathrm{Gamma}(n,\\sum_{i=1}^n x_i)\\) density. A Gamma density is proper when both its hyperparameters are strictly positive, i.e. for any \\(n \\geq 1\\) since all \\(x_i&gt;0\\) with probability one in a gamma distribution and any realized data will therefore have \\(\\sum_{i=1}^n x_i &gt; 0\\).\n\n\n\nc) Find a way to motivate the particular form of the Jeffreys prior as non-informative prior, in addition to its inherent invariance property.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne way to view the Jeffreys’ prior as non-informative is to consider the posterior from the conjugate prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\), which from Exercise @q:iid_exp_conj is known to be \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n, \\beta +\\sum_{i=1}^n x_i).\n\\] The conjugate prior can therefore be seen to contain the information equivalent to a prior sample of \\(\\alpha\\) (imaginary) observations with a data sum equal to \\(\\beta\\). Given the form of the Jeffreys’ posterior \\(\\mathrm{Gamma}(n,\\sum_{i=1}^n x_i)\\), we see that Jeffreys’ prior is equivalent to a prior sample of \\(\\alpha = 0\\) observations."
  },
  {
    "objectID": "exercises/ch4/jeffreys_expon.html",
    "href": "exercises/ch4/jeffreys_expon.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 4.3\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\).\na) Show that Jeffreys’ prior is \\(p(\\theta)\\propto1/\\theta\\). Is it proper?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood is \\[\np(x_1,\\ldots,x_n\\mid\\theta) = \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^{n}e^{-\\theta\\sum_{i=1}^{n}x_{i}}\n\\] with log likelihood \\[\n\\log p(x_1,\\ldots,x_n\\mid\\theta)=n\\log\\theta-\\theta\\sum_{i=1}^{n}x_{i}\n\\] and first derivative \\[\n\\frac{d\\log p(x_1,\\ldots,x_n\\mid\\theta)}{d\\theta}=\\frac{n}{\\theta}-\\sum_{i=1}^{n}x_{i}\n\\] and second derivative \\[\n\\frac{d^{2}\\log p(x_1,\\ldots,x_n\\mid\\theta)}{d\\theta^{2}}=-\\frac{n}{\\theta^{2}}\n\\] so the Fisher information is \\[\nI(\\theta)=E\\left(-\\frac{d^{2}\\log p(x_1,\\ldots,x_n\\mid\\theta)}{d\\theta^{2}}\\right)=\\frac{n}{\\theta^{2}}\n\\] and Jeffreys’ prior is therefore \\[\np(\\theta) = I(\\theta)^{1/2} = \\frac{\\sqrt{n}}{\\theta} \\propto \\frac{1}{\\theta}.\n\\] Jeffreys’ prior is not proper for the iid exponential model since the integral of the prior diverges. Since \\(1/\\theta &gt; 0\\) for all \\(\\theta\\), we can see that the integral diverges as follows: \\[\n\\int_{0}^{\\infty}\\frac{1}{\\theta}d\\theta \\, \\geq\n\\int_{1}^{\\infty}\\frac{1}{\\theta}d\\theta :=   \n\\lim_{a\\rightarrow \\infty} \\int_1^a \\frac{1}{\\theta}d\\theta\n= \\lim_{a\\rightarrow \\infty} [\\log(\\theta)]_1^a = \\lim_{a \\rightarrow \\infty}\\big(\\log(a) - 0 \\big) = \\infty.\n\\]\n\n\n\nb) Derive the posterior of \\(\\theta\\) for Jeffreys’ prior. Is it proper?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe posterior distribution is obtained in the usual way with Bayes’ theorem \\[\np(\\theta \\vert x_1,\\ldots,x_n) = \\theta^{n}e^{-\\theta\\sum_{i=1}^{n}x_{i}} \\cdot \\frac{1}{\\theta} \\propto \\theta^{n-1}e^{-\\theta\\sum_{i=1}^{n}x_{i}}\n\\] which is proportional to a \\(\\mathrm{Gamma}(n,\\sum_{i=1}^n x_i)\\) density. A Gamma density is proper when both its hyperparameters are strictly positive, i.e. for any \\(n \\geq 1\\) since all \\(x_i&gt;0\\) with probability one in a gamma distribution and any realized data will therefore have \\(\\sum_{i=1}^n x_i &gt; 0\\).\n\n\n\nc) Find a way to motivate the particular form of the Jeffreys prior as non-informative prior, in addition to its inherent invariance property.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne way to view the Jeffreys’ prior as non-informative is to consider the posterior from the conjugate prior \\(\\theta \\sim \\mathrm{Gamma}(\\alpha,\\beta)\\), which from Exercise @q:iid_exp_conj is known to be \\[\n\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n, \\beta +\\sum_{i=1}^n x_i).\n\\] The conjugate prior can therefore be seen to contain the information equivalent to a prior sample of \\(\\alpha\\) (imaginary) observations with a data sum equal to \\(\\beta\\). Given the form of the Jeffreys’ posterior \\(\\mathrm{Gamma}(n,\\sum_{i=1}^n x_i)\\), we see that Jeffreys’ prior is equivalent to a prior sample of \\(\\alpha = 0\\) observations."
  },
  {
    "objectID": "exercises/ch5solutions.html",
    "href": "exercises/ch5solutions.html",
    "title": "Chapter 5 - Linear Regression: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution."
  },
  {
    "objectID": "exercises/ch6solutions.html",
    "href": "exercises/ch6solutions.html",
    "title": "Chapter 6 - Prediction and Decision making: Exercise solutions",
    "section": "",
    "text": "Click on the arrow to see a solution.\n\nExercise 6.1\nLet \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\mathrm{Bern}(\\theta)\\), with a \\(\\mathrm{Beta}(\\alpha,\\beta)\\) prior for \\(\\theta\\).\na) Derive the predictive distribution for \\(X_{n+1}\\) conditional on having observed \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution TBD\n\n\n\n\n\n\n\n\n\nb) You need to decide if you bring your umbrella during your daily walk. It has rained on two days during the last ten days, and you assess those ten days to be representative of the weather today, the 11th day. Your utility for the action-state combinations are given in the table below. Assume a \\(\\theta \\sim \\mathrm{Beta}(1,1)\\) prior. Compute the Bayesian decision.\n\n\n\n\n\n\nSolution TBD\n\n\n\n\n\n\n\n\n\nc) How sensitive is your decision in b) to the changes in the prior hyperparameters, \\(\\alpha\\) and \\(\\beta\\)?\n\n\n\n\n\n\nSolution TBD\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.2\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\), and assume the conjugate \\(\\mathrm{Gamma}(\\alpha,\\beta)\\) prior for \\(\\theta\\).\n a) Derive the predictive distribution for a new observation \\(\\tilde{X}_{n+1}\\) conditional on having observed \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe predictive distribution for the next observation \\(\\tilde{X}_{n+1}\\) is \\[\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) = \\int p(\\tilde{x}_{n+1} \\vert \\theta) p(\\theta \\vert x_1,\\ldots,x_n) d\\theta\n\\] where \\(\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n, \\beta + \\sum_{i=1}^n x_i)\\) and \\(p(\\tilde{x}_{n+1} \\vert \\theta)\\) is the pdf of the exponential distribution. So, \\[\n\\begin{aligned}\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) &= \\int \\theta e^{-\\theta\\tilde{x}_{n+1}}\\cdot\\frac{\\left(\\beta+n\\bar{x}\\right)^{\\alpha+n}}{\\Gamma(\\alpha+n)}\\theta^{\\alpha+n-1}e^{-(\\beta+n\\bar{x})\\theta}d\\theta \\\\\n&= \\frac{\\left(\\beta+n\\bar{x}\\right)^{\\alpha+n}}{\\Gamma(\\alpha+n)}\\int \\theta^{(\\alpha+n + 1) -1}e^{-(\\beta+n\\bar{x} + \\tilde{x}_{n+1})\\theta}d\\theta \\\\\n\\end{aligned}\n\\] The integral \\(\\int \\theta^{(\\alpha+n + 1) -1}e^{-(\\beta+n\\bar{x} + \\tilde{x}_{n+1})\\theta}d\\theta\\) is the integral of the unnormalized density function of the \\(\\mathrm{Gamma}(\\alpha+n + 1, \\beta+n\\bar{x} + \\tilde{x}_{n+1})\\) distribution. Hence the integral can be directly obtained from the normalizing constant of this distribution and we obtain \\[\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) = \\frac{\\left(\\beta+n\\bar{x}\\right)^{\\alpha+n}}{\\Gamma(\\alpha+n)}\\frac{\\Gamma(\\alpha+n+1)}{(\\beta+n\\bar{x} + \\tilde{x}_{n+1})^{\\alpha+n+1}}\n\\] This density is not among the really famous ones, but it has a name, the Compound Gamma distribution; see the Compound Gamma distribution widget, if you are curious. The distribution in the widget is actually a little more general, but set the hyperparameter \\(\\kappa=1\\) to get the distribution here. Hence, using the notation in the widget, the predictive distribution is \\[\n\\tilde{X}_{n+1} \\sim \\mathrm{CompoundGamma}(\\alpha+n, \\beta + n\\bar{x}, 1)\n\\] Note that the density expression above is a little messy mainly because of the normalizing constant. We can remove normalization constants and write \\[\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) \\propto \\frac{1}{(\\beta+n\\bar{x} + \\tilde{x}_{n+1})^{\\alpha+n+1}}\n\\] where it is important to rememeber that we are here interested in the distribution of \\(\\tilde{x}_{n+1}\\), so all terms involving \\(\\tilde{x}_{n+1}\\) must be kept.\n\n\n\nb) A medium size software project is interested in predicting the time until the next release. The five most recent releases took: \\(1.58, 7.02, 8.28, 4.67\\) and \\(4.05\\) months to complete. Previous experience from many simular projects suggests that these completion times can be modelled by an exponential distribution with parameter \\(\\theta\\). The project leader believes that the average completion time is around 3 months, but she is not very certain about this, so you decide to use a \\(\\mathrm{Gamma}(1/3,1)\\) prior for \\(\\theta\\) (recall that the mean of \\(\\mathrm{Expon}(\\theta)\\) is \\(1/\\theta\\) not \\(\\theta\\)).\nUse simulation to compute the predictive distribution for the time until the next release given the observed data, and plot a histogram. Overlay a density curve using the result in a). The company could go bankrupt if the next release takes more than 18 months to complete Use the draws from the predictive distribution to compute the probability of this catastrophic event.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe following code plots the predictive density using both simulation and with the analytical result from a).\nThe simulation from the predictive distribution is performed by :\n\nfirst simulating \\(m\\) draws from the posterior of the parameter \\(\\theta \\vert x_1,\\ldots,x_5 \\sim \\mathrm{Gamma}(\\alpha+n, \\beta + n\\bar{x})\\), and then\nfor each parameter draw \\(\\theta^{(i)}\\) simulating predictive draws \\(\\tilde{X}_6^{(i)}\\) from the exponential model \\(\\tilde{X}_6^{(i)} \\vert x_1,\\ldots,x_5 \\sim \\mathrm{Expon}(\\theta^{(i)})\\) for \\(i=1,\\ldots,m\\).\n\n\n# Set up data and compute posterior hyperparameters\nx = c(1.58, 7.02, 8.28, 4.67, 4.05)\nn = length(x)\nxbar = mean(x)\nalphaPrior = 1/3\nbetaPrior = 1\nalphaPost = alphaPrior + n\nbetaPost = betaPrior + n*xbar\n\n# Simulate from predictive distribution and plot histogram\nnSim = 10000\nthetaDraws = rgamma(nSim, alphaPost, betaPost)\nxPredDraws = rexp(nSim, thetaDraws)\nhist(xPredDraws, 100, col = \"cornsilk\", freq = FALSE,  xlim = c(0,50),\n     main = \"Predictive density\", \n     xlab = \"Months until next release\", ylab = \"Preditive density\")\n\n# Defining the general Compound-Gamma density even though we have kappa = 1\ndCompoundGamma &lt;- function(x, alpha, beta_, kappa){\n  dens = (beta_^alpha/gamma(alpha)) * \n    (gamma(alpha + kappa)/gamma(kappa)) * \n    x^(kappa-1)/(beta_ + x)^(alpha + kappa)\n  return(dens)\n}\n\n# Overlaying the analytical density for the Compound Gamma distribution\nxPredGrid = seq(0, 30, length = 1000)\nlines(xPredGrid, dCompoundGamma(xPredGrid, alphaPost, betaPost, 1), \n      lwd = 3, col = \"indianred\")\n\n\n\n\n\n\n\n\nThe predictive probability that the release takes more than 18 months is not very likely, but cannot be completely ruled out:\n\nmean(xPredDraws &gt; 18)\n\n[1] 0.0626\n\n\n\n\n\n\n\n\nExercise 6.3\nA city considers building a new road bridge for a total cost of \\(a\\) dollars. The weight that the bridge can hold (\\(y\\)) at any given time is related to the build cost (\\(a\\)) as follows \\[\ny = 10a\n\\] The city has collected data on the maximal weight on the road during five different days: \\(y_{1}=190,y_{2}=193,y_{3}=192,y_{4}=197\\) and \\(y_{5}=188\\). Assume the model \\(Y_{1},...,Y_{5}\\vert\\theta \\sim N(\\theta,\\sigma^{2})\\), where \\(\\sigma^{2}\\) is assumed known at \\(\\sigma^{2}=10^{2}\\). Assume a \\(N(200,10^2)\\) prior for \\(\\theta\\).\na) Simulate from the predictive distribution of the maximal weight on . Plot a histogram of the simulated values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe predictive distribution for the maximal weight on a future day is \\[\n\\tilde{X} \\sim N(\\mu_n, \\sigma^2 + \\tau_n^2),\n\\] where \\(\\mu_n\\) and \\(\\tau_n^2\\) are computed using the formula for the posterior in the iid normal model with known variance in Chapter 2:\n\ny = c(190,193, 192, 197, 188)\nn = length(y)\nyBar = mean(y)\nsigma = 10\nmu0 = 200\ntau0 = 10\nw = (n/sigma^2)/(n/sigma^2 + 1/tau0^2)\nmun = w*yBar + (1-w)*mu0\ntaun2 = 1/( n/sigma^2 +  1/tau0^2 )\npredDraws = rnorm(10000, mun, sqrt(sigma^2 + taun2))\nhist(predDraws, xlim = c(150,250),\n     xlab = \"Max weight on a future day\", ylab = \"Predictive density\", \n     main = \"Predictive density max weight - single day\", col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n\n\n\nb) Simulate from the predictive distribution of the maximal weight \\(\\tilde{Y}\\) over . Plot a histogram of the simulated values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo obtain the predictive distribution for the maximal weight over a whole year we just simulate max weight for \\(365\\) independent days and take the maximum over all days.\n\nnSim = 10000\nmaxWeightYear = rep(0, nSim)\nfor (i in 1:nSim){\n  maxWeightYear[i] = max(rnorm(365, mun, sqrt(sigma^2 + taun2)))\n}\npar(mfrow = c(2,1))\nhist(predDraws, xlim = c(150,250),\n     xlab = \"Max weight on a future day\", ylab = \"Predictive density\", \n     main = \"Predictive density max weight - single day\", col = \"cornflowerblue\")\nhist(maxWeightYear, , xlim = c(150,250), xlab = \"Max weight over a full year day\", \n     ylab = \"Predictive density\", \n     main = \"Predictive density max weight - whole year\", col = \"cornsilk\")\n\n\n\n\n\n\n\n\n\n\n\nc) The loss function for the building project is \\[\nL(a, \\tilde{Y}) =\n\\begin{cases}\na & \\text{if } \\tilde{Y} \\leq 10a \\,\\,\\text{ (bridge holds up)}\\\\\na + 100 & \\text{if } \\tilde{Y} &gt; 10a \\,\\,\\text{ (bridge collapses)}\n\\end{cases}\n\\] where \\(\\tilde{Y}\\) is the maximal weight on the bridge during the first year. Hence, we assume that there is no loss after the first year, regardless of what happens. Determine the optimal build cost (\\(a\\)) using a Bayesian approach.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe choose the action (building cost \\(a\\), hence the action space is all \\(a&gt;0\\)) that minimizes the expected loss, \\(EL(a) = \\mathbb{E}(L(a,\\tilde{Y}))\\). The only uncertain factor is maximal weight in the coming year: \\(\\tilde{Y}\\). \\[\n\\begin{aligned}\nEL(a) = E\\left[L(c,\\tilde{Y} )\\right] & = a\\cdot\\mathrm{Pr}(\\text{bridge holds}\\vert a,y_{1},...,y_{5})+(100+a)\\cdot\\mathrm{Pr}(\\text{bridge collapses}\\vert a,y_{1},...,y_{5}) \\\\\n& = a\\cdot\\mathrm{Pr}(\\tilde{Y} \\leq 10a \\vert a,y_{1},...,y_{5})+(100+a)\\cdot\\mathrm{Pr}(\\tilde{Y} &gt; 10a \\vert a,y_{1},...,y_{5})\n\\end{aligned}\n\\] We can compute the expected loss for a range of actions \\(a\\) using the simulated draws of the maximal weight during the coming year, \\(\\tilde{Y}\\), from b). The expected loss (EL) is plotted as function of \\(a\\) below.\n\naGrid = seq(0, 50, length = 1000)\nEL = rep(length(aGrid))\nfor (i in 1:length(aGrid)){\n  a = aGrid[i]\n  EL[i] = a*mean(maxWeightYear &lt;= 10*a) + (100+a)*mean(maxWeightYear &gt; 10*a)\n}\nplot(aGrid, EL, xlab = \"building cost, a\", ylab = \"Expected loss\", type = \"l\", \n     lwd = 3, col = \"indianred\")\nabline(v = aGrid[which.min(EL)], lty = \"dotted\")\n\n\n\n\n\n\n\n\nThe optimal building cost 23.974 is marked out by a dotted vertical line."
  },
  {
    "objectID": "exercises/ch6/bern_prediction.html",
    "href": "exercises/ch6/bern_prediction.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 6.1\nLet \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\mathrm{Bern}(\\theta)\\), with a \\(\\mathrm{Beta}(\\alpha,\\beta)\\) prior for \\(\\theta\\).\na) Derive the predictive distribution for \\(X_{n+1}\\) conditional on having observed \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution TBD\n\n\n\n\n\n\n\n\n\nb) You need to decide if you bring your umbrella during your daily walk. It has rained on two days during the last ten days, and you assess those ten days to be representative of the weather today, the 11th day. Your utility for the action-state combinations are given in the table below. Assume a \\(\\theta \\sim \\mathrm{Beta}(1,1)\\) prior. Compute the Bayesian decision.\n\n\n\n\n\n\nSolution TBD\n\n\n\n\n\n\n\n\n\nc) How sensitive is your decision in b) to the changes in the prior hyperparameters, \\(\\alpha\\) and \\(\\beta\\)?\n\n\n\n\n\n\nSolution TBD"
  },
  {
    "objectID": "exercises/ch6/iid_expon_pred.html",
    "href": "exercises/ch6/iid_expon_pred.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 6.2\nLet \\(X_1,\\ldots,X_n \\vert \\theta \\overset{\\mathrm{iid}}{\\sim} \\mathrm{Expon}(\\theta)\\), and assume the conjugate \\(\\mathrm{Gamma}(\\alpha,\\beta)\\) prior for \\(\\theta\\).\n a) Derive the predictive distribution for a new observation \\(\\tilde{X}_{n+1}\\) conditional on having observed \\(x_1,\\ldots,x_n\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe predictive distribution for the next observation \\(\\tilde{X}_{n+1}\\) is \\[\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) = \\int p(\\tilde{x}_{n+1} \\vert \\theta) p(\\theta \\vert x_1,\\ldots,x_n) d\\theta\n\\] where \\(\\theta \\vert x_1,\\ldots,x_n \\sim \\mathrm{Gamma}(\\alpha + n, \\beta + \\sum_{i=1}^n x_i)\\) and \\(p(\\tilde{x}_{n+1} \\vert \\theta)\\) is the pdf of the exponential distribution. So, \\[\n\\begin{aligned}\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) &= \\int \\theta e^{-\\theta\\tilde{x}_{n+1}}\\cdot\\frac{\\left(\\beta+n\\bar{x}\\right)^{\\alpha+n}}{\\Gamma(\\alpha+n)}\\theta^{\\alpha+n-1}e^{-(\\beta+n\\bar{x})\\theta}d\\theta \\\\\n&= \\frac{\\left(\\beta+n\\bar{x}\\right)^{\\alpha+n}}{\\Gamma(\\alpha+n)}\\int \\theta^{(\\alpha+n + 1) -1}e^{-(\\beta+n\\bar{x} + \\tilde{x}_{n+1})\\theta}d\\theta \\\\\n\\end{aligned}\n\\] The integral \\(\\int \\theta^{(\\alpha+n + 1) -1}e^{-(\\beta+n\\bar{x} + \\tilde{x}_{n+1})\\theta}d\\theta\\) is the integral of the unnormalized density function of the \\(\\mathrm{Gamma}(\\alpha+n + 1, \\beta+n\\bar{x} + \\tilde{x}_{n+1})\\) distribution. Hence the integral can be directly obtained from the normalizing constant of this distribution and we obtain \\[\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) = \\frac{\\left(\\beta+n\\bar{x}\\right)^{\\alpha+n}}{\\Gamma(\\alpha+n)}\\frac{\\Gamma(\\alpha+n+1)}{(\\beta+n\\bar{x} + \\tilde{x}_{n+1})^{\\alpha+n+1}}\n\\] This density is not among the really famous ones, but it has a name, the Compound Gamma distribution; see the Compound Gamma distribution widget, if you are curious. The distribution in the widget is actually a little more general, but set the hyperparameter \\(\\kappa=1\\) to get the distribution here. Hence, using the notation in the widget, the predictive distribution is \\[\n\\tilde{X}_{n+1} \\sim \\mathrm{CompoundGamma}(\\alpha+n, \\beta + n\\bar{x}, 1)\n\\] Note that the density expression above is a little messy mainly because of the normalizing constant. We can remove normalization constants and write \\[\np(\\tilde{x}_{n+1} \\vert x_1,\\ldots,x_n) \\propto \\frac{1}{(\\beta+n\\bar{x} + \\tilde{x}_{n+1})^{\\alpha+n+1}}\n\\] where it is important to rememeber that we are here interested in the distribution of \\(\\tilde{x}_{n+1}\\), so all terms involving \\(\\tilde{x}_{n+1}\\) must be kept.\n\n\n\nb) A medium size software project is interested in predicting the time until the next release. The five most recent releases took: \\(1.58, 7.02, 8.28, 4.67\\) and \\(4.05\\) months to complete. Previous experience from many simular projects suggests that these completion times can be modelled by an exponential distribution with parameter \\(\\theta\\). The project leader believes that the average completion time is around 3 months, but she is not very certain about this, so you decide to use a \\(\\mathrm{Gamma}(1/3,1)\\) prior for \\(\\theta\\) (recall that the mean of \\(\\mathrm{Expon}(\\theta)\\) is \\(1/\\theta\\) not \\(\\theta\\)).\nUse simulation to compute the predictive distribution for the time until the next release given the observed data, and plot a histogram. Overlay a density curve using the result in a). The company could go bankrupt if the next release takes more than 18 months to complete Use the draws from the predictive distribution to compute the probability of this catastrophic event.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe following code plots the predictive density using both simulation and with the analytical result from a).\nThe simulation from the predictive distribution is performed by :\n\nfirst simulating \\(m\\) draws from the posterior of the parameter \\(\\theta \\vert x_1,\\ldots,x_5 \\sim \\mathrm{Gamma}(\\alpha+n, \\beta + n\\bar{x})\\), and then\nfor each parameter draw \\(\\theta^{(i)}\\) simulating predictive draws \\(\\tilde{X}_6^{(i)}\\) from the exponential model \\(\\tilde{X}_6^{(i)} \\vert x_1,\\ldots,x_5 \\sim \\mathrm{Expon}(\\theta^{(i)})\\) for \\(i=1,\\ldots,m\\).\n\n\n# Set up data and compute posterior hyperparameters\nx = c(1.58, 7.02, 8.28, 4.67, 4.05)\nn = length(x)\nxbar = mean(x)\nalphaPrior = 1/3\nbetaPrior = 1\nalphaPost = alphaPrior + n\nbetaPost = betaPrior + n*xbar\n\n# Simulate from predictive distribution and plot histogram\nnSim = 10000\nthetaDraws = rgamma(nSim, alphaPost, betaPost)\nxPredDraws = rexp(nSim, thetaDraws)\nhist(xPredDraws, 100, col = \"cornsilk\", freq = FALSE,  xlim = c(0,50),\n     main = \"Predictive density\", \n     xlab = \"Months until next release\", ylab = \"Preditive density\")\n\n# Defining the general Compound-Gamma density even though we have kappa = 1\ndCompoundGamma &lt;- function(x, alpha, beta_, kappa){\n  dens = (beta_^alpha/gamma(alpha)) * \n    (gamma(alpha + kappa)/gamma(kappa)) * \n    x^(kappa-1)/(beta_ + x)^(alpha + kappa)\n  return(dens)\n}\n\n# Overlaying the analytical density for the Compound Gamma distribution\nxPredGrid = seq(0, 30, length = 1000)\nlines(xPredGrid, dCompoundGamma(xPredGrid, alphaPost, betaPost, 1), \n      lwd = 3, col = \"indianred\")\n\n\n\n\n\n\n\n\nThe predictive probability that the release takes more than 18 months is not very likely, but cannot be completely ruled out:\n\nmean(xPredDraws &gt; 18)\n\n[1] 0.0634"
  },
  {
    "objectID": "exercises/ch6/salescampaign.html",
    "href": "exercises/ch6/salescampaign.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 6.3\nLet \\(X_i\\) be the number of sales of a product on month \\(i\\), and let \\(X_{1},\\ldots,X_{n}\\overset{iid}{\\sim}\\mathrm{N}(\\theta,\\sigma^{2})\\) be the (approximate) distribution for the sales. Assume that \\(\\sigma^{2}=25^{2}\\) is known and use the prior \\(\\theta\\sim N(200,50^2)\\) for the average sales.\na) We have observed a dataset with \\(n=5\\) observations and obtained \\(\\bar{x}=320.4\\). Compute the predictive distribution for \\(x_6\\)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nb) The company has the choice of performing a marketing campaign for their product. The marketing campaign costs \\(300\\) and is believed to increase sales by \\(20\\%\\) compared to when no campaign is performed. The company sells the product for \\(p=10\\) dollar and the cost of producing the product is \\(q=5\\) dollar. There are no fixed production costs. Assume that the company’s utility is described by \\(U(y)=1-\\exp(-y/1000)\\), where \\(y\\) is the total profit from sales in the next month. Should the company perform the marketing campaign? : the expected value of the exponential function of a normal random variable \\(S\\sim N(\\mu,\\sigma^{2})\\) is \\(\\mathbb{E}(\\exp(S))=\\exp(\\mu+\\sigma^{2}/2)\\).\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "exercises/ch6/bridgedecision.html",
    "href": "exercises/ch6/bridgedecision.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Exercise 6.3\nA city considers building a new road bridge for a total cost of \\(a\\) dollars. The weight that the bridge can hold (\\(y\\)) at any given time is related to the build cost (\\(a\\)) as follows \\[\ny = 10a\n\\] The city has collected data on the maximal weight on the road during five different days: \\(y_{1}=190,y_{2}=193,y_{3}=192,y_{4}=197\\) and \\(y_{5}=188\\). Assume the model \\(Y_{1},...,Y_{5}\\vert\\theta \\sim N(\\theta,\\sigma^{2})\\), where \\(\\sigma^{2}\\) is assumed known at \\(\\sigma^{2}=10^{2}\\). Assume a \\(N(200,10^2)\\) prior for \\(\\theta\\).\na) Simulate from the predictive distribution of the maximal weight on . Plot a histogram of the simulated values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe predictive distribution for the maximal weight on a future day is \\[\n\\tilde{X} \\sim N(\\mu_n, \\sigma^2 + \\tau_n^2),\n\\] where \\(\\mu_n\\) and \\(\\tau_n^2\\) are computed using the formula for the posterior in the iid normal model with known variance in Chapter 2:\n\ny = c(190,193, 192, 197, 188)\nn = length(y)\nyBar = mean(y)\nsigma = 10\nmu0 = 200\ntau0 = 10\nw = (n/sigma^2)/(n/sigma^2 + 1/tau0^2)\nmun = w*yBar + (1-w)*mu0\ntaun2 = 1/( n/sigma^2 +  1/tau0^2 )\npredDraws = rnorm(10000, mun, sqrt(sigma^2 + taun2))\nhist(predDraws, xlim = c(150,250),\n     xlab = \"Max weight on a future day\", ylab = \"Predictive density\", \n     main = \"Predictive density max weight - single day\", col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\n\n\n\nb) Simulate from the predictive distribution of the maximal weight \\(\\tilde{Y}\\) over all of the coming 365 days. Plot a histogram of the simulated values.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo obtain the predictive distribution for the maximal weight over a whole year we just simulate max weight for \\(365\\) independent days and take the maximum over all days.\n\nnSim = 10000\nmaxWeightYear = rep(0, nSim)\nfor (i in 1:nSim){\n  maxWeightYear[i] = max(rnorm(365, mun, sqrt(sigma^2 + taun2)))\n}\npar(mfrow = c(2,1))\nhist(predDraws, xlim = c(150,250),\n     xlab = \"Max weight on a future day\", ylab = \"Predictive density\", \n     main = \"Predictive density max weight - single day\", col = \"cornflowerblue\")\nhist(maxWeightYear, , xlim = c(150,250), xlab = \"Max weight over a full year day\", \n     ylab = \"Predictive density\", \n     main = \"Predictive density max weight - whole year\", col = \"cornsilk\")\n\n\n\n\n\n\n\n\n\n\n\nc) The loss function for the building project is \\[\nL(a, \\tilde{Y}) =\n\\begin{cases}\na & \\text{if } \\tilde{Y} \\leq 10a \\,\\,\\text{ (bridge holds up)}\\\\\na + 100 & \\text{if } \\tilde{Y} &gt; 10a \\,\\,\\text{ (bridge collapses)}\n\\end{cases}\n\\] where \\(\\tilde{Y}\\) is the maximal weight on the bridge during the first year. Hence, we assume that there is no loss after the first year, regardless of what happens. Determine the optimal build cost (\\(a\\)) using a Bayesian approach.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe choose the action (building cost \\(a\\), hence the action space is all \\(a&gt;0\\)) that minimizes the expected loss, \\(EL(a) = \\mathbb{E}(L(a,\\tilde{Y}))\\). The only uncertain factor is maximal weight in the coming year: \\(\\tilde{Y}\\). \\[\n\\begin{aligned}\nEL(a) = E\\left[L(c,\\tilde{Y} )\\right] & = a\\cdot\\mathrm{Pr}(\\text{bridge holds}\\vert a,y_{1},...,y_{5})+(100+a)\\cdot\\mathrm{Pr}(\\text{bridge collapses}\\vert a,y_{1},...,y_{5}) \\\\\n& = a\\cdot\\mathrm{Pr}(\\tilde{Y} \\leq 10a \\vert a,y_{1},...,y_{5})+(100+a)\\cdot\\mathrm{Pr}(\\tilde{Y} &gt; 10a \\vert a,y_{1},...,y_{5})\n\\end{aligned}\n\\] We can compute the expected loss for a range of actions \\(a\\) using the simulated draws of the maximal weight during the coming year, \\(\\tilde{Y}\\), from b). The expected loss (EL) is plotted as function of \\(a\\) below.\n\naGrid = seq(0, 50, length = 1000)\nEL = rep(length(aGrid))\nfor (i in 1:length(aGrid)){\n  a = aGrid[i]\n  EL[i] = a*mean(maxWeightYear &lt;= 10*a) + (100+a)*mean(maxWeightYear &gt; 10*a)\n}\nplot(aGrid, EL, xlab = \"building cost, a\", ylab = \"Expected loss\", type = \"l\", \n     lwd = 3, col = \"indianred\")\nabline(v = aGrid[which.min(EL)], lty = \"dotted\")\n\n\n\n\n\n\n\n\nThe optimal building cost 23.924 is marked out by a dotted vertical line."
  }
]