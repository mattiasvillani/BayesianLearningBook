[
  {
    "objectID": "notebooks/SpamBern/SpamBernR.html",
    "href": "notebooks/SpamBern/SpamBernR.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Analyzing email spam data with a Bernoulli model\na notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nThe SpamBase dataset from the UCI repository consists of \\(n=4601\\) emails that have been manually classified as spam (junk email) or ham (non-junk email).\nThe dataset also contains a vector of covariates/features for each email, such as the number of capital letters or $-signs; this information can be used to build a spam filter that automatically separates spam from ham.\nThis notebook analyzes only the proportion of spam emails without using the covariates.\n\n\nGetting started\nFirst, load libraries and setting up colors.\n\noptions(repr.plot.width=16, repr.plot.height=5, lwd = 4)\nlibrary(\"RColorBrewer\") # for pretty colors\nlibrary(\"tidyverse\")    # for string interpolation to print variables in plots.\nlibrary(\"latex2exp\")    # the TeX() function makes it possible to print latex math\ncolors = brewer.pal(12, \"Paired\")[c(1,2,7,8,3,4,5,6,9,10)];\n\n\n\nData\n\ndata = read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\", sep=\",\", header = TRUE)\nspam = data$X1 # This is the binary data where spam = 1, ham = 0.\nn = length(spam)\nspam = sample(spam, size = n) # Randomly shuffle the data.\n\n\n\nModel, Prior and Posterior\nModel \\[ X_1,\\ldots,X_n | \\theta \\sim \\mathrm{Bern}(\\theta)\\]\nPrior \\[\\theta\\sim\\mathrm{Beta}(\\alpha,\\beta)\\]\nPosterior \\[\\theta | x_1,\\ldots,x_n \\sim\\mathrm{Beta}(\\alpha+s,\\beta+f),\\]\nwhere \\(s=\\sum_{i=1}^n\\) is the number of ‘successes’ (spam) and \\(f=n-s\\) is the number of ‘failures’ (ham).\nLet us define a function that computes the posterior and plots it.\n\nBernPost <- function(x, alphaPrior, betaPrior, legend = TRUE){\n    thetaGrid = seq(0,1, length = 1000)\n    n = length(x)\n    s = sum(x)\n    f = n - s\n    alphaPost = alphaPrior + s\n    betaPost = betaPrior + f\n    priorPDF = dbeta(thetaGrid, alphaPrior, betaPrior)\n    normLikePDF = dbeta(thetaGrid, s + 1, f + 1) # Trick to get the normalized likelihood\n    postPDF = dbeta(thetaGrid, alphaPost, betaPost)\n    \n    plot(1, type=\"n\", axes=FALSE, xlab = expression(theta), ylab = \"\", \n         xlim=c(min(thetaGrid),max(thetaGrid)), \n         ylim = c(0,max(priorPDF,postPDF,normLikePDF)), \n         main = TeX(sprintf(\"Prior: $\\\\mathrm{Beta}(\\\\alpha = %0.0f, \\\\beta = %0.0f)\", alphaPrior, betaPrior)))\n    axis(side = 1)\n    lines(thetaGrid, priorPDF, type = \"l\", lwd = 4, col = colors[6])\n    lines(thetaGrid, normLikePDF, lwd = 4, col = colors[2])\n    lines(thetaGrid, postPDF, lwd = 4, col = colors[4])\n    if (legend){\n        legend(x = \"topleft\", inset=.05,\n           legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\"),  \n           lty = c(1, 1, 1), pt.lwd = c(3, 3, 3), \n           col = c(colors[6], colors[2], colors[4]))\n    }\n    cat(\"Posterior mean is \", round(alphaPost/(alphaPost + betaPost),3), \"\\n\")\n    cat(\"Posterior standard deviation is \", round(sqrt(  alphaPost*betaPost/( (alphaPost+betaPost)^2*(alphaPost+betaPost+1))),3), \"\\n\")\n    return(list(\"alphaPost\" = alphaPrior + s, \"betaPost\" = betaPrior + f))\n}\n\nLet start by analyzing only the first 10 data points.\n\nn = 10\nx = spam[1:n]\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nSince we only have \\(n=10\\) data points, the posteriors for the three different priors differ a lot. Priors matter when the data are weak. Let’s try with the \\(n=100\\) first observations.\n\nn = 100\nx = spam[1:n]\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nThe effect of the prior is now almost gone. Finally let’s use all \\(n=4601\\) observations in the dataset:\n\nx = spam\npar(mfrow = c(1,3))\npost = BernPost(x, alphaPrior = 1, betaPrior = 5, legend = TRUE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 5, legend = FALSE)\npost = BernPost(x, alphaPrior = 5, betaPrior = 1, legend = FALSE)\n\nWe see two things: * The effect of the prior is completely gone. All three prior give identical posteriors. We have reached a subjective consensus among the three persons. * We are quite sure now that the spam probability \\(\\theta\\) is around \\(0.4\\).\nA later notebook will re-analyze this data using for example logistic regression."
  },
  {
    "objectID": "notebooks/SurveyMultinomial/multinomial.html",
    "href": "notebooks/SurveyMultinomial/multinomial.html",
    "title": "Bayesian analysis of multinomial data",
    "section": "",
    "text": "Prepared for the course: Bayesian Learning Author: Mattias Villani, Stockholm and Linköping University, http://mattiasvillani.com\n\nData\nA company has conduct a survey of mobile phone usage. 513 participants were asked the question: ‘What kind of mobile phone do you main use?’ with the four options:\n\niPhone\nAndroid Phone\nWindows Phone\nOther\n\nThe responses in the four categories were: 180, 230, 62, 41.\n\n\nModel\n\\[\n(y_1,\\ldots,y_4) \\vert \\theta_1,\\ldots,\\theta_4 \\sim \\mathrm{multinomial}(\\theta_1,\\ldots,\\theta_4)\n\\]\n\n\nPrior\nThe conjugate prior for multinomial data is the Dirichlet prior.\n\\[\n(\\theta_1,\\ldots,\\theta_K) \\sim \\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_K),\n\\] where the \\(\\alpha_k\\) are positive hyperparameters such that \\(\\mathbb{E}(\\theta_k) = \\alpha_k /\\sum_{j=1}^K \\alpha_j\\). Also, the sum of the \\(\\alpha\\)’s, \\(\\sum_{k=1}^K \\alpha_j\\), determines the precision (inverse variance) of the Dirichlet distribution. We will determine the prior hyperparameters from data from a similar survey that was conducted four year ago. The proportions in the four categories back then were: 30%, 30%, 20% and 20%. This was a large survey, but since time has passed and user patterns most likely has changed, we value the information in this older survey as being equivalent to a survey with only 50 participants. This gives us the prior: \\[\n(\\theta_1,\\ldots,\\theta_4) \\sim \\mathrm{Dirichlet}(\\alpha_1 = 15,\\alpha_2 = 15,\\alpha_3 = 10,\\alpha_4=10)\n\\]\nnote that \\(\\mathbb{E}(\\theta_1) = 15/50 = 0.3\\) and so on, so the prior mean is set equal to the proportions from the older survey. Also, \\(\\sum_{k=1}^4 \\alpha_k = 50\\), so the prior information is equivalent to a survey based on 50 respondents, as required.\n\n\nPosterior\n\\[(\\theta_1,\\ldots,\\theta_K) \\vert \\mathbf{y} \\sim \\mathrm{Dirichlet}(\\alpha_1 + y_1,\\ldots,\\alpha_K + y_K)\\] We can easily simulate from a Dirichlet distribution since if \\(x_k \\sim \\mathrm{Gamma}(\\alpha_k,1)\\) for \\(k=1,\\ldots,K\\), then the vector \\((z_1,\\ldots,z_K)\\) where \\(z_k = y_k /\\sum_{j=1}^K y_k\\), can be shown to follow the \\(\\mathrm{Dirichlet}(\\alpha_1,\\ldots,\\alpha_K)\\) distribution. The code below in a (inefficient) implementation of this simulator.\n\nSimDirichlet <- function(nIter, param){     \n  nCat <- length(param)     \n  thetaDraws <- as.data.frame(matrix(NA, nIter, nCat)) # Storage.   \n  for (j in 1:nCat){        \n    thetaDraws[,j] <- rgamma(nIter,param[j],1)  \n  }     \n  for (i in 1:nIter){       \n    thetaDraws[i,] = thetaDraws[i,]/sum(thetaDraws[i,]) \n  }     \n  return(thetaDraws) \n}\n\nOk, let’s use this piece of code on the survey data to obtain a sample from the posterior.\n\n# Data and prior\nset.seed(123) # Set the seed for reproducibility\ny <- c(180,230,62,41) # The cell phone survey data (K=4)\nalpha <- c(15,15,10,10) # Dirichlet prior hyperparameters \nnIter <- 1000 # Number of posterior draws\nthetaDraws <- SimDirichlet(nIter,y + alpha)\nnames(thetaDraws) <- c('theta1','theta2','theta3','theta4')\nhead(thetaDraws)\n\n     theta1    theta2    theta3     theta4\n1 0.3530702 0.4539865 0.1079301 0.08501313\n2 0.3676441 0.4020532 0.1406601 0.08964270\n3 0.3347276 0.4782798 0.1030357 0.08395696\n4 0.3395357 0.4487847 0.1217003 0.08997931\n5 0.3769253 0.4175829 0.1180137 0.08747814\n6 0.3607870 0.4186397 0.1275708 0.09300241\n\n\nSo thetaDraws is a nIter-by-4 matrix, where the \\(k\\)th column holds the posterior draws for \\(\\theta_k\\). We can now approximate the marginal posterior of \\(\\theta_k\\) by a histogram or a a kernel density estimate.\n\npar(mfrow = c(1,2)) # Splits the graphical window in four parts\nhist(thetaDraws[,1], breaks = 25, xlab = 'Fraction IPhone users', main ='iPhone', freq = FALSE)  \nlines(density(thetaDraws[,1]), col = \"blue\", lwd = 2)\nhist(thetaDraws[,2], breaks = 25, xlab = 'Fraction Android users', main ='Android', freq = FALSE)\nlines(density(thetaDraws[,2]), col = \"blue\", lwd = 2)\n\n\n\n\nWe can also compute the probability that Android has the largest market share by simply the proportion of posterior draws where Android is largest. You can for example see that this was the case in the first six draws shown above. This code does this calculation.\n\n# Computing the posterior probability that Android is the largest\nPrAndroidLargest <- sum(thetaDraws[,2]>apply(thetaDraws[,c(1,3,4)],1,max))/nIter\nmessage(paste('Pr(Android has the largest market share) = ', PrAndroidLargest))\n\nPr(Android has the largest market share) =  0.991"
  },
  {
    "objectID": "notebooks/SpamLogisticReg/SpamLogisticReg.html",
    "href": "notebooks/SpamLogisticReg/SpamLogisticReg.html",
    "title": "Posterior approximation - logistic regression",
    "section": "",
    "text": "Load packages\n\n# install.packages(\"mvtnorm\") \n# install.packages(\"RColorBrewer\") \nlibrary(mvtnorm) # package with multivariate normal density\nlibrary(RColorBrewer) # just some fancy colors for plotting\nprettyCol = brewer.pal(10,\"Paired\")\n\n\n\nSettings\n\nchooseCov <- c(1:16) # covariates to include in the model\ntau <- 10;           # Prior std beta~N(0,tau^2*I)\n\n\n\nReading data and setting up the prior\n\nData<-read.table(\"https://raw.githubusercontent.com/mattiasvillani/BayesLearnCourse/master/Notebooks/R/SpamReduced.dat\",header=TRUE) # Reduced spambase data (http://archive.ics.uci.edu/ml/datasets/Spambase/)\ncovNames <- names(Data)[2:length(names(Data))]; # Read off the covariate names\ny <- as.vector(Data[,1]); \nX <- as.matrix(Data[,2:17]);\nX <- X[,chooseCov];                             # Pick out the chosen covariates \ncovNames <- covNames[chooseCov];                # ... and their names\nnPara <- dim(X)[2];\n\n# Setting up the prior\nmu <- as.vector(rep(0,nPara)) # Prior mean vector\nSigma <- tau^2*diag(nPara);\n\n\nCoding up the log posterior function\n\nLogPostLogistic <- function(betaVect,y,X,mu,Sigma){\n  nPara <- length(betaVect);\n  linPred <- X%*%betaVect;\n  logLik <- sum( linPred*y -log(1 + exp(linPred)));\n  logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);\n  return(logLik + logPrior)\n}\n\n\n\nFinding the mode and observed information using optim\n\ninitVal <- as.vector(rep(0,nPara)); \nOptimResults<-optim(initVal,LogPostLogistic,gr=NULL,y,X,mu,Sigma,\n  method=c(\"BFGS\"), control=list(fnscale=-1),hessian=TRUE)\npostMode = OptimResults$par\npostCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covariance matrix\npostStd <- sqrt(diag(postCov)) # Computing approximate stdev\nnames(postMode) <- covNames      # Naming the coefficient by covariates\nnames(postStd) <- covNames # Naming the coefficient by covariates\n\n\n\n\nThe posterior mode is\n\nprint(postMode)\n\n          our          over        remove      internet          free \n 0.4182337582  1.1753728476  2.9209159589  0.9696191548  1.2944179828 \n          hpl            X.          X..1     CapRunMax   CapRunTotal \n-1.3114765304  0.5673271835  8.2721841199  0.0118045995  0.0005570864 \n        const            hp        george         X1999            re \n-1.4278739763 -2.0411544503 -6.0021765135 -0.4565997686 -0.8577822552 \n          edu \n-1.6854611460 \n\n\n\n\nThe posterior standard deviations are computed from the covariance\n\nprint(postStd)\n\n         our         over       remove     internet         free          hpl \n0.0730320059 0.2321086478 0.3302456199 0.1671111765 0.1412670451 0.4017479109 \n          X.         X..1    CapRunMax  CapRunTotal        const           hp \n0.0947016268 0.6851475429 0.0017545736 0.0001418867 0.0847302222 0.2998192165 \n      george        X1999           re          edu \n1.1494146395 0.1902088194 0.1476136565 0.2554459768 \n\n\n\n\nPlot the marginal posterior of \\(\\beta\\) for the free and hpl covariates\n\npar(mfrow=c(1,2))\ngridVals = seq(postMode['free']-3*postStd['free'], postMode['free']+3*postStd['free'], \n               length = 100)\nplot(gridVals, dnorm(gridVals, mean = postMode['free'], sd = postStd['free']), \n     xlab = expression(beta), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n     lwd = 2, col = prettyCol[2], main = expression(beta[free]))\ngridVals = seq(postMode['hpl']-3*postStd['hpl'], postMode['hpl']+3*postStd['hpl'], \n               length = 100)\nplot(gridVals, dnorm(gridVals, mean = postMode['hpl'], sd = postStd['hpl']), \n     xlab = expression(beta), ylab= \"posterior density\", type =\"l\", bty = \"n\", \n     lwd = 2, col = prettyCol[2], main = expression(beta[hpl]))\n\n\n\n\n\n\nSimulate from normal approximation and make prediction at mean covariate\n\nxStar = colMeans(X)\nnSim = 1000\nprobSpam = rep(0,nSim)\nspamPred = rep(0,nSim)\nfor (i in 1:nSim){\n  betaDraw = as.vector(rmvnorm(1, postMode, postCov)) # Simulate a beta draw from approx post\n  linPred = t(xStar)%*%betaDraw\n  probSpam[i] = exp(linPred)/(1+exp(linPred)) # draw from posterior of Pr(spam|x)\n  spamPred[i] = rbinom(n=1,size=1,probSpam[i]) # draw from model given probSpam[i]\n}\npar(mfrow=c(1,2))\nhist(probSpam, freq = FALSE, xlab = expression(theta[i]), ylab= \"\", col = prettyCol[3],\n     main = \"Posterior distribution for Pr(spam|x)\", cex.main = 0.7)\nbarplot(c(sum(spamPred==0),sum(spamPred==1))/nSim, names.arg  = c(\"ham\",\"spam\"), col = prettyCol[7],\n     main = \"Predictive distribution spam\", cex.main = 0.7)"
  },
  {
    "objectID": "notebooks/ebayPoissonOneParam/eBayPoissonR.html",
    "href": "notebooks/ebayPoissonOneParam/eBayPoissonR.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Modeling the number of Bids in eBay coin auctions\na notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nWe want learn about the number of bidders in a eBay internet auction. The dataset below contains information on the number of bidders and covariates/features that can be used to predict the number of bidders. We will later use a Poisson regression to build a prediction model, but we will here only analyze the number of bids using an simple iid Poission model without covariates.\n\n\nImport modules and load the data\n\nset.seed(123) # Set the seed for reproducibility\noptions(repr.plot.width=15, repr.plot.height=6, lwd = 4)\n#install.packages(\"RColorBrewer\")\nlibrary(\"RColorBrewer\")\ncolors = brewer.pal(12, \"Paired\")\n\n# Load the data\neBayData = read.csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData$nBids\n\n\n\nData\nThe dataset contains data from 1000 auctions of collector coins. The dataset was collected and first analyzed in the article Bayesian Inference in Structural Second-Price Common Value Auctions. Let’s read in the full dataset and extract the variable of interest, the number of bids (nBids):\n\neBayData = read.csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData$nBids\n\n\n\nPrior-to-Posterior updating\nWe will model these data using a Poisson distribution: \\[y_1,...,y_n \\vert \\theta \\overset{iid}{\\sim} \\mathrm{Poisson}(\\theta)\\]\nwith a conjugate Gamma prior\n\\[\\theta  \\sim \\mathrm{Gamma}(\\alpha, \\beta)\\]\nso that the posterior is also Gamma: \\[\\theta \\vert y_1,\\ldots,y_n \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n y_i, \\beta + n)\\]\n\nPostPoisson <- function(y, alphaPrior, betaPrior, thetaPriorGrid = NA, thetaPostGrid = NA){\n\n    # Compute Prior density and posterior\n    priorDens = dgamma(thetaPriorGrid, shape = alphaPrior, rate = betaPrior)\n    n = length(y)\n    alphaPost = alphaPrior + sum(y)\n    betaPost = betaPrior + n\n    postDens = dgamma(thetaPostGrid, shape = alphaPost, rate = betaPost)\n    \n    message(paste('Mean number of counts = ', mean(y)))\n    message(paste('Prior mean = ', alphaPrior/betaPrior))\n    message(paste('Posterior mean = ', round(alphaPost/betaPost,3)))\n    message(paste('Prior standard deviation = ', sqrt(alphaPrior/(betaPrior**2))))\n    message(paste('Posterior standard deviation = ', sqrt( (alphaPrior+sum(y))/((betaPrior+n)**2)) ))\n    message(paste('Equal tail 95% prior interval: (' ,qgamma(0.025, shape = alphaPrior, rate = betaPrior),',',qgamma(0.975, shape = alphaPrior, rate = betaPrior),')'))\n    message(paste('Equal tail 95% posterior interval: (' ,qgamma(0.025, shape = alphaPost, rate = betaPost),',',qgamma(0.975, shape = alphaPost, rate = betaPost),')'))\n\n    if ( any(is.na(thetaPriorGrid)) != TRUE){\n        par(mfrow = c(1,2))\n        plot(thetaPriorGrid, priorDens, type = \"l\", lwd = 3, col = colors[2], xlab = expression(theta), ylab = \"PDF\", main = 'Prior distribution')\n        plot(thetaPostGrid, postDens, type = \"l\", lwd = 3, col = colors[8], xlab = expression(theta), ylab = \"PDF\", main = 'Posterior distribution')\n    }\n}\n\nalphaPrior = 2\nbetaPrior = 1/2\nPostPoisson(y = nBids, alphaPrior = 2, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 12, length = 10000), thetaPostGrid = seq(3.25, 4, length = 10000))\n\n\n\nFit of the Poisson model\nLet’s plot the data along with the fitted Poisson model. We’ll keep things simple and plot the fit for the posterior mean of \\(\\theta\\).\n\nplotPoissonFit <- function(y, alphaPrior, betaPrior){\n    \n    # Compute empirical distribution of the data\n    n = length(y)\n    yGrid = seq(0, max(y))\n    probs = rep(NA,max(y)+1)\n    for (i in yGrid){\n        probs[i+1] = sum(y == i)/n\n    }\n    \n    # Compute posterior mean and Poisson model fit\n    alphaPost = alphaPrior + sum(y)\n    betaPost = betaPrior + n\n    postMean = alphaPost/betaPost\n    \n    # Plot the data and model fit\n    poisFit = dpois(yGrid, lambda = postMean) \n    plot(yGrid, probs, type = \"o\", lwd = 6, xlab = \"y\", ylab = \"PMF\", col = colors[1], main = 'Fitted Poisson model', \n           ylim = c(0,max(probs, poisFit)))\n    lines(yGrid, poisFit, col = colors[2], lwd = 6, type = \"o\")\n    legend(x = \"topright\", inset=.05, legend = c(\"Data distribution\", \"Poisson fit\"), pch = c(19,19), cex = c(1,1),\n       lty = c(1, 1), pt.lwd = c(3,3), col = c(colors[1], colors[2]))\n}\n\n\nalphaPrior = 2\nbetaPrior = 1/2\nplotPoissonFit(y = nBids, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\nWow, that’s are terrible fit! This data does not look at all like a Poisson distribution. What can we do?\n\n\nAnalyzing only the auctions with low reservation prices\nWe will later model the number of bids using a Poisson regression where we take into account several explanatory variables. But, for now, let’s split the auctions in two subsets:\ni) auctions with low reservation price in relation to the item’s book value (MinBidShare<=0)\nii) auctions with high reservation price in relation to the item’s book value (MinBidShare>0)\nLet’s start with the 550 auction with low reservation prices. The prior for the auction with low reservation prices is set to \\(\\theta \\sim \\mathrm{Gamma}(4,1/2)\\) to reflect a belief that belief that such auctions are likely to attract more bids.\n\n# Auctions with low reservation prices:\nnBidsLow = nBids[eBayData$MinBidShare<=0]\nPostPoisson(y = nBidsLow, alphaPrior = 4, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 25,length = 10000), thetaPostGrid = seq(4.8, 5.8, length = 10000))\n\nAs expected, the posterior for the mean number of bids is concentrated on a larger number of bids. People like to bid on items where the seller’s reservation price is low.\nIs the first for these auctions improved? Yes it is, although there is still room for improvement:\n\n# Plot the fit for low bids\nplotPoissonFit(y = nBidsLow, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\nAnalyzing the auctions with high reservation prices\nBelow are the results for the auction with high reservation bids. The prior is here set to \\(\\theta \\sim \\mathrm{Gamma}(1,1/2)\\) implying less on average.\n\n# Auctions with high reservation prices:\nnBidsHigh = nBids[eBayData$MinBidShare>0]\nPostPoisson(y = nBidsHigh, alphaPrior = 1, betaPrior = 1/2,\n            thetaPriorGrid = seq(0.01, 12, length = 10000), thetaPostGrid = seq(1.3, 1.8,length = 10000))\n\nAnd the fit is not perfect for these bids, but better than before.\n\n# Plot the fit for high bids\nplotPoissonFit(y = nBidsHigh, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\nSo, separating the bids into dataset with low and high reservation prices makes the Poisson model a lot better for the data. Later in the book, we will use a Poisson regression with reservation price as one of the features, which an even more fine grained analysis."
  },
  {
    "objectID": "notebooks/ebayPoissonOneParam/eBayPoissonPython.html",
    "href": "notebooks/ebayPoissonOneParam/eBayPoissonPython.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Modeling the number of bids in eBay coin auctions\nan Jupyter notebook for the book Bayesian Learning by Mattias Villani\nThe dataset contains data from 1000 auctions of collector coins. The dataset was collected and first analyzed in the article Bayesian Inference in Structural Second-Price Common Value Auctions.\n\nImport modules and load the data\n\nimport numpy as np\nimport scipy.stats as sps\nimport pandas as pd\nimport matplotlib.pyplot as plt\nnp.random.seed(seed=123) # Set the seed for reproducibility\n\n# Load the data\neBayData = pd.read_csv('https://github.com/mattiasvillani/BayesianLearningBook/raw/main/data/ebaybids/ebaybids.csv', sep = ',')\nnBids = eBayData['nBids']\n\nWe will model these data using a Poisson distribution: \\[y_1,...,y_n \\vert \\theta \\overset{iid}{\\sim} \\mathrm{Poisson}(\\theta).\\] with a conjugate Gamma prior\n\\[\\theta  \\sim \\mathrm{Gamma}(\\alpha, \\beta)\\]\nso that the posterior is also Gamma:\n\\[\\theta \\vert y_1,\\ldots,y_n \\sim \\mathrm{Gamma}(\\alpha + \\sum_{i=1}^n y_i, \\beta + n)\\]\n\n# Define the Gamma distribution in the rate parametrization\ndef gammaPDF(x, alpha, beta):\n    return(sps.gamma.pdf(x, a = alpha, scale = 1/beta))\n\n\ndef PostPoisson(y, alphaPrior, betaPrior, thetaPriorGrid = None, thetaPostGrid = None):\n\n    # Compute Prior density and posterior\n    priorDens = gammaPDF(x = thetaPriorGrid, alpha = alphaPrior, beta = betaPrior)\n    n = len(y)\n    alphaPost = alphaPrior + np.sum(y)\n    betaPost = betaPrior + n\n    postDens = gammaPDF(x = thetaPostGrid, alpha = alphaPost, beta = betaPost)\n    \n    print('Number of data points = ' + str(len(y)))\n    print('Sum of number of counts = ' + str(np.sum(y)))\n    print('Mean number of counts = ' + str(np.mean(y)))\n    print('Prior mean = ' + str(alphaPrior/betaPrior))\n    print('Prior standard deviation = '+ str(np.sqrt(alphaPrior/(betaPrior**2))))\n    print('Equal tail 95% prior interval: ' + str(sps.gamma.interval(0.95, a = alphaPrior, scale = 1/betaPrior)))  \n    print('Posterior mean = ' + str(round(alphaPost/betaPost,3)))\n    print('Posterior standard deviation = '+ str(np.sqrt(    (alphaPrior+np.sum(y))/  ((betaPrior+n)**2)  )    ))\n    print('Equal tail 95% posterior interval: ' + str(sps.gamma.interval(0.95, a = alphaPost, scale = 1/betaPost)))  \n\n    if (thetaPriorGrid.any() != None):\n        fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n        h1, = ax[0].plot(thetaPriorGrid, priorDens, lw = 3);\n        ax[0].set_xlabel(r'$\\theta$');ax[0].set_ylabel('PDF');\n        ax[0].set_title('Prior distribution');\n\n        h2, = ax[1].plot(thetaPostGrid, postDens, lw = 3, color =\"orange\");\n        ax[1].set_xlabel(r'$\\theta$');ax[1].set_ylabel('PDF');\n        ax[1].set_title('Posterior distribution');\n\nalphaPrior = 2\nbetaPrior = 1/2\nPostPoisson(y = nBids, alphaPrior = 2, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,12,10000), thetaPostGrid = np.linspace(3.25,4,10000))\n\nNumber of data points = 1000\nSum of number of counts = 3635\nMean number of counts = 3.635\nPrior mean = 4.0\nPrior standard deviation = 2.8284271247461903\nEqual tail 95% prior interval: (0.48441855708793014, 11.143286781877796)\nPosterior mean = 3.635\nPosterior standard deviation = 0.06027740643004339\nEqual tail 95% posterior interval: (3.5179903738284697, 3.7542677655304297)\n\n\n\n\n\n\n\nFit of the Poisson model\nLet’s plot the data along with the fitted Poisson model. We’ll keep things simple and plot the fit for the posterior mean of \\(\\theta\\).\n\ndef plotPoissonFit(y, alphaPrior, betaPrior):\n    \n    # Plot data\n    maxY = np.max(y)\n    yGrid = np.arange(maxY)\n    probs = [np.sum(y==k)/len(y) for k in range(maxY)]\n    h1 = plt.bar(yGrid, probs, alpha = 0.3);\n    plt.xlabel('y');plt.ylabel('PMF');\n    plt.xticks(yGrid);\n    plt.title('Fitted Poisson model based on posterior mean estimate');\n    \n    # Compute posterior mean\n    n = len(y)\n    alphaPost = alphaPrior + np.sum(y)\n    betaPost = betaPrior + n\n    postMean = alphaPost/betaPost\n    \n    # Plot the fit based on the posterior mean of theta\n    poisFit = sps.poisson.pmf(yGrid, mu = postMean) \n    plt.plot(yGrid, poisFit, color = 'orange', lw = 3)\n\n\n# Plot the fit for all bids\nalphaPrior = 2\nbetaPrior = 1/2\nplotPoissonFit(y = nBids, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\nWow, that’s are terrible fit! This data does not look at all like a Poisson distribution. What can we do?\n\n\nAnalyzing the auction with low and high reservation prices separately.\nWe will later model the number of bids using a Poisson regression where we take into account several explanatory variables. But, for now, let’s split the auctions in two subsets:\ni) auctions with low reservation price in relation to the item’s book value (MinBidShare<=0)\nii) auctions with high reservation price in relation to the item’s book value (MinBidShare>0)\nLet’s start with the 550 auction with low reservation prices. The prior for the auction with low reservation prices is set to \\(\\theta \\sim \\mathrm{Gamma}(4,1/2)\\) to reflect a belief that belief that such auctions are likely to attract more bids.\n\n# Auctions with low reservation prices:\nnBidsLow = nBids[eBayData['MinBidShare']<=0]\n\nPostPoisson(y = nBidsLow, alphaPrior = 4, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,25,10000), thetaPostGrid = np.linspace(4.8,5.8,10000))\n\nNumber of data points = 550\nSum of number of counts = 2927\nMean number of counts = 5.321818181818182\nPrior mean = 8.0\nPrior standard deviation = 4.0\nEqual tail 95% prior interval: (2.17973074725265, 17.534546139484647)\nPosterior mean = 5.324\nPosterior standard deviation = 0.0983446153216288\nEqual tail 95% posterior interval: (5.13322503650632, 5.518717305739481)\n\n\n\n\n\nAs expected, the posterior for the mean number of bids is concentrated on a larger number of bids. People like to bid on items where the seller’s reservation price is low.\nIs the first for these auctions improved? Yes it is, although there is still room for improvement:\n\n# Plot the fit for low bids\nplotPoissonFit(y = nBidsLow, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\nBelow are the results for the auction with high reservation bids. The prior is here set to \\(\\theta \\sim \\mathrm{Gamma}(1,1/2)\\) implying less on average.\n\n# Auctions with high reservation prices:\nnBidsHigh = nBids[eBayData['MinBidShare']>0]\n\nPostPoisson(y = nBidsHigh, alphaPrior = 1, betaPrior = 1/2,\n            thetaPriorGrid = np.linspace(0.01,12,10000), thetaPostGrid = np.linspace(1.3,1.8,10000))\n\nNumber of data points = 450\nSum of number of counts = 708\nMean number of counts = 1.5733333333333333\nPrior mean = 2.0\nPrior standard deviation = 2.0\nEqual tail 95% prior interval: (0.050635615968579795, 7.377758908227871)\nPosterior mean = 1.574\nPosterior standard deviation = 0.05910555807189499\nEqual tail 95% posterior interval: (1.4600786825716714, 1.6917395497993104)\n\n\n\n\n\nAnd the fit is not perfect for these bids, but better than before.\n\n# Plot the fit for high bids\nplotPoissonFit(y = nBidsHigh, alphaPrior = alphaPrior, betaPrior = betaPrior)\n\n\n\n\nSo, separating the bids into dataset with low and high reservation prices makes the Poisson model a lot better for the data. Later in the book, we will use a Poisson regression with reservation price as one of the features, which an even more fine grained analysis."
  },
  {
    "objectID": "notebooks/DownloadSpeedNormal/DownloadSpeedNormalR.html",
    "href": "notebooks/DownloadSpeedNormal/DownloadSpeedNormalR.html",
    "title": "Bayesian Learning Book",
    "section": "",
    "text": "Analyzing internet download speeds with a Gaussian model\na notebook for the book Bayesian Learning by Mattias Villani\n\nProblem\nThe maximum internet connection speed downstream in my home is 50 Mbit/sec. This maximum will typically never be reached, but my internet service provider (ISP) claims that the average speed is at least 20Mbit/sec. I want to collect some data to investigate this.\n\n\nGetting started\nFirst, some housekeeping: loading libraries and setting up colors.\n\noptions(repr.plot.width=8, repr.plot.height=6, lwd = 4)\nlibrary(\"RColorBrewer\") # for pretty colors\nlibrary(\"tidyverse\")    # for string interpolation to print variables in plots.\nlibrary(\"latex2exp\")    # the TeX() function makes it possible to print latex math\ncolors = brewer.pal(12, \"Paired\")[c(1,2,7,8,3,4,5,6,9,10)];\n\n\n\nData\nI collect a total of five measurements over the course of five consecutive using an speed testing internet service:\n\nx = c(15.77, 20.5, 8.26, 14.37, 21.09)\n\n\n\nModel\nThe measurements are assumed to be \\[x_1,\\ldots,x_n \\overset{\\mathrm{iid}}{\\sim} \\mathrm{N}(\\theta,\\sigma^2),\\] where \\(\\theta\\) is the average speed; we ignore for simplicity that the measurements cannot be negative.\nThe measurements are reported to have a standard deviation of \\(\\sigma=5\\) by speed testing service and we take this as the given \\(\\sigma\\).\n\nsigma2 = 5^2\n\n\n\nPrior\nI will use a prior centered on the average claimed by the ISP, \\(\\mu_0=20\\), with a prior standard deviation of \\(\\tau_0=5\\). My prior beliefs are therefore that \\(\\theta \\in [10,30]\\) with approximately \\(95\\%\\) probability.\n\nmu_0 = 20\ntau2_0 = 5^2\n\n\n\nPosterior\nA normal prior for a normal model gives us a posterior which is also normal:\n\\[ \\theta | \\mathbf{x} \\sim \\mathrm{N}(\\mu_n,\\tau_n^2), \\] where the posterior precision (1/variance) is the sum of the data precision and the prior precision \\[ \\frac{1}{\\tau_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}  \\] and the posterior mean is a weighted average of the sample mean and the prior mean \\[ \\mu_n = w \\bar x + (1-w)\\mu_0\\] where the weight is the relative precision of the data and prior information \\[ w = \\frac{n/\\sigma^2}{n/\\sigma^2 + 1/\\tau_0^2}\\]\nLet’s write a small function that computes the posterior mean and variance, and plots the prior, likelihood and posterior.\n\npostGaussianIID <- function(x, mu_0, tau2_0, sigma2, thetaGrid, areaPoint){\n    \n    # compute posterior mean and variance\n    n = length(x)\n    tau2_n = 1/(n/sigma2 + 1/tau2_0) \n    w = (n/sigma2)/(n/sigma2 + 1/tau2_0)\n    mu_n = w*mean(x) + (1-w)*mu_0\n    \n    # plot PDFs. Likelihood is normalized.\n    priorPDF = dnorm(thetaGrid, mean = mu_0, sd = sqrt(tau2_0))\n    postPDF = dnorm(thetaGrid, mean = mu_n, sd = sqrt(tau2_n))\n    postProbAbovePoint = 1-pnorm(areaPoint, mean = mu_n, sd = sqrt(tau2_n))\n    normLikePDF = dnorm(thetaGrid, mean = mean(x[1:n]), sd = sqrt(sigma2/n))\n    plot(1, type=\"n\", axes=FALSE, xlab = expression(theta), ylab = \"\", \n         xlim=c(min(thetaGrid),max(thetaGrid)), \n         ylim = c(0,max(priorPDF,postPDF,normLikePDF)))\n    axis(side = 1)\n    polygon(c(thetaGrid[thetaGrid>=areaPoint], max(thetaGrid), areaPoint), \n            c(postPDF[thetaGrid>=areaPoint], 0, 0), \n            col=adjustcolor(colors[4],alpha.f=0.3), border=NA)\n    lines(thetaGrid, priorPDF, type = \"l\", lwd = 4, col = colors[6])\n    lines(thetaGrid, normLikePDF, lwd = 4, col = colors[2])\n    lines(thetaGrid, postPDF, lwd = 4, col = colors[4])\n    legend(x = \"topright\", inset=.05, cex = c(1,1,1,1), \n           legend = c(\"Prior\", \"Likelihood (normalized)\", \"Posterior\", \n           TeX(sprintf(\"$Pr(\\\\theta \\\\geq %2.0f | \\\\mathbf{x}) =  %0.3f$\", areaPoint, postProbAbovePoint))),  \n           lty = c(1, 1, 1, 1), pt.lwd = c(3, 3, 3, 3), \n           col = c(colors[6], colors[2], colors[4], adjustcolor(colors[4],alpha.f=0.3)))\n    cat(\"Posterior mean is \", round(mu_n,3), \"\\n\")\n    cat(\"Posterior standard deviation is \", round(sqrt(tau2_n),3), \"\\n\")\n    cat(\"The weight on the sample mean is \", round(w,3))\n    return(list(\"mu_n\" = mu_n, \"tau2_n\" = tau2_n, \"w\" = w))\n}\n\nLet us start by analyzing just the first observation \\(x_1=15.77\\) using this function.\n\nthetaGrid = seq(0, 40, length = 1000) # Some suitable grid of values to plot over\nareaPoint = 20 # shade the region where theta>= areaPoint (20 in my example)\nn = 1\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  17.885 \nPosterior standard deviation is  3.536 \nThe weight on the sample mean is  0.5\n\n\n\n\n\nWe see that the prior and data information happen to get the same weight (w) in the posterior. That is a coincidence from the fact that the prior variance \\(\\tau_0^2\\) is the same as the data variance \\(\\sigma^2\\).\nMoving on, let’s add the next measurement to the analysis:\n\nn = 2\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  18.757 \nPosterior standard deviation is  2.887 \nThe weight on the sample mean is  0.667\n\n\n\n\n\nWe now see that the posterior is more affected by the data information than the prior information (w = 0.666).\nFinally, adding all \\(n=5\\) data points gives:\n\nn = 5\npost = postGaussianIID(x[1:n], mu_0, tau2_0, sigma2, thetaGrid, areaPoint)\n\nPosterior mean is  16.665 \nPosterior standard deviation is  2.041 \nThe weight on the sample mean is  0.833\n\n\n\n\n\nI am now rather sure that my average download speed is less than 20 MBit/sec since the posterior probability of \\(\\theta\\geq20\\) is only \\(0.051\\)."
  },
  {
    "objectID": "observable/distributions.html#beta-distribution",
    "href": "observable/distributions.html#beta-distribution",
    "title": "Distributions",
    "section": "Beta distribution",
    "text": "Beta distribution"
  },
  {
    "objectID": "observable/distributions.html#binomial-distribution",
    "href": "observable/distributions.html#binomial-distribution",
    "title": "Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution"
  },
  {
    "objectID": "observable/distributions.html#multivariate-normal-distribution",
    "href": "observable/distributions.html#multivariate-normal-distribution",
    "title": "Distributions",
    "section": "Multivariate normal distribution",
    "text": "Multivariate normal distribution"
  },
  {
    "objectID": "observable/distributions.html#poisson-distribution",
    "href": "observable/distributions.html#poisson-distribution",
    "title": "Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution"
  },
  {
    "objectID": "observable/distributions.html#scaled-inverse-chi2-distribution",
    "href": "observable/distributions.html#scaled-inverse-chi2-distribution",
    "title": "Distributions",
    "section": "Scaled inverse chi2 distribution",
    "text": "Scaled inverse chi2 distribution"
  },
  {
    "objectID": "observable/distributions.html#student-t-distribution",
    "href": "observable/distributions.html#student-t-distribution",
    "title": "Distributions",
    "section": "Student-t distribution",
    "text": "Student-t distribution"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "This page contains a set of notebooks in Julia, R and Python for some of the data analyses presented in the book.\n\n\n\nChapter\nTopic\nLanguage\n\n\n\n\nSingle-parameter models\nBernoulli model for spam data\nR\nJulia/Pluto\n\n\nSingle-parameter models\nNormal model for internet download speed data\nR\n\n\nSingle-parameter models\nPoisson for number of eBay bidders\nR\nPython\n\n\nMulti-parameter models\nMultinomial model for survey data\nR\nJulia/Pluto\n\n\nClassification\nLogistic regression for spam data\nR"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Learning - the book",
    "section": "",
    "text": "This is the home for the book Bayesian Learning - a gentle introduction, which is still work in progress. The book is currently used for the course Bayesian Learning at Stockholm University.\nA pdf of the book will always be available, even after the book gets published.\nThe Notebooks tab contains Quarto/Jupyter/Pluto notebooks for the chapters in the book.\nThe Interactive tab contains interactive Observable widgets.\n\nContents\n\nThe Bayesics\nSingle-parameter models\nMulti-parameter models\nPriors\nRegression\nPrediction and Decision making\nNormal posterior approximation\nClassification\nPosterior simulation\nVariational inference\nRegularization\nModel comparison\nVariable selection\nGaussian processes\nInteraction models\nMixture models\nDynamic models and sequential inference\nAppendix: Some Mathematical results\n\nThanks to all who found typos and error in the book."
  },
  {
    "objectID": "interactive.html",
    "href": "interactive.html",
    "title": "Interactive applications",
    "section": "",
    "text": "Chapter 2 - One-parameter models\n\nPrior-Posterior Gaussian data with known variance\nhtml widget | notebook on observablehq\nSequential updating - Gaussian data with known variance\nhtml widget | notebook on observablehq\n\n\n\nChapter 6 - Prediction and Decision making\n\nPredictive distribution - Gaussian data with known variance\nhtml widget | notebook on observablehq\n\n\n\nDistributions\n\nBernoulli\nBeta\nBinomial\nMultivariate normal\nPoisson\nScaled-inv-chi2\nStudent-t\n\n\n\nCollection on ObservableHQ\nThe whole collection of interactive Observable notebooks for the book can be found here."
  },
  {
    "objectID": "halloferrors.html",
    "href": "halloferrors.html",
    "title": "Hall of Typos",
    "section": "",
    "text": "The following people 🙏 have kindly reported typos and errors in the book (ordered by the number of typos found):\n\nAlice Jonason, Stockholm University\nMichael Sederlin, KTH Royal Institute of Technology\nTea Unnebäck, Stockholm University\nFederico M. Stefanini, University of Milan\n\nPlease email me any typos and errors at my obvious gmail address. Thanks!"
  }
]